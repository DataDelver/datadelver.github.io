{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Blog","text":""},{"location":"about.html","title":"About","text":"<p>Welcome to my blog data delver! I'm so glad you found your way here!  My name is Chase Greco, I'm currently a machine learning engineer with 5+ years of industry experience.  I love exploring all things machine learning with a particular emphasis on practical application and \"making things real\". I started this blog to document my own \"delves\" into the labyrinth of of Data Science, Machine Learning, and MLOps along with any resulting knowledge I have unearthed.</p> <p>If you would like to contact me you can do so at chase@datadelver.com</p>"},{"location":"tags.html","title":"Tags","text":""},{"location":"tags.html#tag:fun","title":"Fun","text":"<ul> <li>            Delve 3: Fun Linux Utilities          </li> <li>            Delve 5: The Quest for a Full Screen Raspberry Pi Application          </li> <li>            Delve 16: The Quest for a Full Screen Raspberry Pi Application - Part 2          </li> </ul>"},{"location":"tags.html#tag:modern-ml-microservices","title":"Modern ML Microservices","text":"<ul> <li>            Delve 6: Let's Build a Modern ML Microservice Application - Part 1          </li> <li>            Delve 7: Let's Build a Modern ML Microservice Application - Part 2, The Data Layer          </li> <li>            Delve 8: Let's Build a Modern ML Microservice Application - Part 3, The Business Logic and Interface Layers          </li> <li>            Delve 10: Let's Build a Modern ML Microservice Application - Part 4, Configuration          </li> <li>            Delve 11: Let's Build a Modern ML Microservice Application - Part 5, Testing          </li> <li>            Delve 12: Let's Build a Modern ML Microservice Application - Part 6, Containerization          </li> <li>            Delve 13: Let's Build a Modern ML Microservice Application - Part 7, Model Tracking and APIs with MLFlow          </li> <li>            Delve 15: Let's Build a Modern ML Microservice Application - Part 8, The Orchestrator Service          </li> <li>            Delve 19: Let's Build a Modern ML Microservice Application - Part 9, Docker Container Optimization          </li> </ul>"},{"location":"tags.html#tag:opinion","title":"Opinion","text":"<ul> <li>            Delve 1: The (Hidden) Danger of Notebooks in Production          </li> <li>            Delve 4: The ML Engineer, Coming to an Enterprise Near You          </li> <li>            Delve 14: Reflections on a Job Quest          </li> <li>            Delve 17: Data Science, An Industry Perspective          </li> <li>            Delve 18: The Challenges of AI in Industry          </li> <li>            Delve 20: A Look back at 2025          </li> </ul>"},{"location":"tags.html#tag:resource","title":"Resource","text":"<ul> <li>            Delve 2: My Recommended Machine Learning &amp; Data Science Resources!          </li> <li>            Delve 14: Reflections on a Job Quest          </li> </ul>"},{"location":"tags.html#tag:series","title":"Series","text":"<ul> <li>            Delve 6: Let's Build a Modern ML Microservice Application - Part 1          </li> <li>            Delve 7: Let's Build a Modern ML Microservice Application - Part 2, The Data Layer          </li> <li>            Delve 8: Let's Build a Modern ML Microservice Application - Part 3, The Business Logic and Interface Layers          </li> <li>            Delve 10: Let's Build a Modern ML Microservice Application - Part 4, Configuration          </li> <li>            Delve 11: Let's Build a Modern ML Microservice Application - Part 5, Testing          </li> <li>            Delve 12: Let's Build a Modern ML Microservice Application - Part 6, Containerization          </li> <li>            Delve 13: Let's Build a Modern ML Microservice Application - Part 7, Model Tracking and APIs with MLFlow          </li> <li>            Delve 15: Let's Build a Modern ML Microservice Application - Part 8, The Orchestrator Service          </li> <li>            Delve 19: Let's Build a Modern ML Microservice Application - Part 9, Docker Container Optimization          </li> </ul>"},{"location":"tags.html#tag:tools","title":"Tools","text":"<ul> <li>            Delve 3: Fun Linux Utilities          </li> <li>            Delve 5: The Quest for a Full Screen Raspberry Pi Application          </li> <li>            Delve 9: Migrating from Jekyll to Material for MkDocs          </li> <li>            Delve 16: The Quest for a Full Screen Raspberry Pi Application - Part 2          </li> <li>            Delve 21: A Local Claude Code          </li> </ul>"},{"location":"tags.html#tag:tutorial","title":"Tutorial","text":"<ul> <li>            Delve 6: Let's Build a Modern ML Microservice Application - Part 1          </li> <li>            Delve 7: Let's Build a Modern ML Microservice Application - Part 2, The Data Layer          </li> <li>            Delve 8: Let's Build a Modern ML Microservice Application - Part 3, The Business Logic and Interface Layers          </li> <li>            Delve 10: Let's Build a Modern ML Microservice Application - Part 4, Configuration          </li> <li>            Delve 11: Let's Build a Modern ML Microservice Application - Part 5, Testing          </li> <li>            Delve 12: Let's Build a Modern ML Microservice Application - Part 6, Containerization          </li> <li>            Delve 13: Let's Build a Modern ML Microservice Application - Part 7, Model Tracking and APIs with MLFlow          </li> <li>            Delve 15: Let's Build a Modern ML Microservice Application - Part 8, The Orchestrator Service          </li> <li>            Delve 19: Let's Build a Modern ML Microservice Application - Part 9, Docker Container Optimization          </li> <li>            Delve 21: A Local Claude Code          </li> </ul>"},{"location":"2023/11/06/delve-0-hello-labyrinth-world.html","title":"Delve 0: Hello Labyrinth (World)!","text":"<p>It seemed so daunting, \"I need to make this model work!\", \"Running this in a notebook isn't good enough, we need to drive live site traffic against this!\", \"All of this data is bad!\".</p>"},{"location":"2023/11/06/delve-0-hello-labyrinth-world.html#entering-the-labyrinth","title":"Entering the Labyrinth","text":"<p>Welcome to my blog data delver! I'm so glad you found your way here!  If you're like me, when you first started out with data science and machine learning, you may have been feeling overwhelmed. With so many different concepts to learn it may have seemed as if there was an insurmountable labyrinth of information ahead of you, with no clear path towards mastery and practical application.  Fear not! For you have found a resource which shall aid you in your own quest to navigate the maze.</p>"},{"location":"2023/11/06/delve-0-hello-labyrinth-world.html#purpose-of-this-blog","title":"Purpose of this Blog","text":"<p>The purpose of this blog is to document my own \"delves\" into this labyrinth and any resulting knowledge I have unearthed. I plan to focus on a range of topics, from general software engineering to data science, machine learning engineering, and MLOps, pulling from a range of experiences across my own career as a machine learning engineer and data scientist with a focus on practical, grounded application in industry following best practice.</p>"},{"location":"2023/11/06/delve-0-hello-labyrinth-world.html#who-am-i","title":"Who am I?","text":"<p>My name is Chase Greco, I'm currently a machine learning engineer with 5+ years of industry experience. I love exploring all things machine learning with a particular emphasis on practical application and \"making things real\". When I'm not writing code, I enjoy swing dancing, reading, and playing video games.</p>"},{"location":"2023/11/06/delve-0-hello-labyrinth-world.html#delve-data","title":"Delve Data","text":"<ul> <li>Welcome to my blog!</li> <li>Stay tuned for more posts on data science, machine learning, and MLOps!</li> </ul>"},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html","title":"Delve 1: The (Hidden) Danger of Notebooks in Production","text":"<p>\"Coming together is a beginning. Keeping together is progress. Working together is success.\" - Henry Ford</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#when-good-intentions-go-awry","title":"When Good Intentions go Awry","text":"<p>At many points in my career I have come across the topic of deploying code related to machine learning models in the form of Jupyter Notebooks. Often, the push towards this idea comes from a place of good intentions, of speeding up the the model deployment process or enabling better access to and understanding of the production environment by data scientists. However, despite the good intentions, this approach has in my experience created an environment of quite negative effect for the engineering teams asked to maintain these systems. In this delve, I will share my own personal experiences on working with notebooks in production systems, some of the ways I have observed them creating unnecessary friction between data scientists and ML engineers, and reflect how I think notebooks can be used as part of a healthy production system.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#what-are-notebooks-anyway","title":"What are Notebooks Anyway?","text":"<p>According to Wikipedia:</p> <p>Jupyter Notebook (formerly IPython Notebook) is a web-based interactive computational environment for creating notebook documents. Jupyter Notebook is built using several open-source libraries, including IPython, ZeroMQ, Tornado, jQuery, Bootstrap, and MathJax. A Jupyter Notebook application is a browser-based REPL containing an ordered list of input/output cells which can contain code, text (using Github Flavored Markdown), mathematics, plots and rich media.</p> <p>Jupyter Notebook is similar to the notebook interface of other programs such as Maple, Mathematica, and SageMath, a computational interface style that originated with Mathematica in the 1980s. Jupyter interest overtook the popularity of the Mathematica notebook interface in early 2018.</p> <p>Essentially this makes notebooks a shell with several quality of life enhancements. They come with the ability to run blocks of code, called \"cells\", individually and in any order and inspect their output. They provide the ability to embed rich markdown snippets alongside code blocks to capture any documentation right with the code. And perhaps most usefully, they provide support for visualization, allowing the user to visual complex graphs or tables directly in the notebook.</p> <p>All of these capabilities combine to create a one-stop-shop for data exploration, visualization, and processing. This creates several advantages, namely in simplicity and ease of use. Notebooks provide a mechanism for individuals without a software engineering background to interface with and manipulate data in an intuitive way, providing a force multiplier to the data science space in much the same way excel spreadsheets were in the business world.</p> <p>However, just as when business processes grow to complex to manage within a spreadsheet and more rigorous engineering process must take over, so too can the work of a data scientist become too unwieldy to manage within a production environment in the form of a notebook.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#a-production-environment","title":"A \"Production\" Environment","text":"<p>Production can mean a lot of different things to different people, an analyst might say his work is in production when it is being used to influence business decisions. However, for the sake of this discussion I'd like to adopt a slightly more rigorous definition:</p> <p>An ML model is said to be in production when it's outputs are being used to affect a business process in some way, and the output needs to be accessed programmatically. </p> <p>That's it. Note: I have seen models used for a one-time report, or even ad-hoc reporting on a continual basis. However, the common thread there is such reporting can be done without leaving the data scientist's personal environment, be that their own laptop or cloud environment. As soon as a model needs the support of an external engineering team to deliver value I consider it in production.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#why-notebooks-in-production-are-so-appealing","title":"Why Notebooks in Production are so Appealing","text":"<p>The initial idea of using a notebook directly in production seems very appealing at first. The data scientist has already spent a considerable about of effort getting their code working, why can't we just use that? Wouldn't that create a much faster path for our data scientists to test new ideas and iterate faster?</p> <p>This argument has become so compelling that whole ecosystems of tools have sprung up around the idea of serving models directly from notebooks. Notably Databricks will espouse running notebooks within their platform as software engineering best practice and a preferred mechanism for deploying ML pipelines.</p> <p>However, time and again throughout my career I have seen engineering organizations adopt this paradigm of productionizing notebooks only to regret it and go through expensive refactors later. Why is this the case?</p> <p>The answer is that fundamentally notebooks are an exploratory and prototyping tool. This is what they were originally designed for, and no amount of tooling, or a surrounding platform, can substitute them for the principles of good software engineering.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#dangers-explicit-and-covert","title":"Dangers, Explicit and Covert","text":"<p>Notebooks, from their inception, were designed as a tool to enable the exploration and visualization of code in a highly collaborative way. At this objective they excel! There is a reason why notebooks are the preferred mechanism for sharing ideas on portals such as Kaggle. They are much more readable that a verbose script, and are excellent for producing demos of a concept and visualizing results in real time. I often rely on notebooks when producing reports or giving a presentation of model outputs to stakeholders. The dangers of notebooks are manifested not when they are used in this context for which they were envisioned, arguable no other tool does it better, but when the are used in an environment for which they were never intended, production engineering.</p> <p>From an engineering standpoint, notebooks present several challenges, all of which contribute to a cumbersome production environment that is difficult to maintain, namely:</p> <ul> <li>Notebooks are inherently difficult to test (especially unit test), which makes it difficult to determine if they are producing the correct outputs programmatically.</li> <li>Notebooks are difficult to debug, they don't play nice with the debugging tools built into popular IDEs like VSCode, nor with the native python debugger.</li> <li>Notebooks are difficult to check in to version control, the notebook file format does not easily incorporate into git.</li> <li>Notebooks do not promote software engineering best practices such as breaking code into modules, classes, or re-useable snippets, leading to large swaths of copy-pasta throughout the code base.</li> <li>Notebooks allow for out of order execution of cells, leading to non-deterministic execution patterns.</li> </ul> <p>For each of these use cases I will present potentially workarounds I have seen employed and why they usually fall flat.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#notebooks-are-difficult-to-test","title":"Notebooks are Difficult to Test","text":"<p>Automated Unit Testing and testing in general are some of the best tools we have as software engineers to prevent bugs from being released into production.  A software suite with a good amount of test coverage I have observed has been able to be modified more frequently and with more confidence by the developers tasked with maintaining it. Unfortunately, the notebook format is very antithetical to concept of unit testing. You can seen in the Databricks blog post above that though they claim to support unit testing with notebooks, the test themselves are written in plain python files and the notebook is only serving as a wrapper (with a funky prod toggle built into it no less) to execute the tests. This presents several challenges:</p> <ul> <li>It promotes different code behavior in different environments, how can you be confident about releasing your code to production if the are parts of the code that only run in production?</li> <li>It doesn't actually test the code in the notebook itself, all of the attempts I've seen to unit test notebooks treat the notebook itself as one \"unit\", however this doesn't let you break the notebook into smaller atomic pieces to test individual parts of the notebook. If you have a massive 500 line notebook with a bug in it it doesn't do you much good to know that it's somewhere in those 500 lines of code.</li> <li>It introduces much more complexity, all of the python unit testing frameworks (pytest, pyunit, nose, etc.) already provide excellent mechanisms for managing suites of unit tests, using the notebook as a wrapper to execute the tests feels like an attempt to shoehorn a software engineering best practice into an environment not designed to support it just for the sake of claiming that notebooks support unit testing.</li> </ul> <p>At the end of the day, whether or not it's possible to support unit testing in notebooks isn't really the issue. The issue is if running unit tests within notebooks is the most straightforward or maintainable way to test your code. I can speak from experience that the answer is no, so much so that I have seen engineering teams forgo automated testing all together when working in a notebook environment. If your solution to supporting a use case is so cumbersome that teams just avoid using it, it's not much of a solution is it?</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#notebooks-are-difficult-to-debug","title":"Notebooks are Difficult to Debug","text":"<p>Related to the previous issue, notebooks are notoriously difficult to debug. While this is changing and extensions are being released to enable better debugging experiences when working in notebooks, they are no where close to the debug experiences proper IDEs such as VSCode support for plain old python code out of the box. Concepts like breakpoints or variable inspection are not easy to implement in notebooks, requiring the insertion of ugly debug statements within your notebook code.  While notebooks do support running cells individually and inspecting their outputs, you are at the mercy of how long those cells are. I have seen massive 1000+ line cells be produced by data scientists that become impossible to debug when something goes wrong. Usually in such situations, engineers resort to copy pasting the code into a regular python file to to use their familiar debugging tools on it, which is not always feasible depending on how intricate the dependencies for they block of code are.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#notebooks-are-difficult-to-version-control","title":"Notebooks are Difficult to Version Control","text":"<p>The standard iPython notebook file format is JSON with source code, markdown, images, and cell outputs all stored in a single file. This makes it incredibly hard to handle merge conflicts or even viewing diffs of notebooks in git. For example, has this notebook file changed simply because it's been run and its cell outputs are now visible or has their been a functional change to the code? In order to solve this an engineer must dive through a pile of nested JSON to try and find out, yuck!</p> <p>This is such an issue that whole suites of tools have been developed such as Jupyterlab-Git to try and solve it, but they all required stepping outside the bounds of plain old git in favor of adding additional complexity and tooling to your version control stack.</p> <p>This speaks to the nature of what notebooks where intended to be used for, they were never meant to hold production code, otherwise their file format would have been designed differently to work with industry standard version control tools such as git. As is a recurring them, if we want to use notebooks in this manner for which they were not intended we must introduce additional complexity to make notebooks \"fit\" within best practices.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#notebooks-do-not-promote-software-engineering-best-practice","title":"Notebooks do not Promote Software Engineering Best Practice","text":"<p>Notebooks promote a linear style of scripting by their very design. You run one cell and then the next and then the next and so on. This is fine for very simple problems but breaks down when the problems to solve become more complex. One of the core tenets of software engineering is breaking a complex problem down into smaller sub-problems and solving each of those individually. This is often done but employing methods of object oriented programming: classes, functions, methods, and libraries.  Unfortunately these concepts are note available natively in notebooks, while you can import python code into notebooks and execute it (as is done in the aforementioned Databricks article) this delegates the notebook to be merely a wrapper to call well designed python code. The only way this would be maintainable is that if no business logic was within the notebook itself and the notebook just served as a \"main\" function, calling other non-notebook python modules. However in this scenario, what value is the notebook providing? Wouldn't it just be simpler at that point to have your main function within a plain old python script as well? The answer is none and yes.  </p> <p>Notebooks do allow calling other notebooks, however this promotes a false sense of modularity, instead of having your linear script within one notebook, you have simply distributed it across multiple notebooks, increasing the complexing of determining what is going on without any of the benefits of modular python code.</p> <p>In practice, notebooks promote the development of very brittle, linear scripts, spanning multiple files, they don't allow for any kind of modularity natively and their only mechanism to employ such modularity is to hold no logic at all and merely serve as a wrapper. Unfortunately I have never seen this done in industry and instead the engineers must shoulder the burden of maintaining massive linear scripts.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#notebooks-allow-out-of-order-execution-of-cells","title":"Notebooks Allow Out of Order Execution of Cells","text":"<p>One of notebooks greatest strengths, the ability to run code cell by cell, is also one of their greatest weaknesses. There is no guarantee that cells are run in the order in which they are defined within the notebooks, what's worse is that cells can be modified and re-run mid execution, leading to all sorts of weird state behaviors. While this can certainly be useful when experimenting it can lead to a more extreme version of the \"works on my machine\" syndrome which is \"works during my session\". Imagine a scenario where a a data scientist is working in a notebook, they run the first 5 cells and encounter and issue. In order to resolve the issue the go back an modify one of the earlier cells and re-run it. They then proceed to run the rest of the notebook without issue. Unfortunately, when they modified the pervious cell they accidentally deleted a critical import statement. This wouldn't affect their current session as the module was already imported from the previous cell run. However, time passes and another data scientist takes up the task of reproducing the previous scientist's work. They go to run the cells in the notebook and are surprised to find that it is not producing the correct output, it is throwing a missing import error instead.</p> <p>The above scenario in various forms is all to common when working with notebooks, the lack of guardrails to prevent entering into bad states means that reproducibility and consistency become very large challenges.  Notice how the above scenario didn't involve an engineer? This leads to another question:</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#are-notebooks-even-good-tools-for-doing-data-science-work","title":"Are Notebooks Even Good Tools for Doing Data Science Work?","text":"<p>In the above scenario, one data scientist had trouble reproducing the work of another data scientist. As reproducibility is one of the core tenets of any kind of scientific method, it begs the question, should we be using notebooks to do \"science\" work at all. In my opinion, probably not. I hope to expand on this idea in a future delve, but for now I'll simply say that when it comes time to re-train models (You are retraining your models to account for concept drift right?), for many of the same reasons notebooks present challenges for engineering teams to contend with, so to do similar challenges arise when model training code within notebooks need to be run.  Again, notebooks are excellent for exploration and experimentation, but when they are used to produce any kind of production artifact, whether that be the inference code or the model artifact themselves, they fall decidedly flat compared to plain old python code. </p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#the-hidden-cost-of-notebooks","title":"The Hidden Cost of Notebooks","text":"<p>While from an engineering perspective the issues described above may seem self apparent, and are often brought up in various engineering blogs. There is another hidden cost to using notebooks in production that I think is far more nefarious and severe, but is much less discussed: It creates friction and even a level of resentment between engineers and data scientists.</p> <p>Fundamentally the issue is that engineers need to better understand the day to day work of data scientists, and data scientists need to better understand the day to day work of engineers. They must work together to productionize a model after all. So often I see an \"over the wall\" dynamic develop at companies between data scientists and machine learning engineers. The data scientists produce some highly experimental code and toss it \"over the wall\" for the engineers to deploy. The engineers look at the code and realize that in order to deploy it they must completely re-write it from scratch in order for it to be maintainable. The data scientists become frustrated and dis-trustful of engineers because it takes so long to deploy their models. The engineers become frustrated and resentful of data scientists because they feel pressure to deliver as fast as possible and feel stymied by the code they receive from the data scientists.</p> <p>This dynamic can lead to a very non-functional organization with an \"us vs them\" mentality between scientists and engineers, which ultimately stifles innovation and affects the bottom line and causes talent attrition as individuals look for greener pastures!</p> <p>As a leader within an organization you are hopefully thinking \"That sounds terrible! How do I prevent that from happening? What is the path forward?\". Harmonious collaboration between scientists and engineers is possible, and when it is done well, I have seen it lead to tremendous unlocking of value. However, it requires re-thinking the relationship between data scientists and machine learning engineers.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#the-path-forward","title":"The Path Forward","text":"<p>As mentioned previously, the solution to stamping out this unhealthy dynamic is more awareness of job roles between data scientists and machine learning engineers. So scientists and engineers should talk to each other more, problem solved! Right? Well to some extent yes, certainly having more discussions between scientists and engineers is a good start, but if just talking to each other more would solve the problem, why does this seems to be such a pervasive issue when you do a cursory search online. The answer, perhaps unsurprisingly after reading this delve is simple: Notebooks.</p> <p>In order for scientists and engineers to truly collaborate during the model development and deployment process they must speak a common language and work with a common tool set. If they do not there will always be the metaphorical \"fence\". This means data scientists should strive to work in the same codebases as engineers and engineers should do everything they can to empower the data scientists to operate in their engineering codebases. While the idea of giving a data scientist access to your code may at first make an engineer's skin crawl, the benefits over the long term will be numerous. Once data scientists exit the exploratory phase of their projects (notebooks) they can work hand and hand with machine learning engineers to productionize their models. This gives the scientists a sense of empowerment and control \"I'm not just waiting for the engineers to hurry up and finish\" and also gives them visibility into the engineering process, which should naturally lead to them producing code that is more easily productionizable. The engineering teams feel less pressure to deliver code, because now they are receiving help from the scientist who originally wrote it, and ultimately higher quality code is produced and released faster, driving the innovation cycle.</p> <p>To conclude, the goal was never about putting notebooks in production, that's a distraction, it's to empower data scientists and include them in the responsibility of delivering functional software that delivers value to the business. Shifting away from notebooks for anything but exploration and experimentation is the most efficient mechanism to achieve this in my opinion.</p>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#additional-reading","title":"Additional Reading","text":"<ul> <li>Don't put data science notebooks into production - David Johnston provides an excellent perspective on why notebooks in production create several hurdles for engineering teams to solve</li> <li>Why (and how) to put notebooks in production - Eduardo Blancas provides a counterpoint of an approach to use notebooks in production</li> <li>Jupyter Notebooks in production......NO! JUST NO! - Reddit thread on <code>r/datascience</code> where several individuals offer their opinions both for and against using notebooks in production</li> </ul>","tags":["Opinion"]},{"location":"2023/12/10/delve-1-the-hidden-danger-of-notebooks-in-production.html#delve-data","title":"Delve Data","text":"<ul> <li>Notebooks are an excellent tool for data exploration and model experimentation</li> <li>Notebooks have several engineering drawbacks when used as a mechanism for deploying models to production</li> <li>Using notebooks in production can lead to distrust and resentment between data science and engineering teams</li> <li>Having data scientists shift away from notebooks once experimentation has concluded and into the same codebases and tooling as the engineering teams is the best way to reduce friction and increase delivery frequency of model pipelines </li> </ul>","tags":["Opinion"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html","title":"Delve 2: My Recommended Machine Learning &amp; Data Science Resources!","text":"<p>\"If I have seen further than others, it is by standing upon the shoulders of giants.\" - Isaac Newton</p>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#my-go-to-list-of-machine-learning-data-science-resources","title":"My Go To List of Machine Learning &amp; Data Science Resources","text":"<p>I have often been asked what resources I recommend for those looking to get into machine learning, whether you want to be a data scientist or ml engineer. In this delve I'll cover my go to list of resources I continue to rely on whenever I need to refresh my own knowledge or delve deeper into a specific subject matter.</p>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#books","title":"Books","text":"<p>I have found that books are an excellent way for me to absorb knowledge. I still enjoy having a physical book shelf I can refer to, books I can add sticky notes in, and physical copies I can lend out to others to read. If you enjoy learning from books, these are my go to list in no particular order:</p> <ul> <li> <p>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur\u00e9lien G\u00e9ron - This is easily the best reference book I have come across for learning the basic (and advanced!) concepts of machine learning, equivalent to several semesters of graduate level coursework. The first half of the book covers \"classical\" methods of machine learning such as linear models, SVM, and trees, while the second half of the book is fully devoted to neural networks. I have run several books clubs using this book and have had great success using it as a reference material whenever I need a refresher on a particular concept. My only minor criticism is that the second half of the book utilizes Tensorflow over PyTorch (where I have seen the industry trending to over the last few years) for implementing neural networks, however all of the concepts are easily transferable between frameworks. </p> </li> <li> <p>Practical DataOps Delivering Agile Data Science at Scale by Harvinder Atwal - If you are interested in learning more about the process of utilizing machine learning as part of a product, and the software delivery management best practices involved look no further than this book! Whether you are an individual contributor or the director of several teams, there is something for every practitioner in the ML space. Whenever I join a new organization, this is the first book I recommend to the leadership team to read. I have not found anything else that comes as close to perfectly describing the challenges involved in putting machine learning into products and how to overcome them.</p> </li> <li> <p>Feature Engineering for Machine Learning by Alice Zheng &amp; Amanda Casari - A more specialized book once you have a grasp of machine learning fundamentals, this book is a great reference for specific feature engineering techniques. Whenever I have a specific type of data, be it numerical, categorical, text, or images, this books has a reference for the appropriate feature engineering techniques available.</p> </li> <li> <p>The Lean Startup by Eric Ries - Classic book on how to correctly ideate through an invocation cycle, find out what works, and pivot when needed. Not specifically an ML book, however its concepts of trying new ideas, measuring their impact, and adjusting are just as applicable when testing out new ML models.</p> </li> </ul>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#blogs-newsletters","title":"Blogs &amp; Newsletters","text":"<p>Blogs and newsletters are some of the best ways I've found to stay up to date on the latest developments, research, and industry trends. Here are some of my favorites to follow:</p> <ul> <li> <p>DataDelver - It's my hope that this blog becomes an invaluable resource for the world of ML Ops, as informative as some of the other blogs on this list!</p> </li> <li> <p>ruder.io - Sebastian Ruder's excellent blog/newsletter with a focus on Natural Language Processing (NLP) research. My go to source for the latest NLP research developments.</p> </li> <li> <p>Pycoders Weekly - Not specifically focused on machine learning, but an excellent weekly newsletter on all things python, I've found quite a few nuggets of knowledge or useful packages following this newsletter!</p> </li> <li> <p>Zillow - The Zillow AI blog is an excellent resource for examining how machine learning is applied at a large organization, particularly in the realm of recommendation and personalization. This post in particular is one of my favorites!</p> </li> <li> <p>AirBnB - The AirBnB blog is another excellent resource for gaining an industry perspective into utilizing machine learning. I always enjoyed this post on computer vision!</p> </li> </ul>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#courses","title":"Courses","text":"<p>I have not relied on many online courses within my career, instead preferring books or blogs, but here the the ones I have personally gone through and gotten value from:</p> <ul> <li>Building Recommender Systems with Machine Learning and AI - Excellent foundational course on building recommendation systems. Covers the traditional approaches for building recommendation systems such as content-based filtering, collaborative filtering, and matrix factorization. Also covers deep learning approaches. Importantly, it spends a great deal of time on how to properly evaluate recommendation systems. Lots of great content!</li> </ul>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#reddit","title":"Reddit","text":"<p>Finally here are some of the subreddits I follow to stay up to date:</p> <ul> <li> <p>r/MachineLearning - Generalist subreddit on all things machine learning, with a particular focus on research.</p> </li> <li> <p>r/datascience - Generalist subreddit on all things data science, with less emphasis on research and more beginner friendly.</p> </li> <li> <p>r/LanguageTechnology - Natural language processing focused subreddit, if you need to process text this one's for you!</p> </li> <li> <p>r/StableDiffusion - If you want to get started using GenAI models to create your own images start here! Also see what other people have been able to generate!</p> </li> <li> <p>r/dataisbeautiful - Great resource for learning techniques for one of the trickiest parts of machine learning, visualizing the data in a way people can understand.</p> </li> </ul>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#conclusion","title":"Conclusion","text":"<p>That's my list of resources, I hope some of them are useful to you all on your own delves! Thank you to all of the people that contribute to them, I know they've certainly made my own journey much easier! With this delve I close out 2023, I look forward to many more delves in 2024!</p>","tags":["Resource"]},{"location":"2023/12/23/delve-2-my-recommended-machine-learning--data-science-resources.html#delve-data","title":"Delve Data","text":"<ul> <li>There are lots of great resources out there for learning about machine learning, data science, and MLOps, many of which are free.</li> <li>I hope this blog becomes such a resource for you!</li> <li>Stay tuned for more delves in 2024!</li> </ul>","tags":["Resource"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html","title":"Delve 3: Fun Linux Utilities","text":"<p>\"People rarely succeed unless they have fun in what they are doing\" - Dale Carnegie</p>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#nix-the-workhorse-of-mlops","title":"*nix the Workhorse of MLOps","text":"<p>Welcome to 2024 data delvers! I hope you had a wonderful holiday season! As we enter into the new year I'd like wanted to take some time to talk about things that make my day to day as a developer fun! As I hope to get into in future delves, for many reasons I prefer a nix (Unix or Linux) based environment for doing development. Many people use MacOS as their nix environment of choice, however my preferred method of achieving this in recent years has been the Windows Subsystem for Linux (WSL). It is super easy to set up on any modern windows machine, integrates very nicely with my IDE of choice, Visual Studio Code, and avoids many of the dangers associated with partitioning your hard drive to dual-boot your machine with multiple operating systems. As a bonus, when working with cloud providers you are almost always deploying your model on a Linux server, so mirroring that same environment on your dev machine makes everything that much smoother.</p> <p>Importantly for our delve today, WSL provides a full bash shell to interact with the OS. Over the years many a Linux developer has created fun and useful utilities to use within a shell, some of which I'd like to share with you today!</p>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#text-based-utilities","title":"Text-Based Utilities","text":"<p>The following utilities work by printing text out to your terminal.</p> <ul> <li> <p>cmatrix - This has long been a staple of my dev machines, if you ever wanted to feel like a hacker in \"The Matrix\", this utility will simulate the iconic terminal screens from the film.</p> <p></p> </li> <li> <p>cbonsai - In a similar vein to cmatrix, cbonsai procedurally generates a bonsai tree within your terminal.</p> <p></p> </li> <li> <p>fortune - A very old, but still fun utility. Fortune prints a random fortune every time it is executed.</p> <pre><code>$ fortune\nQ:      Why was Stonehenge abandoned?\nA:      It wasn't IBM compatible.  \n</code></pre> </li> <li> <p>cowsay - Another old, but very fun utility. Cowsay allows you to make an ascii art cow (or other animal) say a phrase of your choice.</p> <pre><code>$ cowsay Hello world!\n ______________\n&lt; Hello world! &gt;\n --------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>For more fun you can pipe fortune into cowsay to get your animal to give you a random fortune.</p> <pre><code>$ fortune | cowsay -f tux\n _____________________\n/ Big book, big bore. \\\n|                     |\n\\ -- Callimachus      /\n ---------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n</code></pre> <ul> <li>wttr.in - Allows you to get a weather report directly in your terminal.</li> </ul> <pre><code>$ curl wttr.in\nWeather for City: Paris, France\n\n     \\   /     Clear\n      .-.      10 \u2013 11 \u00b0C\n   \u2015 (   ) \u2015   \u2191 11 km/h\n      `-\u2019      10 km\n     /   \\     0.0 mm\n</code></pre> </li> </ul> <p>For each of these utilities (except cmatrix!), you can add them to the Message of the Day (motd) shown when you first log in to your shell by following these steps:</p> <p>Note: This assumes you are using the Ubuntu distribution in WSL.</p> <ol> <li>Navigate to <code>/etc/update-motd.d</code></li> <li>Create a file called <code>01-custom-message</code></li> <li>Paste any commands you like to be run in this file and save it.     <pre><code>#!/usr/bin/env bash\n\nfortune | cowsay -f tux\n</code></pre></li> <li>Make the file executable <code>$ sudo chmod +x 01-custom-message</code></li> </ol> <p>Now when you log into your machine you'll get your own custom message!</p> <pre><code>Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.133.1-microsoft-standard-WSL2 x86_64)\n ______________________________________\n&lt; Today is what happened to yesterday. &gt;\n --------------------------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n</code></pre>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#audio-visualizer-utilities","title":"Audio Visualizer Utilities","text":"<p>Another fun category of shell utilities are audio visualizers. I find these really fun to use when playing a music playlist and wanted to have some kind of display for the music. The two utilities I use for this are:</p> <ul> <li> <p>cava - Very easy to install and set up, runs on pretty much anything, and does exactly what it says it does, providing a configurable audio spectrum visualizer.</p> <p></p> </li> <li> <p>cli-visualizer - This one takes a bit more effort to set up but provides many fancy audio visualizations such as lorenz visualizations.</p> <p></p> </li> </ul> <p>Note: These won't work within WSL when playing audio on Windows, but work great if you install a browser like Chrome into WSL and play audio through that.</p>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#system-information-utilities","title":"System Information Utilities","text":"<p>Finally, there's a few utilities I like to use to print info about by system. These are:</p> <ul> <li> <p>neofetch - Prints out basic system information in a nice format.</p> <p></p> </li> <li> <p>hwinfo - Allows you to query for very specific system information.</p> <pre><code>$ hwinfo --short --disk --cdrom\ndisk:\n    /dev/sda             WDC WD10EARS-00Y\n    /dev/sdb             ST2000DM001-1CH1\ncdrom:\n    /dev/sr0             PLDS DVD+-RW DS-8ABSH\n</code></pre> </li> </ul>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#conclusion","title":"Conclusion","text":"<p>Hopefully you find some of these utilities fun and make working in your *nix environment of choice that much better! As we enter the new year stay tuned for more delves into more practical utilities for MLOps workflows!</p>","tags":["Tools","Fun"]},{"location":"2024/01/28/delve-3-fun-linux-utilities.html#delve-data","title":"Delve Data","text":"<ul> <li>A *nix (MacOS or Linux) environment is the preferred development environment for MLOps.</li> <li>Windows Subsystem for Linux (WSL) is a good option for getting this type of environment set up on a Windows machine.</li> <li>There are a number of fun shell utilities that you can install in these environments!</li> </ul>","tags":["Tools","Fun"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html","title":"Delve 4: The ML Engineer, Coming to an Enterprise Near You","text":"<p>\"Life is like riding a bicycle. To keep your balance, you must keep moving.\" - Albert Einstein</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#who-am-i","title":"Who am I?","text":"<p>Hello data delvers! I hope your year is off to a good start! For this delve I wanted to cover a question that I get asked often, especially whenever I meet someone new, the dialog usually goes something like this:</p> <p>Me: \"Hi I'm Chase, nice to meet you!\"</p> <p>Other Person: \"Hello Chase, it's nice to meet you too! I'm \\&lt;Insert Name Here&gt;. I'm a \\&lt;Insert Profession Here&gt;. What do you do for work?</p> <p>Me: \"Oh! I'm a machine learning engineer!\"</p> <p>Other Person: \"Oh that's neat... What's a machine learning engineer?\"</p> <p>Ok, the conversations aren't usually that contrived and with the explosion in popularity of ChatGPT more and more people have heard the term machine learning than ever before but you understand the point. Those of you that have read the about section of this blog will notice that (at the time of this writing) I call myself a machine learning engineer, but what does that profession actually entail? How does it differ from other professions in the data science and machine learning space such as data engineer or data scientist? Is it any different from a software engineer?  </p> <p>In this delve, I intend to answer these questions and more by first providing a brief overview of the traditional roles in the \"Big Data\" space, introducing the emerging role of the machine learning engineer, and finally providing some commentary on how I think this specialization can most effectively be utilized within an enterprise.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#traditional-roles-in-big-data","title":"Traditional Roles in Big Data","text":"<p>When we think of the world of data science and machine learning, often referred to as \"Big Data\", historically three distinct roles emerge:</p> <ul> <li>Data Analyst</li> <li>Data Scientist</li> <li>Data Engineer</li> </ul> <p>These roles have been the pillars on which many enterprises have built the foundation of their success in deriving value from their vast quantities of data. Understanding the responsibilities of these roles is crucial to leveraging them efficiently and effectively to drive business value. So what are they?</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#data-analyst","title":"Data Analyst","text":"<p>Arguably the profession that has been around the longest, data analysts specialize in collecting data from a wide variety of company sources and distilling it into reports, charts, and visualizations that stakeholders can use to inform business decisions.</p> <p>Data analysts usually spend most of their time shifting through the enterprise's data lake and/or warehouse to answer a specific business question or provide an insight into a trend or pattern that is being observed in the business. </p> <p>The typical tools and technologies I have seen analysts rely on are data query languages like SQL or SAS, and data visualization suites such as PowerBI or Tableau.</p> <p>The biggest strength of the business analyst is the ability to understand the context of a business problem and present just the right amount of data in order to solve it.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#data-scientist","title":"Data Scientist","text":"<p>Deemed the \"Sexist Job of the 21st Century\" by the Harvard Business Review, data scientists use their advanced knowledge of statistics and machine learning to build and evaluate predictive models on data and solve complex problems.</p> <p>Data scientists often spend much of their time in the same environments as data analysts, the data lake/warehouse. However, where the analysts focus on producing visualizations of the data, data scientists often focus on cleaning and organizing data to feed into their models. There's an often quoted statistic that \"80% of a data scientists time is spent cleaning data\". Due to the sensitivity of many modeling techniques to poor quality data, I can attest that this is indeed unfortunately often true. Once a cleaned collection of data has been produced, often called a training set, the data scientist will then use various machine learning techniques to attempt to build a model on that data which can be used to solve the business problem. Importantly, once a model has been trained, the data scientist will then evaluate that model with various statistical methods to ensure it is reasonably robust.</p> <p>Tools and technologies data scientists use are programming languages such as R and Python along with the machine learning packages contained within them, big data processing frameworks such as Spark, and computation environments such as Jupyter notebooks (although if you've read my previous delve, you may understand why this isn't the best idea).</p> <p>The biggest strength of the data scientist is their advanced knowledge of machine learning that enables them to solve complex problems.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#data-engineer","title":"Data Engineer","text":"<p>If you noticed, the last two roles depend on having all of the data available in a centralized location, often referred to as a data lake or warehouse. (Technically a lake and warehouse are not the same thing but the distinction is unimportant for this discussion.) It is the job of the data engineer to set up the pipelines to feed data into this centralized location.</p> <p>Data engineers spend much of their time communicating with other engineering teams setting up ingestion processes for their data. Their primary function is to create ETL (Extract Transform Load) jobs to move and cleanse data between different systems. Often these jobs are run on a recurring cadence and take the form of python scripts, either within notebooks or plain files. </p> <p>Tools and technologies in this specialization include many of the same technologies data scientists use, Python and Spark, but the emphasis is on moving and transforming data rather than building ML models. In addition, job schedulers such as Airflow can be leveraged to manage ETL jobs. </p> <p>The data engineer's biggest strength is a deep technical understanding of how to process very large quantities of data at scale quickly and efficiently. </p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#traditional-enterprise-big-data-role-organization","title":"Traditional Enterprise Big Data Role Organization","text":"<p>Taking these roles, enterprises in my experience typically arrange them in the follow way:</p> <p>Figure 1: A Typical Enterprise Big Data Role Organization</p> <p></p> <p>The centralized component is the data lake. Data engineers focus on hydrating the lake with the data required by the data scientists and analysts. The scientists and analysts in turn leverage the data to produce their work artifacts. This setup is extremely effective at producing models and reports that can be used to inform business decisions and deliver business value however there is a catch: What if instead of a data scientist or analyst communicating their results to a business stakeholder, we want the stakeholder to interact with the output of their work product directly, or even have an external customer interact with it? </p> <p>It is at this point this organization breaks down. How do we take an ML model and turn it into something that our front end applications can consume? What happens if we need the data in real time rather than in a batch ETL job that runs every 30 mins? How can we scale to have every one of our customers interacting with this model at the same time?</p> <p>For those of you that have read some of my previous delves, this is what I call the \"Production Problem\". Suddenly an ad-hoc report is no longer enough, the model needs the support of engineers to deliver business value.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#the-production-challenge","title":"The Production Challenge","text":"<p>The challenge at this point is that none of the roles in the traditional big data setup have the skills or experience to solve this dilemma. Where I've seen organization struggle is when they ask the data engineers, scientists, and analysts to solve this problem themselves. What ends up happening is that the individuals in these roles fall back on the technologies they are familiar with. Data engineers try to make ETL jobs run faster, data scientists string together notebooks in \"pipelines\" to generate the features the models need, analysts aren't really sure where they fit into a production system at all.</p> <p>As a consequence, the resultant system usually turns out to be very expensive and inefficient to run if it works at all, the individuals that built it are frustrated being asked to deliver functionality they are not equipped to maintain, and the business value delivered suffers greatly as a result.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#data-big-and-small","title":"Data Big and Small","text":"<p>When we think of training an ML model we often discuss the challenges of Big Data, of building ETL pipelines to move large batches of data between storage appliances to facilitate model training, however there is an equally challenging and often overlooked problem space of sending data to the model at inference time. Sometimes referred to as the last mile problem, this data is Small Data often individual samples of data, not the large batches of data the model was trained on. This challenge is often where I have seen enterprises tripped up. Why? Simply put, the tools, skills, and expertise to solve challenges in this space are completely different, they are much more akin to conventional software engineering challenge than big data ETL pipelines. </p> <p>What is needed is a new type of engineer, one which specializes in the last mile of getting ML models into a production environment.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#enter-the-machine-learning-engineer","title":"Enter the Machine Learning Engineer","text":"<p>The machine learning engineer, sometimes called an MLOps engineer, is the missing piece of this puzzle. Unlike data engineers, they are more familiar with conventional software engineering, and capable of turning analytic insights and models into services that can be consumed by the larger enterprise as well as having the ML knowledge and experience to interface with data scientists and understand the inference requirements of their models.</p> <p>In one of my favorite articles on this subject, Data Engineers vs. Data Scientists, author Jesse Anderson presents a juxtaposition between the skill sets of data scientists and engineers in the following graphic:</p> <p>Figure 2: Diagram illustrating the core competencies of data scientists and data engineers and their overlapping skills. Image Credit: Jessie Anderson</p> <p></p> <p>He then presents the machine learning engineer as the middle ground between these two extremes:</p> <p>Figure 3: Diagram illustrating where a machine learning engineer fits with a data scientist and data engineer.  Image Credit: Jessie Anderson</p> <p></p> <p>While I adore this visualization and agree with many of the points raised in the article, particularly about how to correctly staff resources surrounding ML (Business leaders: Don't hire a bunch of data scientists and expect \"magic\" to happen without engineering support!), when I think about my own career as someone who calls themselves a machine learning engineer this narrative of the data engineer who cross trains on data science side to end up as a hybrid between the two roles doesn't particularly resonate with me. While it is tantalizingly elegant to place the machine learning engineer between a data scientist and data engineer I have found that the types of problems I tend to work on and solve day to day have much more in common with conventional software engineering than those I typically see data engineers work on, with some additional data science complexity added on top.</p> <p>If I adopt the same overlapping skill diagram I find the following to be a much better representation of the type of work I do:</p> <p>Figure 4: Diagram illustrating where a machine learning engineer fits with a data scientist and software engineer.</p> <p></p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#a-software-engineer-with-data-science-know-how","title":"A Software Engineer with Data Science Know-How","text":"<p>When I think about the types of problems that I solve as an ML engineer they are usually formulated as something like this:</p> <p>In response to an event (http request, or streaming event), fetch some data and send it to a model for inference and publish the result (either as an http response, or to a downstream event consumer).</p> <p>A more concrete example might look something like this:</p> <p>When the customer logs into our system (the event), grab their profile (the data), and display a personalized welcome message from our model on their homepage (the result).</p> <p>If you think to yourself, \"Hey that sounds like something a typical microservice architecture could solve!\" you'd be right! As mentioned previously, the scale of the data needed at model inference time is typically not the scale of data at the time the model was trained. It's single samples at a time not the large batches of data the model was trained on. It is true that there are production use cases for batch inference, but in my experience it's almost always better to go with a streaming-based approach as you can fairly easily process records in batch through a streaming system (by batching the event consumer) but the inverse is not true.</p> <p>The Machine Learning Engineer is the specialist in this sphere. They have the software engineering chops to build scalable and sustainable microservice architectures to deploy ML models as capabilities other engineering teams can leverage. They also have the Data Science know-how to interface with the data scientists, understand their requirements, and help develop the model training code into an engineering artifact. </p> <p>Another way to look at the difference between an ML Engineer and a Data Engineer is what their work product is and who consumes it:</p> <ul> <li>A Data Engineer produces ETL Pipelines the output of which is consumed by Data Scientists and Data Analysts</li> <li>A ML Engineer produces Models as a Service the output of which is consumed by Other Engineering Teams</li> </ul> <p>Figure 5: Diagram illustrating the differences in work product and consumers between Data Engineers and ML Engineers</p> <p></p> <p>As you can see illustrated above, based on the who the end consumer of their work product is, it makes sense that data engineers and ml engineers would have different skill sets and strengths. The real value-add of the ML engineer is the ability to take the output of the data scientist (models) and turn it into a product, that is a service that other engineering teams can consume. Many ML engineers also have the data science know-how to build ML models themselves, which presents an opportunity for a shift within the industry.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#the-future-of-ml-engineers-and-mlops","title":"The Future of ML Engineers and \"MLOps\"","text":"<p>With the barrier to entry for building ML models continuing to become lower, the task of training a model that once took a highly specialized data scientist months of effort complete is often now being reduced to a matter of calling <code>model.fit()</code> in whatever ML library of choice you'd like to use. Even the highly complex Large Language Models (LLMs) that have taken the world by storm over the past year are often being exposed behind convenient APIs that any ML engineer should be able to reasonably utilize (creating a whole sub domain of MLOps know as LLMOps). This poses the question then, instead of having a data scientist spend much time and effort producing a model in the form of a Jupyter notebook and then have the ML engineer translate it into \"production\", wouldn't it be more efficient to have the ML engineer write the code from the start?</p> <p>It is this thought that I have seen slowly shift the approach enterprises' take in staffing their data science and ML departments. The days of the data scientist producing notebooks and tossing them over the wall to engineering are coming to an end. While there will continue to be a place for data scientists doing experimentation and research within the enterprise, there will be fewer of them, with more of the burden of training and deploying models shifting to ML engineers. As such, I expect the demand for ML engineers to continue to increase in the coming years, and be accelerated by the LLM boom under the banner of the LLM engineer. What does this mean if you are in one of the other roles in the \"Big Data\" space?</p> <p>If you are a data scientist or data engineer it means you should start learning more conventional software engineering practices. If you are data analyst, learn to start leveraging the output of the ML services created by ML engineers as part of your analysis and A/B testing. Though cliche, it is true that the only constant in this industry is change and if they last year has taught us anything, it's that change is only happening faster than ever before.</p>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#additional-reading","title":"Additional Reading","text":"<ul> <li>Data Engineers vs Data Scientists - The previously mentioned article by Jessie Anderson that introduces the concept of the ML engineer and where they fit in a business organization</li> <li>What is a Machine Learning Engineer? - Principal ML Engineer David Hundley provides his perspective on what the role is</li> <li>[D] Is Data Science Dead? A discussion thread on <code>r/datascience</code> regarding the future of data science as a career path and the industry trend of shifting towards ML engineering</li> </ul>","tags":["Opinion"]},{"location":"2024/04/27/delve-4-the-ml-engineer-coming-to-an-enterprise-near-you.html#delve-data","title":"Delve Data","text":"<ul> <li>Historically the three major roles in big data have been Data Analysts, Data Engineers, and Data Scientists</li> <li>In the last few years a new role, ML Engineer, has emerged specializing in turning ML models into products</li> <li>ML Engineers have the potential to take over much of the work of data scientists while reducing the burden of translating notebook code into production applications</li> </ul>","tags":["Opinion"]},{"location":"2024/06/29/delve-5-the-quest-for-a-full-screen-raspberry-pi-application.html","title":"Delve 5: The Quest for a Full Screen Raspberry Pi Application","text":"<p>\"All life is problem solving.\" - Karl Popper</p>","tags":["Tools","Fun"]},{"location":"2024/06/29/delve-5-the-quest-for-a-full-screen-raspberry-pi-application.html#full-screen-applications-on-the-raspberry-pi-why-so-hard","title":"Full Screen Applications on the Raspberry Pi, Why so Hard?","text":"<p>Hello data delvers! Apologies for the lack of updates, life has been busy! For today I have a quick delve on a frustrating problem I had to solve, longer delves are on the way!</p> <p>Like I'm sure many of you, I greatly enjoy doing side projects on the Raspberry Pi mini computer. If you've read my previous delve, you'll know there are lots of fun utilities you can run on a linux machine. One such application I like to use my Raspberry Pi for is to stream music from web services such as Spotify to my television and then use an audio visualizer like those covered in my previous delve to provide a visual.</p> <p>This worked great in combination with making the terminal full screen so that the only thing visible on the screen was the audio visualizer. Previously this was as simple as executing the keyboard shortcut Alt+F11. However, I noticed when I updated my Linux distribution from Debian 11 \"Bullseye\" to Debian 12 \"Bookworm\" the full screen command shortcut stopped working! Confused I thought some configuration got messed up, doing some quick internet searching was not yielding any helpful results and so I resolved myself to live with unmaximized applications for now...</p>","tags":["Tools","Fun"]},{"location":"2024/06/29/delve-5-the-quest-for-a-full-screen-raspberry-pi-application.html#whats-going-on","title":"What's Going On?","text":"<p>A few months go by and I recently got the itch to tinker with my Raspberry Pi again. I wanted to stream some music and was confronted with the same issue of no full screen! Picking up the trail searching I saw that one of the big differences in the new distribution was the switch to using Wayland as the display system. \"Ah ha!\" I thought, \"Maybe the shortcuts have changed in the new display!\". More searching later I eventually stumbled upon this guide on the Raspberry Pi forums discussing how to make the new distribution more keyboard centric. Buried in the configuration options it discusses is a modification for windows settings in <code>~/.config/wayfire.ini</code>:</p> <p><pre><code># Actions related to window management functionalities.\n[wm-actions]\ntoggle_maximize = &lt;super&gt; KEY_F\ntoggle_fullscreen = &lt;super&gt; &lt;shift&gt; KEY_F\n</code></pre> Excitedly, I modified my configuration file to add the shortcuts for the above actions and rebooted my Pi. Upon rebooting, I opened a terminal and executed the shortcut and \"Tada!\", my application was now full screen again!</p> <p>Why this functionality was removed in the new distribution by default I can't say, it also seems to be conspicuously undocumented in the official sources, but I'm glad to have it back. It's my hope that this delve will help others stuck with the same issue overcome it faster than I did!</p> <p>That's all I have for now, expect some more ML engineering delves soon!</p> <p>As a side note, I have to check out some of the other shortcuts mentioned in that guide!</p>","tags":["Tools","Fun"]},{"location":"2024/06/29/delve-5-the-quest-for-a-full-screen-raspberry-pi-application.html#delve-data","title":"Delve Data","text":"<ul> <li>The Alt+F11 full screen shortcut was removed in the Debian 12 \"Bookworm\" distribution of the Raspberry Pi OS</li> <li>The solution is to add a new shortcut to the <code>wayfire.ini</code> file</li> </ul>","tags":["Tools","Fun"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html","title":"Delve 6: Let's Build a Modern ML Microservice Application - Part 1","text":"<p>\"The beginning is the most important part of the work.\" - Plato</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#a-new-year-a-new-start","title":"A New Year, A New Start","text":"<p>Hello data delvers! Happy New Year! I hope you all have been well! It's been some time since our last delve (sorry!) but I want to kick off 2025 with a new series of delves I've been wanting to do for some time, building out an ML application using modern microservices principles.</p> <p>In this multi-part series we'll focus on the tools, techniques, and strategies I use to bring ML applications to life in a maintainable, scalable, and extendable way. If that sounds of interest to you, put on your delving gear and join me as we go deep into the depths of the labyrinth!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#ml-redefining-application-development","title":"ML, Redefining Application Development?","text":"<p>To begin this delve I'm going to start by stating that for part one I am not going to be discussing machine learning at all, only core software engineering principles. \"What? How can you talk about building ML applications and not ML at all!\" I could hear you say. Firstly, I will talk about ML eventually, however the main point I want to raise here is there is nothing special about ML that invalidates good software engineering practices. I think this is such an important point that I will state it again, repeat with me:</p> <p>There is nothing special about ML that invalidates good software engineering practices.</p> <p>This is something I have seen even experienced practitioners get tripped up on. Particularly those that have come from a software engineering background but have never worked with ML before. They join a team, see all the data scientists working in notebooks and see everyone else nodding their heads and working to productionize those notebooks and think \"Gee, this doesn't seem to make sense to me but this is what everyone else is doing so it must be right.\" and go along with the herd. I'm here to say that you are right to feel like something is off.</p> <p>Historically the data science community has not come from a software engineering background, but one of statistics and analytics. As a result, a whole crop of tools and companies have sprung up to make productionizing their work \"easy\". Sometimes even advocating for running notebooks in production as \"best practice\". If you've read my previous delve on the subject you know that this approach is full of pitfalls.</p> <p>In addition to the above drawbacks, the mentality of \"ML is special, it requires a different paradigm to productionize\" forces us to ignore all of the wonderful best practices that have developed over the past several decades of software engineering to improve reliability, testability, and extendability (Object Oriented Programming is over 50 years old at this point, I don't think it's going anywhere). As such, instead of asking \"How do we modify our software development best practices to accommodate ML?\" I prefer to ask opposite question: \"How can we develop ML capabilities in a way that can benefit from all the best practices of software engineering?\". It is this approach that I intend to begin to explore in this delve.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#preparing-the-excavation-site","title":"Preparing the Excavation Site","text":"<p>Hopefully by now I have piqued your interest in this approach, which means we now need to prepare a workspace to begin developing it. The first choice we must make is what operating system to develop on, which is influenced somewhat by our choice of programming language.</p> <p>As you are probably already aware, Python has become the lingua franca of the Data Science and Machine Learning world. According to the 2024 Stack Overflow Developer Survey, a whopping 51% of respondents stated they work with Python. Compare this to R the other main language used for modeling at only 4.3% and it should be clear Python is currently enjoying a resounding popularity.</p> <p>While it is possible to develop Python natively on a Windows operating system, it has not been in my experience very pleasant, with most developers preferring either a macOS or Linux environment to develop in. However, if you are on a Windows machine before you go dual-booting your hard drive to install Linux you should definitely consider the Windows Subsystem for Linux (WSL), which allows you to run a full Linux kernel on your Windows machine. This is actually my preferred development setup and the one I will be using for this series.</p> <p>With the operating system out of the way, the next major choice to make is which Integrated Development Environment (IDE) to choose. For a long time my answer here would have been PyCharm as it was the most feature rich and complete. However, in recent years Visual Studio Code (VSCode) has taken the development world by storm, rising to 73.6% of respondents using it in the same 2024 Stack Overflow Developer Survey. It also has the added benefit of being completely free to use. Given this, I will be using VSCode as my IDE, though most of what we cover should be IDE agnostic.</p> <p>With that all out of the way, let's boot up VSCode and get to coding!</p> <p>Except, when you first boot up VSCode it won't be equipped to develop Python. Head over to the extensions tab and grab the official Python Extension from Microsoft and if you are on Windows and want to use WSL, grab the WSL extension too (make sure to check out the extension instructions as to how to open a VSCode instance connected to WSL).</p> <p>Open up a new Terminal in VSCode a do a quick Python Hello World to verify everything is working as expected:</p> <p></p> <p>One thing to note here, there are plenty of good resources out there to learn Python, https://www.learnpython.org is unironically a good one, so for this series I'm going to assume you have some basic Python knowledge already but I'll point out specifics that I find valuable. </p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#moving-in-the-digging-equipment","title":"Moving in the Digging Equipment","text":"<p>If you've ever worked with Python one thing you probably know already is that managing dependencies is hard! Trying to make sure you have all the right versions of your dependencies installed and they don't conflict with each other has been a recurring challenge in the Python ecosystem. Up until recently, there hasn't be a good tool that manages all of this well. Previously I used a amalgamation of pyenv, pipenv, and some custom setup scripts to manage it all. While this worked it was finicky and brittle. Some other tools like poetry came along but I never really found it compelling enough to switch. That all changed with uv. Simply put, this is the best tool out there, and it does it all: managing different versions of Python on your machine, blazing fast dependency resolution, and distribution packaging to boot. It has easily replaced 5-6 tools in my workflow. Needless to say, I will be using uv for all my projects going forward.</p> <p>While we are on the same topic of Python tools, the same company that maintains uv also has another tool: Ruff which is great for auto-formatting your python code so that it meets your stylistic standards. This again replaced several tools in my existing workflow and it is available as a VSCode Extension. Not a hard requirement but I highly encourage using it as well. Not having to argue with other engineers on the correct way to style code and just having a tool do it for you has saved me countless headaches.</p> <p>Finally, you should hopefully already have Git installed on your machine. You are version controlling your code aren't you?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#start-digging","title":"Start Digging","text":"<p>Ok! That should be enough setup for now, let's get coding! Open up a shell and type <code>uv init modern-ml-microservices</code> this will create a new starter project directory for us to work with. You can then open up this directory in VSCode by typing <code>code modern-ml-microservices</code> and we're up and running!</p> <p></p> <p>To start let's take a look at the <code>pyproject.toml</code> file. As defined by PEP 621, this file is the modern standard for storing our project configuration. It includes things like what version of python our project is compatible with and what its dependencies are. When starting out there are a few things I like to change like adding your own project description and configuring any tool options:</p> <pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = []\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.format]\nquote-style = \"single\"\n</code></pre> <p>One of the things I like about Ruff is it has several configuration options that you can adjust to suit your preferences. For example, I like to use single quotes <code>'</code> for denoting strings in Python and have a max line length of 120 characters. You can tweak these settings to your desire but they should be consistent across your projects and teams. </p> <p>Next we can work with our Python code directly. We can start by deleting <code>hello.py</code>, we won't be using it. We it comes to managing your source code there are a few different philosophies, however I subscribe to the idea that source code should be kept in a separate directory for project files. This becomes important as the project base grows and the complexity of the code increases. To that end create a directory called <code>src</code> and a file called <code>main.py</code> within it. This we become the main script for our application, but what will our application do?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#apis-and-microservices-the-core-tools-in-your-belt","title":"APIs and Microservices, the Core Tools in your Belt","text":"<p>It used to be in years past, all the features of a application were created in one code base and the application was deployed as a single unit (the so called Monolith architecture). As applications began to grow and their complexities increased the Microservice Architecture Pattern developed to support these more complex applications. The core concept of this architecture are small, independently deployable services each with ownership of a single business capability. The services communicate with each other over the network using APIs and together form the whole application. As we'll come to see, this architecture pattern lends itself very well to the complexities of an ML powered application.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#start-building","title":"Start Building","text":"<p>For this initial application we are going to be using the free public API provided by the Metropolitan Museum of Art. For or starting task we'd like to search the museum's collection for an artwork and if found, get a link to an image of the artwork.</p> <p>We can see that this should be possible in the API, we can search the collection using the <code>/public/collection/v1/search</code> route. We can also get the details of a specific object in the collection, including its primary image, using the <code>/public/collection/v1/objects/[objectID]</code> route. However, we can't get all the information we need from a single call. Therefore the business logic we need to create is:</p> <p><pre><code>GIVEN: The name of an artwork\n\nWHEN: That artwork is found in the collection\n\nTHEN: Get the url to the primary image of the artwork and return it\n</code></pre> Side Note: This Given, When, Then formulation comes from Behavior Driven Development something we'll be touching on in the future.</p> <p>Sounds simple enough, so then the question becomes how do we make these API calls in our Python code? Again, if you had asked me this question previously I would have said the Requests library was your go to standard for doing this. However recently, the HTTPX library has been gaining steam as a faster, more efficient replacement and is the one I will be using.</p> <p>Firstly, we need to add this library as a dependency of our project. Fortunately uv makes this easy for use, simply run <code>uv add httpx</code> in your console and make sure to accept the pop up from VSCode to use the created virtual environment for the project.</p> <p>uv has just simplified a great number of steps for us it has:</p> <ol> <li>Created a virtual environment for our project with the project's specified version of Python (3.13 in our case) so that all the dependencies of your project stay separate from your system's Python dependencies</li> <li>Installed the specified dependency into the newly created virtual environment</li> <li>Added the dependency to the project's <code>pyproject.toml</code> file</li> <li>Created a <code>uv.lock</code> file to hold all of that dependency's dependencies.</li> </ol> <p>All that to say, uv is great and that process used to take several different commands with several different tools to pull off.</p> <p>Now that we have HTTPX installed is pretty easy to write a quick function to handle the desired business logic:</p> <pre><code>from typing import Optional\nimport httpx\n\ndef search(title: str) -&gt; str:\n    search_request: httpx.Response = httpx.get(\n        'https://collectionapi.metmuseum.org/public/collection/v1/search',\n        params={'q': title, 'title': True, 'hasImages': True},\n    )\n\n    object_ids: Optional[list[int]] = search_request.json().get('objectIDs')\n\n    if object_ids:\n        object_request = httpx.get(f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_ids[0]}')\n        primary_image_url = object_request.json().get('primaryImage')\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>Some things to note about this code, PEP 484 defined type hints for Python, use them! They make your code much more readable and easier to understand. In 2025 there is no excuse not to type hint your Python code! Understanding that this function should take in a string and return one is good to know!</p> <p>On the subject of documentation for your code, there is one other thing that this function is missing and that is a docstring. Defined by PEP 257, docstrings are Python's way of documenting functions for the developers that come after you (and yourself!). Again, use them! Adding a docstring to this function would look something like this:</p> <pre><code>def search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n    search_request: httpx.Response = httpx.get(\n        'https://collectionapi.metmuseum.org/public/collection/v1/search',\n        params={'q': title, 'title': True, 'hasImages': True},\n    )\n\n    object_ids: Optional[list[int]] = search_request.json().get('objectIDs')\n\n    if object_ids:\n        object_request = httpx.get(f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_ids[0]}')\n        primary_image_url = object_request.json().get('primaryImage')\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>Unfortunately, there is no official format for docstrings, with several competing styles out there. I myself prefer the style defined in the Google Python Style Guide, though there are other styles you can use if you wish. The important thing is to pick one and be consistent about it. VSCode conveniently has an extension that will autogenerate this format of docstring for you, as well as several other popular formats.</p> <p>You'll also want to update your <code>pyproject.toml</code> to let Ruff know which format you are using:</p> <pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"httpx&gt;=0.28.1\"\n]\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.format]\nquote-style = \"single\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n</code></pre> <p>We can put this function in our <code>main.py</code> file and load up the python REPL to try it out:</p> <p>If we search for an artwork that is not currently in the collection we should get our \"No results found.\" message.</p> <pre><code>&gt;&gt;&gt; from main import search\n&gt;&gt;&gt; search('Mona Lisa')\n'No results found.'\n</code></pre> <p>And if we search for a work that is currently in the collection we should get the url to that work back:</p> <pre><code>&gt;&gt;&gt; search('The Death of Socrates')\n'https://images.metmuseum.org/CRDImages/cl/original/DP102929.jpg'\n</code></pre> <p>Success!</p> <p>Our code works but how can we expose it so that other parts of our application (services) can use it?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#digging-deeper","title":"Digging Deeper","text":"<p>The answer is something we've already discussed: Microservices! We can turn our code into a simple service with an API that other services can call to use this functionality. So how do we add an API to our code?</p> <p>There are a few different libraries we could use: Flask is the tried and true library that is still widely used today. However, (you may be picking up on a theme at this point), FastAPI is a newer, and well faster, library that comes with some quality of life features that I quite like so I'll be using it with this project. Turning our function into an API is as simple as adding 3 lines of code to our <code>main.py</code> file:</p> <pre><code>from typing import Optional\nfrom fastapi import FastAPI\nimport httpx\n\napp = FastAPI()\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n    search_request: httpx.Response = httpx.get(\n        'https://collectionapi.metmuseum.org/public/collection/v1/search',\n        params={'q': title, 'title': True, 'hasImages': True},\n    )\n\n    object_ids: Optional[list[int]] = search_request.json().get('objectIDs')\n\n    if object_ids:\n        object_request = httpx.get(f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_ids[0]}')\n        primary_image_url = object_request.json().get('primaryImage')\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>What we've done here is told FastAPI that our function will take in a GET request on the route <code>/api/search</code>.</p> <p>We can test this out ourselves, run your application and then head to http://127.0.0.1:8000/docs in your web browser, you should see something like the below screen:</p> <p></p> <p>This is an application known as Swagger and comes pre-installed with FastAPI and provides a nice interface for testing your API. Go head and hit the \"Try it out!\" button and execute some searches and view the results.</p> <p>You now have an API! Notice a few other things here as well. Remember those type hints we discussed before? FastAPI is using them to do validation on your requests. It's correctly marking that the title should be a <code>string</code> and the response of a successful request should also be a <code>string</code>. It also creates a 422 response if something other than a <code>string</code> is passed as input. That's a lot of benefit for a few type hints!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#reaching-a-respite","title":"Reaching a Respite","text":"<p>Before we wrap, make sure you commit your code and push it to your own remote Git repository for safe keeping (I tend to use GitHub though there are plenty of other suitable options as well). In this delve we've covered a lot of ground already! We've set up our main development environment, installed our necessary tooling, built a simple application, and turned it into an API. Believe it or not, the core flow of this application of taking an input from an API call, sending it to a different API to get some data, and finally sending it to a third API to get the final result and sending it back after some postprocessing is the same exact flow we will be using with our ML powered applications as well. Though that exploration will come in a future delve.</p> <p>That concludes our delve for today, be on the lookout for a part two where we will harden our codebase and reorganize it into something that is more scalable and enterprise ready! A full copy of the code can be found here. Looking forward to seeing you all at the next one!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/01/26/delve-6-lets-build-a-modern-ml-microservice-application---part-1.html#delve-data","title":"Delve Data","text":"<ul> <li>The use of ML does not preclude us from benefiting from software engineering best practices, on the contrary, we should embrace them</li> <li>*nix operating systems are generally preferred for Python development, though WSL is a very good option for Windows machines</li> <li>In recent years, several new tools have emerged that have streamlined the Python development process</li> <li>The microservice architecture lends itself well to ML application development</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html","title":"Delve 7: Let's Build a Modern ML Microservice Application - Part 2, The Data Layer","text":"<p>\"Data is not just the new oil, it's also the new soil.\" - David McCandless</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#ml-microservices-the-second","title":"ML Microservices, the Second","text":"<p>Hello data delvers! In part one of this series we left off creating a basic application that allowed us to search for a work by title in the Metropolitan Museum of Art's collection. We were able set up a basic project structure as well as the tooling we would need to get the project off the ground. In this second part, I'd like to focus on how we can reorganize our code to make it a bit easier to manage as the complexity of our application scales. However, to begin I'd like to take a slight detour and discussing debugging.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#to-debug-or-not","title":"To Debug or Not","text":"<p>In part one I mentioned running the main script to check its output, but I didn't discuss how to do that specifically in VSCode. You could of course open a shell and simply execute <code>python3 main.py</code> however that missed out on one of the most powerful tools an IDE provides you: the debugger. VSCode relies on something know as a launch configuration to define how code should be executed. Go ahead and follow the instruction in that link to create a launch configuration for FastAPI (VSCode should be smart enough to auto detect it). Run the configuration and:</p> <pre><code>ERROR:    Error loading ASGI app. Could not import module \"main\".\n</code></pre> <p>What's going on? Remember how in part one we moved all of our Python source code to a directory called <code>src</code>? This will have a lot of benefits as we will see but one of the downsides is that by default VSCode expects our main script to be in the root directory of the project. Not to worry though, this is easy to fix. Open up your <code>launch.json</code> file that was created in the <code>.vscode</code> directory and modify it like so:</p> <pre><code>{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python Debugger: FastAPI\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"module\": \"uvicorn\",\n            \"args\": [\n                \"main:app\",\n                \"--reload\"\n            ],\n            \"jinja\": true,\n            \"cwd\": \"${workspaceFolder}/src\",\n            \"env\": {\n                \"PYTHONPATH\": \"${cwd}:${env:PYTHONPATH}\"\n            },\n        }\n    ]\n}\n</code></pre> <p>We need to add two additional arguments:</p> <ul> <li>The <code>cwd</code> argument tells the configuration to change the current working directory to the <code>src</code> folder relative to the workspace folder.</li> <li>The <code>PYTHONPATH</code> argument appends the current working directory to the <code>PYTHONPATH</code> environment variable, this will ensure that imports within our project codebase will work correctly.</li> </ul> <p>Go ahead and save that configuration and run again, your FastAPI application should run now! I also encourage you to play around with starting a debugging session, setting a few breakpoints and following the execution of the code. Reading the full debugging guide for VSCode will give you an idea of the options you have available to you.</p> <p>With debugging all set up we are now ready to discuss refactoring our code!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#refactor-really","title":"Refactor, Really?","text":"<p>When we left off in part one we had created a single file application with a <code>main.py</code> that looked something like this:</p> <pre><code>from typing import Optional\nfrom fastapi import FastAPI\nimport httpx\n\napp = FastAPI()\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n    search_request: httpx.Response = httpx.get(\n        'https://collectionapi.metmuseum.org/public/collection/v1/search',\n        params={'q': title, 'title': True, 'hasImages': True},\n    )\n\n    object_ids: Optional[list[int]] = search_request.json().get('objectIDs')\n\n    if object_ids:\n        object_request = httpx.get(f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_ids[0]}')\n        primary_image_url = object_request.json().get('primaryImage')\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>Now, while there isn't a lot going on in this file right now, I'd argue from an application scalability perspective it's already too complicated. It may seem outlandish to argue that functionally 8 lines of code is too complicated but I'm not arguing from a number of lines perspective but from a scope perspective. Right now this single file contains all of our business logic, that's fine while our business logic is simple, but what happens as we want to add more complexity and processing steps to our application? The way our application is structured right now we just keep adding more complexity to this single file. This is analogous to the workflow I've seen many times with data scientists creating ever larger uber Jupyter notebooks that contain all of their logic. In the same way maintaining a notebook with hundreds of lines of code is unmaintainable, so too is maintaining a single file application. In this delve I intend to refactor this application into something that is more maintainable and scalable without increasing the complexity of the application so we can focus purely on the refactor. To that end we can break our application into 3 main pieces of functionality:</p> <ol> <li>We make API requests to external systems to provide data to our application</li> <li>We encapsulate some business logic (In this case call the search API to retrieve an Object, then call the Objects API to retrieve an image) within our service</li> <li>We provide an interface (API) to allow external customers to interact with our application</li> </ol> <p>Intuitively as you may suspect, these are the three layers we will break our application into. If any of you are familiar with the concept of Multitier Architecture, this is very similar to the three tier architecture often discussed in works of that nature, but zoomed into the scope of our service itself. For this delve we are going to focus on the first of these three layers, the Data Layer.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#the-data-layer","title":"The Data Layer","text":"<p>The Data Layer, as the name implies, is all about data. Functionally, that means we have two objectives we must complete at this layer:</p> <ul> <li>Be able to request data from other components (both internal to the application like a database or external to the application like other services)</li> <li>Be able to represent the requested data within the application</li> </ul> <p>Starting with the first objective, looking at the Metropolitan Museum of Art API that we are leveraging we can see there are four different operations we can perform:</p> <ul> <li>Objects - A listing of all valid Object IDs available for access.</li> <li>Object - A record for an object, containing all open access data about that object, including its image (if the image is available under Open Access)</li> <li>Departments - A listing of all valid departments, with their department ID and the department display name</li> <li>Search - A listing of all Object IDs for objects that contain the search query within the object's data</li> </ul> <p>Though currently we are only using the Object and Search operations. In order to represent these operations without our code, we can lean into our OOP principles and create a client class with four methods, one for each operation.</p> <p>Before we do though I'd like to talk about naming. Generally, I like to split my client classes into two categories. Those that simply provide data from other systems, and those that let me modify data in other systems. I like to call a client in the first case a Provider, as it simply provides data to the application. In the second case I like to call the client a Repository, as it allows us to modify a repository of data (typically a database).</p> <p>In our case our API does not allow modifying the collection of artwork (I hope), and so it falls firmly into the provider use case.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#the-provider-component","title":"The Provider Component","text":"<p>To that end we are now ready to refactor our code, we can create a new directory under our <code>src</code> folder called <code>provider</code>. This folder will hold all the provider clients for our application. Within that folder we need to create two files, first a empty file called <code>__init__.py</code>, this will mark this directory as a Python module and thus allow Python files within it to be imported to other parts of the application, and a file called <code>met_provider.py</code> this file as you can guess will hold our client object.</p> <p>We should now have a directory structure under <code>src</code> that looks like this:</p> <pre><code>src\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 provider\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 met_provider.py\n</code></pre> <p>Inside <code>met_provider.py</code> we can create a simple class to hold our API operations:</p> <pre><code>class MetProvider:\n    \"\"\"A client for the Metropolitan Museum of Art API.\n\n    Args:\n        base_url: The base URL of the API.\n    \"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n</code></pre> <p>Now you might be asking, why not just hard code the URL to the API? We know what it is. The reason I prefer to make the url to the API a constructor parameter is to allow us to easily point to different deployments of the API with the same client. This often comes up if you follow a Dev/QA/Prod style of deployment. You might have your production API at <code>https://www.my-api.com</code> and a QA version of the API hosted at <code>https://www.my-api-qa.com</code> and a dev version at <code>https://www.my-api-dev.com</code>. Having the API url be a parameter allows us to switch between all three urls without needing to change the code of the client.</p> <p>Next we can add a method for the Objects operation to our API like so:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\nimport httpx\n\n\nfrom shared.view.met_view import DepartmentResponse, ObjectResponse, ObjectsResponse, SearchResponse\n\n\nclass MetProvider:\n    \"\"\"A client for the Metropolitan Museum of Art API.\n\n    Args:\n        base_url: The base URL of the API.\n    \"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_objects(\n        self, metadata_date: Optional[datetime] = None, department_ids: Optional[list[int]] = None\n    ) -&gt; dict:\n        \"\"\"Retrieves objects from the Metropolitan Museum of Art API.\n\n        Args:\n            metadata_date: Returns any objects with updated data after this date.\n            department_ids: Returns any objects in a specific department.\n\n        Returns:\n            A list of objects.\n        \"\"\"\n\n        query_params = {}\n\n        if metadata_date:\n            query_params['metadataDate'] = metadata_date.strftime('%Y-%m-%d')\n        if department_ids:\n            query_params['departmentIds'] = '|'.join(map(str, department_ids))\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/objects',\n            params=query_params,\n        )\n\n        return r.json()\n</code></pre> <p>We we can already see the benefits of creating a separate class to handle calling the API. The Met API requires that department IDs be joined by a <code>|</code> character, we can hide that implementation detail at this layer and instead allow the user to pass in a much more natural and pythonic list of integers. Similarly, we can pass in a <code>datetime</code> object and allow this layer to put it in the proper format for this API. In this way we can hide the specific details of the API and allow the user to work with much more natural and easy to use Python objects. </p> <p>We can similarly flesh out the rest of the operations in our provider class:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\nimport httpx\n\n\nfrom shared.view.met_view import DepartmentResponse, ObjectResponse, ObjectsResponse, SearchResponse\n\n\nclass MetProvider:\n    \"\"\"A client for the Metropolitan Museum of Art API.\n\n    Args:\n        base_url: The base URL of the API.\n    \"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_objects(\n        self, metadata_date: Optional[datetime] = None, department_ids: Optional[list[int]] = None\n    ) -&gt; dict:\n        \"\"\"Retrieves objects from the Metropolitan Museum of Art API.\n\n        Args:\n            metadata_date: Returns any objects with updated data after this date.\n            department_ids: Returns any objects in a specific department.\n\n        Returns:\n            A list of objects.\n        \"\"\"\n\n        query_params = {}\n\n        if metadata_date:\n            query_params['metadataDate'] = metadata_date.strftime('%Y-%m-%d')\n        if department_ids:\n            query_params['departmentIds'] = '|'.join(map(str, department_ids))\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/objects',\n            params=query_params,\n        )\n\n        return r.json()\n\n    def get_object(self, object_id: int) -&gt; ObjectResponse:\n        \"\"\"Retrieves an object from the Metropolitan Museum of Art API.\n\n        Args:\n            object_id: The ID of the object to retrieve.\n\n        Returns:\n            The object.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/objects/{object_id}')\n        return r.json()\n\n    def get_departments(self) -&gt; dict:\n        \"\"\"Retrieves departments from the Metropolitan Museum of Art API.\n\n        Returns:\n            A list of departments.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/departments')\n        return r.json()\n\n    def search(self, q: str, title: Optional[bool] = None, has_images: Optional[bool] = None) -&gt; dict:\n        \"\"\"Executes a search against the Metropolitan Museum of Art API.\n\n        Args:\n            q: The query string.\n            title: Whether to search the title field.\n            has_images: Whether to search for objects with images.\n\n        Returns:\n            The search results.\n        \"\"\"\n\n        query_params = {'q': q}\n\n        if title is not None:\n            query_params['title'] = str(title).lower()\n\n        if has_images is not None:\n            query_params['hasImages'] = str(has_images).lower()\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/search',\n            params=query_params,\n        )\n\n        return r.json()\n</code></pre> <p>One other benefit of using a provider class is we only have to implement what we need. The Search operation has many more parameters (and you are welcome to map them out as an exercise) but for our logic we only need the <code>title</code> and <code>hasImages</code> functionality.</p> <p>You should now have a fully working client for the Met API, start up a shell and try it out!</p> <pre><code>&gt;&gt;&gt; from provider.met_provider import MetProvider\n&gt;&gt;&gt; p = MetProvider('https://collectionapi.metmuseum.org')\n&gt;&gt;&gt; p.get_departments()\n'{\"departments\":[{\"departmentId\":1,\"displayName\":\"American Decorative Arts\"},{\"departmentId\":3,\"displayName\":\"Ancient Near Eastern Art\"},{\"departmentId\":4,\"displayName\":\"Arms and Armor\"},{\"departmentId\":5,\"displayName\":\"Arts of Africa, Oceania, and the Americas\"},{\"departmentId\":6,\"displayName\":\"Asian Art\"},{\"departmentId\":7,\"displayName\":\"The Cloisters\"},{\"departmentId\":8,\"displayName\":\"The Costume Institute\"},{\"departmentId\":9,\"displayName\":\"Drawings and Prints\"},{\"departmentId\":10,\"displayName\":\"Egyptian Art\"},{\"departmentId\":11,\"displayName\":\"European Paintings\"},{\"departmentId\":12,\"displayName\":\"European Sculpture and Decorative Arts\"},{\"departmentId\":13,\"displayName\":\"Greek and Roman Art\"},{\"departmentId\":14,\"displayName\":\"Islamic Art\"},{\"departmentId\":15,\"displayName\":\"The Robert Lehman Collection\"},{\"departmentId\":16,\"displayName\":\"The Libraries\"},{\"departmentId\":17,\"displayName\":\"Medieval Art\"},{\"departmentId\":18,\"displayName\":\"Musical Instruments\"},{\"departmentId\":19,\"displayName\":\"Photographs\"},{\"departmentId\":21,\"displayName\":\"Modern Art\"}]}'\n</code></pre> <p>This works but it's a bit tricky to work with the output of our client. Notice how all the methods of our client have a return type of <code>dict</code>. This means when using the client we have no guarantees of the structure of the data we are getting back from our client, and once more, when we do get it back it's difficult to deal with. For example, to get the display name of the first department we'd need some code that looks something like this:</p> <pre><code>r = p.get_departments()\nr['departments'][0]['displayName']\n</code></pre> <p>Yuck! While you can type hint more complex return types, <code>dict[str, list[dict[str, Union[str, int]]]]</code> would be the type hint for that return value for example, this gets pretty ugly and does not help us with the second issue of accessing our data once it's returned. We still have to worry about handling cases where data is missing, or perhaps was a different data type than expected. We also have to type out long Python statements to access nested data structures. Luckily, there is another component of the Data Layer that can help us here: Data Models.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#the-data-model-component","title":"The Data Model Component","text":"<p>When discussing how to represent data within an application, you will often hear the concept of Data Models brought up. These are not models in the machine learning sense, but simply a representation or model of data within the system (hence the name). You can think of these models as simple classes with no methods, only attributes representing the data of the system. These data models are then passed between the layers of the application to facilitate the flow of data.</p> <p>For example, in the above departments example I could have a <code>Department</code> class to represent a single department, every department has a <code>department_id</code> and a <code>display_name</code> so we can model that like so:</p> <pre><code>class Department:\n    department_id: int\n    display_name: str\n</code></pre> <p>We could then have another class that models the response from the API <code>DepartmentResponse</code> which in this case is a list of <code>Department</code> objects.</p> <pre><code>class DepartmentResponse:\n    departments: list[Department]\n</code></pre> <p>This helps us better organize the data we are getting back but does not help us at all when it comes to validating that the data is following the schema that we expect. Fortunately, FastAPI ships with another library that can help us here: Pydantic which will bring our data models to the next level.</p> <p>Pydantic allows use to create data models like above but will also validate that the data provided matches the schema of the type hints of our models. I can't understate how big of a deal this is. This brings what is one of the core strengths of statically typed languages like Java or C# over to Python. Let's try it out!</p> <p>Before we jump into code I also want to talk about naming again. Just like with clients, I like to group my data models based on what they are for. If the data model represents something external to the system, like the schema of an API or database I like to call an instance of those models a View, as it represents a view into an external component. If that data model is instead used to pass data between components of the application I like to call an instance of those models a Data Transfer Object or DTO, as it is transferring data between components. If that seems a little fuzzy right now don't worry, in this series we will see examples of both!</p> <p>To start lets create a new directory under <code>src</code> called <code>shared</code> and make it a module by adding an empty <code>__init__.py</code> file. This is where I like to keep things like data models that are potentially shared between layers of the application. Within this folder create another one called <code>view</code> and similarly make it a module. This is where we will store our view models. Inside here create a Python file called <code>met_view.py</code>, you can probably guess what will go here: our view models for the Met API!</p> <p>You should now have a <code>src</code> directory structure that looks like this:</p> <pre><code>src\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 provider\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 met_provider.py\n\u2514\u2500\u2500 shared\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 view\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 met_view.py\n</code></pre> <p>Inside of our <code>met_view.py</code> we can create our data models as above, however they will inherit from a special parent class, <code>BaseModel</code> from pydantic.</p> <pre><code>from pydantic import BaseModel\n\nclass Department(BaseModel):\n    department_id: int\n    display_name: str\n\n\nclass DepartmentResponse(BaseModel):\n    departments: list[Department]\n</code></pre> <p>Once nice thing to point out here is we can use data models inside of other data models, as we do to create an attribute of type <code>list[Department]</code> in the <code>DepartmentResponse</code> model. Go ahead and start up a shell to see the data validation in action! Let's try create a valid <code>Department</code>:</p> <pre><code>&gt;&gt;&gt; from shared.view.met_view import Department\n&gt;&gt;&gt; d = Department(department_id=1, display_name=\"My Met Department\")\n&gt;&gt;&gt; d\nDepartment(department_id=1, display_name='My Met Department')\n</code></pre> <p>It works! But what happens when we try to create a department with a string id instead of an int?</p> <pre><code>&gt;&gt;&gt; d = Department(department_id=\"one\", display_name=\"My Met Department\")\nTraceback (most recent call last):\n  File \"&lt;python-input-6&gt;\", line 1, in &lt;module&gt;\n    d = Department(department_id=\"one\", display_name=\"My Met Department\")\n  File \"/home/overlord/Documents/PythonProjects/DataDelver/modern-ml-microservices/.venv/lib/python3.13/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 1 validation error for Department\ndepartment_id\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='one', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/int_parsin\n</code></pre> <p>Here we see that we get a <code>ValidationError</code> with a helpful message that we need to supply an int rather than a string.</p> <p>Note: Pydantic does try to type cast fields for you if it makes sense so <code>Department(department_id=\"1\", display_name=\"My Met Department\")</code> works!</p> <p>We can now modify our <code>get_deparments()</code> function to return an instance of <code>DepartmentResponse</code> rather than a <code>dict</code>:</p> <pre><code>def get_departments(self) -&gt; DepartmentResponse:\n    \"\"\"Retrieves departments from the Metropolitan Museum of Art API.\n\n    Returns:\n        A list of departments.\n    \"\"\"\n\n    r = httpx.get(f'{self.base_url}/public/collection/v1/departments')\n    return DepartmentResponse.model_validate(r.json())\n</code></pre> <p><code>model_validate</code> is the key step here. This tells Pydantic we want to validate that data contained in another object (in this case the json response from the API) and return a Pydantic data model if the validation passes.</p> <p>Let's try it out!</p> <pre><code>&gt;&gt;&gt; from provider.met_provider import MetProvider\n&gt;&gt;&gt; p = MetProvider('https://collectionapi.metmuseum.org')\n&gt;&gt;&gt; p.get_departments()\nTraceback (most recent call last):\n  File \"&lt;python-input-2&gt;\", line 1, in &lt;module&gt;\n    p.get_departments()\n    ~~~~~~~~~~~~~~~~~^^\n  File \"/home/overlord/Documents/PythonProjects/DataDelver/modern-ml-microservices/src/provider/met_provider.py\", line 67, in get_departments\n    return DepartmentResponse.model_validate(r.json())\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n  File \"/home/overlord/Documents/PythonProjects/DataDelver/modern-ml-microservices/.venv/lib/python3.13/site-packages/pydantic/main.py\", line 627, in model_validate\n    return cls.__pydantic_validator__.validate_python(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        obj, strict=strict, from_attributes=from_attributes, context=context\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\npydantic_core._pydantic_core.ValidationError: 38 validation errors for DepartmentResponse\ndepartments.0.department_id\n  Field required [type=missing, input_value={'departmentId': 1, 'disp...erican Decorative Arts'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\n...\n</code></pre> <p>What's going on? The shaper-eyed among you may have noticed a different in casing when we created our data model. Typically, JSON utilizes camelCase when naming attributes, for example: <code>departmentId</code>. However, our data models follow the python convention of using snake_case for our attributes, so for example <code>department_id</code>. As a result of this mis-match in casing, the data is not being correctly parsed. How can we fix this?</p> <p>One way we can do this is to take advantage of the Field Alias feature of Pydantic. This allows us, as the name implies, to create aliases other fields can go by. For our <code>Department</code> model that would look something like this:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Department(BaseModel):\n    department_id: int = Field(alias='departmentId')\n    display_name: str = Field(alias='displayName')\n</code></pre> <p>This works but is a bit tedious to type out. Fortunately, for common casing changes, Pydantic provides another option. We can specify a model_config for our models that changes its default behavior. One of the options is an alias_generator which allows us to programmatically generate aliases for our models. Pydantic also comes with a number of pre-made alias generators for common use cases, including one for converting to camelCase!</p> <p>Utilizing this we can change our class to:</p> <pre><code>from pydantic import BaseModel, ConfigDict, Field\nfrom pydantic.alias_generators import to_camel\n\nclass Department(BaseModel):\n    model_config = ConfigDict(alias_generator=to_camel)\n    department_id: int\n    display_name: str\n</code></pre> <p>Let's try it out!</p> <pre><code>&gt;&gt;&gt; from provider.met_provider import MetProvider\n&gt;&gt;&gt; p = MetProvider('https://collectionapi.metmuseum.org')\n&gt;&gt;&gt; p.get_departments()\nDepartmentResponse(departments=[Department(department_id=1, display_name='American Decorative Arts'), Department(department_id=3, display_name='Ancient Near Eastern Art'), Department(department_id=4, display_name='Arms and Armor'), Department(department_id=5, display_name='Arts of Africa, Oceania, and the Americas'), Department(department_id=6, display_name='Asian Art'), Department(department_id=7, display_name='The Cloisters'), Department(department_id=8, display_name='The Costume Institute'), Department(department_id=9, display_name='Drawings and Prints'), Department(department_id=10, display_name='Egyptian Art'), Department(department_id=11, display_name='European Paintings'), Department(department_id=12, display_name='European Sculpture and Decorative Arts'), Department(department_id=13, display_name='Greek and Roman Art'), Department(department_id=14, display_name='Islamic Art'), Department(department_id=15, display_name='The Robert Lehman Collection'), Department(department_id=16, display_name='The Libraries'), Department(department_id=17, display_name='Medieval Art'), Department(department_id=18, display_name='Musical Instruments'), Department(department_id=19, display_name='Photographs'), Department(department_id=21, display_name='Modern Art')])\n</code></pre> <p>It works! We now have our departments correctly modeled. Let's take a look at what getting the display name of the first department looks like now:</p> <pre><code>r = p.get_departments()\nr.departments[0].display_name\n</code></pre> <p>Much more readable! Before we go ahead and create the rest of our views though we should consider that we want all of them to have the same config behavior of allowing camelCase payloads to be validated. We could go and add the <code>model_config</code> property to all of them however a more elegant way to solve this is to just have a base class that all our views inherit from that specifies this configuration, that way we only have to specify it once!</p> <p>Go ahead under <code>src/shared</code> and create a new Python file called <code>data_model_base.py</code> this will (unsurprisingly) hold the base class for our data models.</p> <p>You should now have a directory structure that looks like this:</p> <pre><code>src\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 provider\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 met_provider.py\n\u2514\u2500\u2500 shared\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 data_model_base.py\n    \u2514\u2500\u2500 view\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 met_view.py\n</code></pre> <p>Inside we can create a new class <code>ViewBase</code> which will be the Pydantic base class for all of our views:</p> <pre><code>from pydantic import BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_camel\n\n\nclass ViewBase(BaseModel):\n    model_config = ConfigDict(alias_generator=to_camel)\n</code></pre> <p>We can now let all of our views inherit from this base class:</p> <pre><code>from typing import Optional\n\nfrom pydantic import Field\nfrom shared.data_model_base import ViewBase\n\n\nclass SearchResponse(ViewBase):\n    total: int\n    object_ids: Optional[list[int]] = Field(alias='objectIDs')\n\n\nclass Department(ViewBase):\n    department_id: int\n    display_name: str\n\n\nclass DepartmentResponse(ViewBase):\n    departments: list[Department]\n\n\nclass ObjectResponse(ViewBase):\n    object_id: int = Field(alias='objectID')\n    title: str\n    primary_image: str\n    additional_images: list[str]\n\n\nclass ObjectsResponse(ViewBase):\n    total: int\n    object_ids: list[int] = Field(alias='objectIDs')\n</code></pre> <p>Notice we still had to use aliases in cases where the attribute followed non-standard casing. <code>ID</code> not <code>Id</code> really? Also note, there are many more attributes available in the schema of the response to the Object operation in the Met API, however we only need to model the attributes we need. By default Pydantic will just ignore any extra fields, though you can change this in the model configuration.</p> <p>Finally we can update our <code>MetProvider</code> class to utilize these views:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\nimport httpx\n\n\nfrom shared.view.met_view import DepartmentResponse, ObjectResponse, ObjectsResponse, SearchResponse\n\n\nclass MetProvider:\n    \"\"\"A client for the Metropolitan Museum of Art API.\n\n    Args:\n        base_url: The base URL of the API.\n    \"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_objects(\n        self, metadata_date: Optional[datetime] = None, department_ids: Optional[list[int]] = None\n    ) -&gt; ObjectsResponse:\n        \"\"\"Retrieves objects from the Metropolitan Museum of Art API.\n\n        Args:\n            metadata_date: Returns any objects with updated data after this date.\n            department_ids: Returns any objects in a specific department.\n\n        Returns:\n            A list of objects.\n        \"\"\"\n\n        query_params = {}\n\n        if metadata_date:\n            query_params['metadataDate'] = metadata_date.strftime('%Y-%m-%d')\n        if department_ids:\n            query_params['departmentIds'] = '|'.join(map(str, department_ids))\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/objects',\n            params=query_params,\n        )\n\n        return ObjectsResponse.model_validate(r.json())\n\n    def get_object(self, object_id: int) -&gt; ObjectResponse:\n        \"\"\"Retrieves an object from the Metropolitan Museum of Art API.\n\n        Args:\n            object_id: The ID of the object to retrieve.\n\n        Returns:\n            The object.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/objects/{object_id}')\n        return ObjectResponse.model_validate(r.json())\n\n    def get_departments(self) -&gt; DepartmentResponse:\n        \"\"\"Retrieves departments from the Metropolitan Museum of Art API.\n\n        Returns:\n            A list of departments.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/departments')\n        return DepartmentResponse.model_validate(r.json())\n\n    def search(self, q: str, title: Optional[bool] = None, has_images: Optional[bool] = None) -&gt; SearchResponse:\n        \"\"\"Executes a search against the Metropolitan Museum of Art API.\n\n        Args:\n            q: The query string.\n            title: Whether to search the title field.\n            has_images: Whether to search for objects with images.\n\n        Returns:\n            The search results.\n        \"\"\"\n\n        query_params = {'q': q}\n\n        if title is not None:\n            query_params['title'] = str(title).lower()\n\n        if has_images is not None:\n            query_params['hasImages'] = str(has_images).lower()\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/search',\n            params=query_params,\n        )\n\n        return SearchResponse.model_validate(r.json())\n</code></pre>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#wrangling-our-data-layer","title":"Wrangling our Data (Layer)","text":"<p>That wraps up our build of the data layer of our application! It may seem like a lot of code to do a small amount, but hopefully it's clear that the benefits of abstracting out the data modeling and validation to its own layer of the application will make it much easier to manage the data needs of our application as it increases in scale and the scope of the data required expands (something that often happens in ML applications). Full code for this part is available here. In the next part we'll see how to leverage this layer in the other parts of the application: the Business Logic Layer and the Interface layer. See you next time!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/05/delve-7-lets-build-a-modern-ml-microservice-application---part-2-the-data-layer.html#delve-data","title":"Delve Data","text":"<ul> <li>As the complexity of our application increases, a single <code>main.py</code> file application will become messy</li> <li>Adopting a three layer architecture for our application's code can help us to manage this complexity</li> <li>The first of these layers, the Data Layer is responsible for requesting data from other components and representing the requested data within the application</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html","title":"Delve 8: Let's Build a Modern ML Microservice Application - Part 3, The Business Logic and Interface Layers","text":"<p>\"For every minute spent organizing, an hour is earned.\" - Benjamin Franklin</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#ml-microservices-completing-the-layers","title":"ML Microservices, Completing the Layers","text":"<p>Hello data delvers! In part two of this series we refactored to create a separate data layer of our application. As a refresher, these are the three layers of the application we are creating:</p> <p>Figure 1: The Three Layer Application</p> <p></p> <p>For this delve, we will be creating the remaining two layers.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#the-business-logic-layer","title":"The Business Logic Layer","text":"<p>The Business Logic Layer is responsible for implementing the core logic of our application. It does this by manipulating the data models we receive from the data layer and translating the results of the manipulations into business entities to pass on to the Interface Layer. You will sometimes see this layer abbreviated as BLL.</p> <p>Let's take a look at the main function of our app:</p> <pre><code>@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n    search_request: httpx.Response = httpx.get(\n        'https://collectionapi.metmuseum.org/public/collection/v1/search',\n        params={'q': title, 'title': True, 'hasImages': True},\n    )\n\n    object_ids: Optional[list[int]] = search_request.json().get('objectIDs')\n\n    if object_ids:\n        object_request = httpx.get(f'https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_ids[0]}')\n        primary_image_url = object_request.json().get('primaryImage')\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>The logic we are implementing here is:</p> <ul> <li>Search for a work in the Met collection by the title</li> <li>If the work exists:<ul> <li>Fetch the primary image of the work and return it</li> </ul> </li> <li>Else:<ul> <li>Return \"No results found.\"</li> </ul> </li> </ul> <p>Pretty simple and hopefully you can see how we can use the data layer operations we built out in the last part to implement it!</p> <pre><code>from provider.met_provider import MetProvider\n\nmet_provider = MetProvider('https://collectionapi.metmuseum.org')\n\ndef search_by_title(title: str): -&gt; str\n\n    # Search for a work in the Met collection by title\n    search_response = met_provider.search(q=title, title=True, has_images=True)\n    object_ids = search_response.object_ids\n\n    # If the work exists\n    if object_ids:\n\n        # Fetch the primary image of the work\n        object_request = self.met_client.get_object(object_id=object_ids[0])\n        primary_image_url = object_request.primary_image\n\n        return primary_image_url\n    else:\n        return 'No results found.'\n</code></pre> <p>Using the <code>MetProvider</code> to implement this logic gives us all the data validation benefits we discussed in the last part so we don't have to worry about validating the schema of the data we are working with at this layer, the provider handles all that for us!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#the-service-component","title":"The Service Component","text":"<p>Similarly to how we broke the Data Layer into components (Namely Providers and Views) I like to break the Business Logic Layer into Service components. These are not services in the microservice sense, but a component of the application that performs a function or set of related functions. In our case the function we want to perform is to search for a work by it's title (though in the future we may search by other attributes as well). To that end we can create a new folder under <code>src</code> called <code>service</code> to hold our application services. Inside of it we can create an <code>__init__.py</code> file and a <code>search_service.py</code> file. We should now have a directory structure that looks like the following:</p> <pre><code>src\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 provider\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 met_provider.py\n\u251c\u2500\u2500 service\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 search_service.py\n\u2514\u2500\u2500 shared\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 data_model_base.py\n    \u2514\u2500\u2500 view\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 met_view.py\n</code></pre> <p>Inside of <code>search_service.py</code> we can create our <code>SearchService</code> class:</p> <pre><code>from provider.met_provider import MetProvider\n\n\nclass SearchService:\n    \"\"\"A service for searching the Metropolitan Museum of Art API.\n\n    Args:\n        met_provider: A client for the Metropolitan Museum of Art API.\n    \"\"\"\n\n    def __init__(self, met_provider: MetProvider):\n        self.met_provider = met_provider\n\n    def search_by_title(self, title: str) -&gt; str:\n        \"\"\"Searches the Metropolitan Museum of Art API by title.\n\n        Args:\n            title: The title of the work to search for.\n\n        Returns:\n            The url of the primary image of the first search result.\n        \"\"\"\n\n        # Search for a work in the Met collection by title\n        search_response = self.met_provider.search(q=title)\n        object_ids = search_response.object_ids\n\n        # If the work exists\n        if object_ids:\n            # Fetch the primary image of the work\n            object_request = self.met_provider.get_object(object_id=object_ids[0])\n            primary_image_url = object_request.primary_image\n\n            return primary_image_url\n        else:\n            return 'No results found.'\n</code></pre> <p>On thing to point out here is we are not instantiating the <code>MetProvider</code> class within our <code>SearchService</code> class directly, instead we are passing it in the constructor. This is a form of dependency injection, this means our search service doesn't need to know the information needed to create a <code>MetProvider</code> object (in this case the url to the Met API), it is simply provided one. In separating these concerns, our application layers can be much more loosely coupled and easier to refactor without needing to modify multiple layers at once. Let's go ahead and try this service out!</p> <pre><code>&gt;&gt;&gt; from provider.met_provider import MetProvider\n&gt;&gt;&gt; from service.search_service import SearchService\n&gt;&gt;&gt; search_service = SearchService(MetProvider('https://collectionapi.metmuseum.org'))\n&gt;&gt;&gt; search_service.search_by_title(\"The Death of Socrates\")\n'https://images.metmuseum.org/CRDImages/ep/original/DP-13139-001.jpg'\n</code></pre> <p>It works! However, what if we wanted to display a different error message to the user on the front end of our application. Right now our service is determining the response to be shown when that should really be determined by the Interface layer of our application. What if instead of returning the error message directly our service could throw an Error instead. Then our Interface layer could decide how to handle that error, whether that be displaying an error message or some other behavior. Let's go ahead and make that change:</p> <pre><code>from provider.met_provider import MetProvider\n\n\nclass SearchService:\n    \"\"\"A service for searching the Metropolitan Museum of Art API.\n\n    Args:\n        met_provider: A client for the Metropolitan Museum of Art API.\n    \"\"\"\n\n    def __init__(self, met_provider: MetProvider):\n        self.met_provider = met_provider\n\n    def search_by_title(self, title: str) -&gt; str:\n        \"\"\"Searches the Metropolitan Museum of Art API by title.\n\n        Args:\n            title: The title of the work to search for.\n\n        Returns:\n            The url of the primary image of the first search result.\n\n        Raises:\n            ValueError: If no results are found.\n        \"\"\"\n\n        # Search for a work in the Met collection by title\n        search_response = self.met_provider.search(q=title)\n        object_ids = search_response.object_ids\n\n        # If the work exists\n        if object_ids:\n            # Fetch the primary image of the work\n            object_request = self.met_provider.get_object(object_id=object_ids[0])\n            primary_image_url = object_request.primary_image\n\n            return primary_image_url\n        else:\n            raise ValueError('No results found.')\n</code></pre> <p>Don't forget to update the docstring of your function to indicate this function now raises an exception!</p> <p>Finally, what if in addition to the primary image url, we also want to return any additional images the work has? One way we could do this is by returning a <code>tuple</code> of both the primary and additional images. Something like:</p> <pre><code>primary_image_url = object_request.primary_image\nadditional_image_urls = object_request.additional_images\n\nreturn primary_image_url, additional_image_urls\n</code></pre> <p>This works but it means whoever is using this service has to remember what order the values are in to correctly use them. This gets even worse the more data we want to return. Instead of this we can utilize a concept we touched on briefly in the last part, the Data Transfer Object.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#bringing-dto-to-the-bll","title":"Bringing DTO to the BLL","text":"<p>If you recall, in part 2 we introduced the concept of Data Models, classes using the Pydantic library to represent data within our application. We then further sub-divided them into Views, data models for representing something external to the system, and Data Transfer Objects (DTO) for internal representations. We heavily used Views when creating our <code>MetProvider</code> class. In much the same way we can represent the response from our service as a data model. We could represent it something like so:</p> <pre><code>class SearchResult:\n    primary_image: str\n    additional_images: list[str]\n</code></pre> <p>In this way, in our service layer we are combining elements of the underlying Views into something that makes more sense as a business entity a Search Result.</p> <p>To implement this we can first create a new folder under <code>src/shared</code> called <code>dto</code> to hold all our Data Transfer Objects. Inside this folder create your empty <code>__init__.py</code> and a Python file called <code>search_result.py</code>.</p> <p>In much the same way we had a base class for all our views, we can have one for our DTOs as well. Open up the <code>data_model_base.py</code> file and create a new base class for our DTOs to inherit:</p> <pre><code>from pydantic import BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_camel\n\n\nclass ViewBase(BaseModel):\n    model_config = ConfigDict(alias_generator=to_camel)\n\n\nclass DTOBase(BaseModel):\n    pass\n</code></pre> <p>Our base class doesn't do anything for now, we shouldn't need to convert between cases at this layer for example, but it will be useful to have for the future.</p> <p>We should now have a file structure that looks like this:</p> <pre><code>src\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 provider\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 met_provider.py\n\u251c\u2500\u2500 service\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 search_service.py\n\u2514\u2500\u2500 shared\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 data_model_base.py\n    \u251c\u2500\u2500 dto\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 search_result.py\n    \u2514\u2500\u2500 view\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 met_view.py\n</code></pre> <p>Inside of <code>search_result.py</code> we can then create our DTO:</p> <pre><code>from shared.data_model_base import DTOBase\n\n\nclass SearchResult(DTOBase):\n    object_id: int\n    title: str\n    primary_image: str\n    additional_images: list[str]\n    total_results: int\n</code></pre> <p>I added in a few more fields that might be useful in the future!</p> <p>We can then modify our search service to utilize the DTO like so:</p> <pre><code>from provider.met_provider import MetProvider\nfrom shared.dto.search_result import SearchResult\n\n\nclass SearchService:\n    \"\"\"A service for searching the Metropolitan Museum of Art API.\n\n    Args:\n        met_provider: A client for the Metropolitan Museum of Art API.\n    \"\"\"\n\n    def __init__(self, met_provider: MetProvider):\n        self.met_provider = met_provider\n\n    def search_by_title(self, title: str) -&gt; SearchResult:\n        \"\"\"Searches the Metropolitan Museum of Art API by title.\n\n        Args:\n            title: The title of the work to search for.\n\n        Returns:\n            The search results.\n\n        Raises:\n            ValueError: If no results are found.\n        \"\"\"\n\n        # Search for a work in the Met collection by title\n        search_response = self.met_provider.search(q=title)\n        object_ids = search_response.object_ids\n\n        # If the work exists\n        if object_ids:\n            # Fetch the details of the work\n            object_request = self.met_provider.get_object(object_id=object_ids[0])\n\n            return SearchResult(\n                object_id=object_request.object_id,\n                title=object_request.title,\n                primary_image=object_request.primary_image,\n                additional_images=object_request.additional_images,\n                total_results=search_response.total,\n            )\n        else:\n            raise ValueError('No results found.')\n</code></pre> <p>Let's try it out!</p> <pre><code>&gt;&gt;&gt; from provider.met_provider import MetProvider\n&gt;&gt;&gt; from service.search_service import SearchService\n&gt;&gt;&gt; search_service = SearchService(MetProvider('https://collectionapi.metmuseum.o\\\nrg'))\n&gt;&gt;&gt; search_service.search_by_title(\"The Death of Socrates\")\nSearchResult(object_id=436105, title='The Death of Socrates', primary_image='https://images.metmuseum.org/CRDImages/ep/original/DP-13139-001.jpg', additional_images=['https://images.metmuseum.org/CRDImages/ep/original/LC-31_45_suppl_003.jpg', 'https://images.metmuseum.org/CRDImages/ep/original/LC-31_45-2.jpg', 'https://images.metmuseum.org/CRDImages/ep/original/LC-31_45-3.jpg'], total_results=23874)\n</code></pre> <p>Much better! We now have all the details of the work nicely organized in a data validated model for us, neat!</p> <p>That completes our Business Logic Layer for now! All that's left is to now modify our Interface Layer to utilize this service!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#the-interface-layer","title":"The Interface Layer","text":"<p>The final layer of our application is the <code>Interface Layer</code>, this is where users of our application will interact with it (typically through an API). We already had this layer all along it's our <code>main.py</code> file. But now we can greatly simplify it using the capabilities we've built up in the previous layers:</p> <pre><code>from fastapi import FastAPI\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\n\napp = FastAPI()\nsearch_service = SearchService(MetProvider('https://collectionapi.metmuseum.org'))\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n    search_result = search_service.search_by_title(title)\n    return search_result.primary_image\n</code></pre> <p>Much simpler! If the requirements of our application change, it's also easy to modify. Want to get the additional images too? Easy. Total number of results? Done. All without increasing the complexity of our interface layer! One final thing to do here is to provide some better error handling in the case we get no results though. If you search for something that the Met does not have right now this will cause a <code>ValueError</code> to bubble up to the top and result in a <code>500: Internal Server Error</code> response. We can change that at this layer to be something more natural like a 404 status code:</p> <pre><code>from fastapi import FastAPI, HTTPException\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\n\napp = FastAPI()\nsearch_service = SearchService(MetProvider('https://collectionapi.metmuseum.org'))\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n\n    try:\n        search_result = search_service.search_by_title(title)\n        return search_result.primary_image\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n</code></pre> <p>Congratulations! You now have a fully 3 layered application that will be flexible to changing requirements in the future! In future delves we'll look at how we can harden this application even further and make it even more configurable. We are well on our way to plugging machine learning into it too! Full code for this part is available here.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/02/16/delve-8-lets-build-a-modern-ml-microservice-application---part-3-the-business-logic-and-interface-layers.html#delve-data","title":"Delve Data","text":"<ul> <li>Breaking our application into three layers Data, Business Logic, and Interface allows us to separate concerns within our codebase and make it more flexible and robust. </li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html","title":"Delve 9: Migrating from Jekyll to Material for MkDocs","text":"<p>\"Good tools make good work.\" - Unknown</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#from-one-static-site-to-another","title":"From one Static Site to Another","text":"<p>Greetings data delvers! The sharper-eyed among you may have noticed that the website looks a little bit different now. No you aren't seeing things. I recently completely changed the backend of the site from Jekyll to Material for MkDocs. The process was overall pretty smooth but had some hiccups which I think are worth documenting. However, before we get into that, why the change in the first place?</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#jekyll-where-all-the-complexity-hydes","title":"Jekyll, where all the complexity Hydes","text":"<p>The Jekyll project has been around for quite some time (it was originally released in 2008!), and during that time it became the most popular static site generator out there. However, much like its namesake Dr. Jekyll it has developed an ugly side. Jekyll comes from a time when Ruby on Rails ruled the web, however according to the most recent Stack Overflow Developer Survey only 5.2% of respondents use Ruby, compared to languages like Python or Javascript which both are over 50% usage.</p> <p>We can see this stagnation if we look at commits to the Jekyll project as well on Github:</p> <p>Figure 1: Jekyll Commits over Time</p> <p></p> <p>We can see that commits peaked around 2016 and then declined to the point where there are hardly any at all. The last major release of the project was over a year ago and it has only received minor updates since then. Despite this Jekyll is still the recommended static site generator for GitHub Pages which is why I built the site using it in the first place. This in it of itself would not be an issue if the project was stable, however there have been several occasions where the build of this blog would break due to some Ruby dependency requiring debugging just to publish my latest article, not great.  In addition, as someone who has never really professionally worked with Ruby, the unfamiliar toolchain also made extending the framework to add additional functionality difficult, leading to my interest in finding an alternative static site engine. </p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#material-for-mkdocs-made-of-the-right-stuff","title":"Material for MkDocs, Made of the Right Stuff","text":"<p>My inspiration for migrating came when I was browsing some of the articles in my favorite Python newsletter PyCoders Weekly and came across one that was using a slick UI (It's a good read by the way). At the bottom of the article was a link stating \"Made with Material for MkDocs\". Following the link brought me to the Material for MkDocs project and I was instantly impressed with what they had put together. It builds on top of the already popular MkDocs project to provide a clean, modern theme inspired by material design principles. In addition, unlike Jekyll it has pretty active development community and is used by some very popular projects to generate their websites like uv and FastAPI.</p> <p>Figure 2: Material for MkDocs Commits over Time</p> <p></p> <p>Finally, it uses Python as its backend which means it's much easier for me to extend and work with. All of these reasons led me to decide to take the plunge and migrate.</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#migrating-from-jekyll-to-material","title":"Migrating from Jekyll to Material","text":"<p>To kick things off I started by reading the basic blog tutorial documentation on the Material website. This is a good starting point but as I came to learn, it simplifies some things so it's not an end-all-be-all resource. Fundamentally, both Jekyll and Material use Markdown documents to provide the content for the generated pages, so with some minor adjustments to our folder structure we can get the content of the site to render. This involved migrating to the following directory structure:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 CNAME\n\u2502   \u251c\u2500\u2500 about.md\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 posts\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 uv.lock\n</code></pre> <p>Note</p> <p>For my site which is primarily a blog I followed this blog only section of the setup guide to have all of my posts directly under the <code>docs/posts</code> path.</p> <p>Some other nice things here is since Material is a Python package we can use uv and a <code>pyproject.toml</code> file to manage our dependencies!</p> <p>The next step was to clean up the Markdown of the posts themselves. For example, the Markdown of my very first post looked something like this:</p> <pre><code>---\nlayout: post\ntitle:  \"Delve 0: Hello Labyrinth (World)!\"\nauthor: Chase\ncategories: Meta\nbanner: \n    image: \"/assets/images/banners/delve0.png\"\n---\n\n&gt; It seemed so daunting, \"I need to make this model work!\", \"Running this in a notebook isn't good enough, we need to drive live site traffic against this!\", \"All of this data is bad!\".\n\n## Entering the Labyrinth\n\nWelcome to my blog data delver! I'm so glad you found your way here!  If you're like me, when you first started out with data science and machine learning, you may have been feeling overwhelmed. With so many different concepts to learn it may have seemed as if there was an insurmountable labyrinth of information ahead of you, with no clear path towards mastery and practical application.  Fear not! For you have found a resource which shall aid you in your own quest to navigate the maze.\n\n### Purpose of this Blog\n\nThe purpose of this blog is to document my own \"delves\" into this labyrinth and any resulting knowledge I have unearthed. I plan to focus on a range of topics, from general software engineering to data science, machine learning engineering, and MLOps, pulling from a range of experiences across my own career as a machine learning engineer and data scientist with a focus on practical, grounded application in industry following best practice.\n\n### Who am I?\n\nMy name is Chase Greco, I'm currently a machine learning engineer with 5+ years of industry experience. I love exploring all things machine learning with a particular emphasis on practical application and \"making things real\". When I'm not writing code, I enjoy swing dancing, reading, and playing video games.\n\n## Delve Data\n* Welcome to my blog!\n* Stay tuned for more posts on data science, machine learning, and MLOps!\n</code></pre> <p>This needed to be slightly modified like so:</p> <pre><code>---\ndate: 2023-11-06\ncategories: \n    - Meta\n---\n\n# Delve 0: Hello Labyrinth (World)!\n\n![Banner](../assets/images/banners/delve0.png)\n\n&gt; It seemed so daunting, \"I need to make this model work!\", \"Running this in a notebook isn't good enough, we need to drive live site traffic against this!\", \"All of this data is bad!\".\n\n## Entering the Labyrinth\n\nWelcome to my blog data delver! I'm so glad you found your way here!  If you're like me, when you first started out with data science and machine learning, you may have been feeling overwhelmed. With so many different concepts to learn it may have seemed as if there was an insurmountable labyrinth of information ahead of you, with no clear path towards mastery and practical application.  Fear not! For you have found a resource which shall aid you in your own quest to navigate the maze.\n\n&lt;!-- more --&gt;\n\n### Purpose of this Blog\n\nThe purpose of this blog is to document my own \"delves\" into this labyrinth and any resulting knowledge I have unearthed. I plan to focus on a range of topics, from general software engineering to data science, machine learning engineering, and MLOps, pulling from a range of experiences across my own career as a machine learning engineer and data scientist with a focus on practical, grounded application in industry following best practice.\n\n### Who am I?\n\nMy name is Chase Greco, I'm currently a machine learning engineer with 5+ years of industry experience. I love exploring all things machine learning with a particular emphasis on practical application and \"making things real\". When I'm not writing code, I enjoy swing dancing, reading, and playing video games.\n\n## Delve Data\n* Welcome to my blog!\n* Stay tuned for more posts on data science, machine learning, and MLOps!\n</code></pre> <p>A few things to point out:</p> <ul> <li>The post metadata is much more simplified</li> <li>An explicit date now needs to be added to the metadata</li> <li><code>categories</code> metadata now needs to be represented as a list</li> <li>The <code>title</code> and <code>banner</code> needed to be moved to the main body of the post</li> <li>A <code>&lt;!-- more --&gt;</code> comment needed to be added to facilitate marking a cutoff for the post excerpt</li> </ul> <p>Note</p> <p>For posts that contained links to other posts on this site Material provides a convenient syntax for post links. All of the links needed to be modified to this format to make them work.</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#adding-tags","title":"Adding Tags","text":"<p>Many of my posts also utilized tags. Fortunately Material has a plugin which supports this. I did need to add a <code>tags.md</code> as the setup guide describes to render a tags index.</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#adding-an-author","title":"Adding an Author","text":"<p>Material also provides a capability for adding an author to each post. If you'd like to use this feature an author avatar image is required.</p> <p>Tip</p> <p>Instead of adding the author field to every post metadata, if you'd like all your posts to have the same author you can use the Material Meta Plugin to apply the desired author tag to the entire posts directory by creating a <code>.meta.yml</code> file in your <code>posts</code> directory with the following contents:</p> <pre><code>authors:\n- chase # (1)!\n</code></pre> <ol> <li>Replace with your author name</li> </ol>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#adding-comments","title":"Adding Comments","text":"<p>Material also provides instructions for adding comments to your posts using Giscus. Follow the linked instructions and if using the Material Meta Plugin add the following contents to your <code>.meta.yml</code> file to enable comments for all of your posts:</p> <pre><code>authors:\n  - chase\ncomments: true\n</code></pre> <p>With that you should have your posts fully rended and configured!</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#handling-link-migrations","title":"Handling Link Migrations","text":"<p>On thing that was very important to me was to not break any existing links to my blog that may have been published. Out of the box Material follows a different schema for post urls which would break any existing links. Fortunately there is a workaround for this.</p> <p>To begin by default Material does not add <code>.html</code> to the end of urls like Jekyll, however we can change this behavior by adding the following to our <code>mkdcos.yml</code> file:</p> <pre><code>use_directory_urls: false\n</code></pre> <p>This adds <code>.html</code> to the end of post urls but Jeykll by default included other information in post urls like categories. For example:</p> Engine URL Jekyll https://www.datadelver.com/meta/2023/11/06/hello-labyrinth.html Material https://www.datadelver.com/2023/11/06/delve-0-hello-labyrinth-world.html <p>We can handle this by utilizing the mkdocs-redirects plugin to manually redirect these Jekyll url formats to the new Material ones. The trick is we have to link to the old url as if a Markdown file existed at that location.</p> <p>Functionally this looks like the following section in the <code>mkdocs.yml</code> file:</p> <pre><code>- redirects:\n      redirect_maps:\n        'meta/2023/11/06/hello-labyrinth.md': 'posts/2023-11-06-hello-labyrinth.md'\n</code></pre> <p>Using this approach we can redirect all of the old urls to the new format, preserving the functionality of any existing links!</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#customizing-the-home-page","title":"Customizing the Home Page","text":"<p>One final thing I wanted to modify for my blog was the homepage, the default landing page simply displays the list of posts, however I wanted to add a header image. In researching how this could be achieved I found this excellent tutorial by A3Bagged which covers this exact topic.</p> <p>Note</p> <p>If you still want to display blog posts on your home page you will need to copy the contents of the blog template and add it to the bottom of your <code>home.html</code> override.</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#publishing-to-github","title":"Publishing to Github","text":"<p>Material provides a guide for publishing your site to GitHub Pages. However, since I am using uv to manage the project dependencies I had to modify their script slightly:</p> <pre><code>name: ci \non:\n  push:\n    branches:\n      - master \n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version-file: \".python-version\"\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV \n\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache \n          restore-keys: |\n            mkdocs-material-\n\n      - name: Install the project\n        run: uv sync --all-extras --dev\n\n      - run: uv run mkdocs gh-deploy --force\n</code></pre> <p>With the above modifications publishing to GitHub pages worked flawlessly!</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#thoughts-on-migrating","title":"Thoughts on Migrating","text":"<p>Overall, the migration process was pretty smooth. One thing I did notice is the Material documentation sometimes conflicts particularly between the tutorials and the setup guides. I've found a good rule of thumb is to:</p> <ol> <li>Prefer the plugins section for up to date documentation</li> <li>Use the setup section to get context on how to use the plugins</li> <li>Use the getting started section sparingly, it seems to be the most out of date</li> </ol> <p>I also didn't fully cover all of the plugins I used for this blog as I think the already existing plugin documentation does a good job of covering it. I encourage you to look at the source code for this site if you are curious how I set anything up though!</p> <p>I'm very happy I made the jump to Material, I like how the site looks now and am much more confident in my ability to support it moving forward! Let me know in the comments what you think of the redesign!</p>","tags":["Tools"]},{"location":"2025/03/15/delve-9-migrating-from-jekyll-to-material-for-mkdocs.html#delve-data","title":"Delve Data","text":"<ul> <li>Jekyll has historically been the go-to solution for generating static websites to host on GitHub Pages, but has lost steam as of late</li> <li>Material for MkDocs provides a more modern, Python-based alternative for generating static websites</li> </ul>","tags":["Tools"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html","title":"Delve 10: Let's Build a Modern ML Microservice Application - Part 4, Configuration","text":"<p>\"The measure of intelligence is the ability to change.\" - Albert Einstein</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html#ml-microservices-the-great-env-scape","title":"ML Microservices, The Great Env-scape","text":"<p>Hello data delvers! In part three of this series we refactored our application into three separate layers, allowing us to better separate concerns within our codebase. However, if we examine certain parts of our code we can still observe some brittleness:</p> src/main.py<pre><code>from fastapi import FastAPI, HTTPException\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\n\napp = FastAPI()\nsearch_service = SearchService(MetProvider('https://collectionapi.metmuseum.org'))# (1)!\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n\n    try:\n        search_result = search_service.search_by_title(title)\n        return search_result.primary_image\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n</code></pre> <ol> <li>Notice a problem here?</li> </ol> <p>The problem occurs on line 7, right now we are hardcoding a link to <code>https://collectionapi.metmuseum.org</code>. However, what if this link changes? We would have to change our code! What's the big deal you may ask? I can just update my code when needed right? While yes you could, the problem comes if we want to deploy different versions of our code each with a different url. This can happen in a few different scenarios:</p> <p>Scenario 1:</p> <p>Imagine we want to load balance our application. In this setup we have one copy of our application deployed on servers on the east coast and another copy deployed on servers on the west. In addition, the search API has hypothetically deployed two different copies of itself, one in east and one it west each behind two different urls:</p> <ul> <li>https://collectionapi-east.metmuseum.org</li> <li>https://collectionapi-west.metmuseum.org</li> </ul> <p>We want to deploy the same code to each region but change the url it is pointing to.</p> <p>Scenario 2:</p> <p>Perhaps more common, we are following a Deployment Environment pattern in our codebase and we have 3 separate environments:</p> <ul> <li>DEV - Where we deploy changes we are actively working on</li> <li>QA - Where we test that our code is behaving as expected</li> <li>PROD - Where consumers of our application directly interact with it</li> </ul> <p>We'd like to only hit the \"real\" search service in our PROD environment and hit dummy urls in our lower environments to make sure we don't overwhelm the real search service with our tests.</p> <p>Note</p> <p>There are other variations of this pattern with more environments, for this example I'm choosing to use a relatively simple setup with only 3.</p> <p>There are other scenarios where the url can change but I find these two to be the most frequent and for this delve I'd like to hone in on the second scenario as the deployment environment pattern is extremely common in practice. If you haven't encountered it before I encourage you to read up on it, every enterprise application I've ever worked on has used some variation of the pattern without fail, it is that ubiquitous. So how can we implement this pattern in our codebase?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html#configure-it-out","title":"(Con)figure it Out!","text":"<p>One simple way to handle this is to introduce a configuration file to our application that could hold the url per environment. We could then set a flag (such as an environment variable) the lets the application know which environment it is running in so it can choose which setting to read. To start we need to choose how we define our config file. A few options to choose from that I've seen before include:</p> <ul> <li>.env - Probably one of the simplest formats out there, simply stores key value pairs</li> <li>JSON - Just like with our APIs, we can use JSON to specify our app configuration, though it is a bit verbose</li> <li>HOCON - A superset of JSON designed to be more human readable, though not as common in Python projects</li> <li>YAML - Another superset of JSON with additional capabilities, fairly common</li> <li>TOML - A simplified format somewhere between .env and YAML, starting to become more popular in Python projects, especially with the advent of the <code>pyproject.toml</code> standard</li> </ul> <p>You could be successful using any of these formats but I'm going to go with YAML as I like that it is not as verbose as JSON but provides support for more complex structures than TOML.</p> <p>To begin, let's create a new file to hold our configuration:</p> src/config/shared/config/config.yaml<pre><code>dev: \n  met_api_url: https://collectionapi-dummy.metmuseum.org\n\nqa:\n  met_api_url: https://collectionapi-dummy.metmuseum.org\n\nprod:\n  met_api_url: https://collectionapi.metmuseum.org\n</code></pre> <p>Take note of the filepath, that will come in handy later. We now have a file that accomplishes our objective: having a dummy url in our non-prod environments. However, there's a problem, we're copy pasting the same url multiple times! This isn't a big deal now, but you can probably imagine as our app grows and has more and more settings making sure we copy paste everything correctly could become a challenge. Fortunately, we can take advantage of one of the more advanced features of YAML here to help us: anchors and references. Essentially we can define a separate section of our file that defines the default values a field should take and only override the default when necessary. That looks something like this:</p> src/config/shared/config/config.yaml<pre><code>default: &amp;default # (1)!\n  met_api_url: https://collectionapi-dummy.metmuseum.org\n\ndev: \n  &lt;&lt;: *default # (2)!\n\nqa:\n  &lt;&lt;: *default\n\nprod:\n  &lt;&lt;: *default\n  met_api_url: https://collectionapi.metmuseum.org # (3)!\n</code></pre> <ol> <li>We use the <code>&amp;</code> syntax to define an anchor to this section of the document, we name the anchor the same as the section for simplicity</li> <li>We can break this syntax into two parts: <code>*default</code> which is a reference to the anchor named <code>default</code> and <code>&lt;&lt;:</code> which means this section should inherit all of the values defined in the following reference</li> <li>We then need to explicitly override the <code>met_api_url</code> value in prod</li> </ol> <p>In setting up our file in this way, we reduce the amount of duplicate lines in our configuration file we have to maintain! So now that we have a config file, how do we load it into our code?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html#loading-time","title":"Loading Time","text":"<p>Fortunately Pydantic comes to the rescue again here with the Pydantic Settings extension which makes loading config files a breeze!</p> <p>To get started we can add <code>pydantic-settings</code> as a dependency of our project:</p> <pre><code>uv add pydantic-settings\n</code></pre> <p>Next we can create a new module in our python project under <code>src/shared/config</code> called <code>config_loader.py</code> to handle loading our configuration files. To start we can include a simple Pydantic data model to define our project's settings:</p> src/config/shared/config/config_loader.yaml<pre><code>from pydantic import BaseModel\n\nclass Settings(BaseModel):\n    met_api_url: str\n</code></pre> <p>Note</p> <p>Don't forget to add an empty <code>__init__.py</code> file in the directory to make the module importable!</p> <p>Next, we create a model to define our config file's structure, including each environment:</p> src/config/shared/config/config_loader.yaml<pre><code>from pydantic import BaseModel\n\nclass Settings(BaseModel):\n    met_api_url: str\n\nclass Config(BaseSettings):\n    default: Settings\n    dev: Settings\n    qa: Settings\n    prod: Settings\n</code></pre> <p>So far, so good. Now we need to tell Pydantic where the config file is located on disk and specify the logic for loading it in. Admittedly this isn't very clear in the Pydantic documentation so I'll share what I found to work and then break it down (make sure to scroll over to see all the annotations!):</p> src/config/shared/config/config_loader.yaml<pre><code>import pathlib\nfrom typing import Tuple, Type\nfrom pydantic import BaseModel\nfrom pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, YamlConfigSettingsSource\n\n\nclass Settings(BaseModel):\n    met_api_url: str\n\n\nclass Config(BaseSettings):\n    default: Settings\n    dev: Settings\n    qa: Settings\n    prod: Settings\n    model_config = SettingsConfigDict(yaml_file=pathlib.Path(__file__).parent.resolve() / 'config.yaml') # (1)!\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: Type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; Tuple[PydanticBaseSettingsSource, ...]: # (2)!\n        return (\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n            YamlConfigSettingsSource(settings_cls),\n        )\n</code></pre> <ol> <li> <p><code>model_config</code> is a magic attribute that tells Pydnatic where to look for the configuration settings, it can actually point to many sources but here we are telling it to look for a yaml file. We use a trick I like to call the file loader pattern where we have the file we want to load in the same directory as the script that loads it. This allows us to make the path to the file relative to the current Python file's path (which we can get from the <code>__file__</code> keyword). This means we don't have to worry about what context we are importing the Python file from, it will always be able to find the config file.</p> </li> <li> <p>This method looks scary until you understand what it does. Pydantic Settings allows you to pull configuration values from multiple different sources each with an assigned priority. Pydantic will look for a setting value from the highest priority source first and only look in lower priority sources if it doesn't find it. This allows us to easily override settings if we want to. In this setup, I'm putting the YAML file at the end of the returned tuple so it has the lowest priority. This allows us to effectively override the value of our config setting by setting a value in one of the other sources for example if we need to.</p> </li> </ol> <p>Tip</p> <p>Pydantic Settings supports a wide variety of settings sources including some of the alternative file formats we discussed, so you are free to choose whichever format you like best!</p> <p>Finally we need to provide a function for loading the config when it is needed:</p> src/config/shared/config/config_loader.yaml<pre><code>from functools import lru_cache\nimport pathlib\nfrom typing import Tuple, Type\nfrom pydantic import BaseModel\nfrom pydantic_settings import BaseSettings, PydanticBaseSettingsSource, SettingsConfigDict, YamlConfigSettingsSource\n\n\nclass Settings(BaseModel):\n    met_api_url: str\n\n\nclass Config(BaseSettings):\n    default: Settings\n    dev: Settings\n    qa: Settings\n    prod: Settings\n    model_config = SettingsConfigDict(yaml_file=pathlib.Path(__file__).parent.resolve() / 'config.yaml')\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: Type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -&gt; Tuple[PydanticBaseSettingsSource, ...]:\n        return (\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n            YamlConfigSettingsSource(settings_cls),\n        )\n\n\n@lru_cache # (1)!\ndef load_config_settings(env: str) -&gt; Settings:\n    appconfig = Config()  # type: ignore (2)\n    return getattr(appconfig, env) # (3)!\n</code></pre> <ol> <li>This is an optimization that will only execute this function once and cache the result (provided the arguments to the function do not change). This means we only have to pay the cost of loading the file from disk once and any subsequent calls will load the already cached result.</li> <li>Our linter complains here that we are not passing in all of the model's attributes but we can safely ignore that warning since we will be loading them from our YAML config file</li> <li>This is a trick that allows use to return only the settings for the environment we care about, you can read more about getattr here!</li> </ol> <p>Now, we could make this more robust make making env an <code>enum</code> but this will work for now. Now that we have our config loader class we can use it to load our app settings!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html#powering-our-app-with-settings","title":"Powering our App with Settings!","text":"<p>To use this loader we only need to make a few edits to our <code>main.py</code> function:</p> src/main.py<pre><code>import os\nfrom fastapi import FastAPI, HTTPException\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\nfrom shared.config.config_loader import load_config_settings\n\napp = FastAPI()\napp_settings = load_config_settings(os.getenv('ENV', 'dev')) # (1)!\nsearch_service = SearchService(MetProvider(app_settings.met_api_url)) # (2)!\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n\n    try:\n        search_result = search_service.search_by_title(title)\n        return search_result.primary_image\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n</code></pre> <ol> <li>Here we look for our current environment from an environment variable called <code>ENV</code>, if it is not found we default to <code>dev</code></li> <li>Now we can pull our url from our settings object rather than hard coding it!</li> </ol> <p>Now all that's left is to let our code know what environment it is running in and test out the app! An easy way to do this is create a new file called <code>.env</code> in the root of the project with the following contents:</p> .env<pre><code>ENV=prod\n</code></pre> <p>When you start up a new shell uv is smart enough to load these environment variables for you. Go ahead and try it out! The app should still work as before, try changing the environment to QA and verify that the app breaks when it hits the dummy url! Congratulations, you now have a per-environment configurable app! Full code for this part is available here.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/03/25/delve-10-lets-build-a-modern-ml-microservice-application---part-4-configuration.html#delve-data","title":"Delve Data","text":"<ul> <li>Applications often have a need to change variable values per Deployment Environment</li> <li>Hard coding these values makes this difficult</li> <li>Using tools like Pydantic Settings, we can create a configuration file that makes it easy to switch these values per environment</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html","title":"Delve 11: Let's Build a Modern ML Microservice Application - Part 5, Testing","text":"<p>\"More than the act of testing, the act of designing tests is one of the best bug preventers known.\" - Boris Beizer</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#ml-microservices-keep-calm-and-run-your-tests","title":"ML Microservices, Keep Calm and Run Your Tests","text":"<p>Hello data delvers! In part four of this series we refactored our application to include a configuration file to make it easy to switch configuration values per development environment. In this part we'll cover a critical element to building scalable systems: Testing.  </p> <p>As the complexity of the application grows, so too does the difficulty in verifying that it is behaving as expected. Right now, it is fairly straightforward to test our application. We can bring up the swagger docs, and try a few sample requests to make sure everything is working. However, we can imagine as we add more and more functionality to our app, it will become more tedious to do this type of manual testing every time we make a change to verify nothing has broken. Once more, if something does break, this testing may make it difficult to determine where in our application the break actually occurred (unless we have very, very helpful error messages). A better approach would be to have a set of automated tests that run whenever we make a change to verify nothing has broken. It is this type of testing that I would like to focus on for the subject of this delve.</p> <p>Info</p> <p>There are many types of software testing. For this delve we will be focusing on small subset of testing strategies. Other forms of testing may be covered in future delves.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#lets-get-testy","title":"Let's Get Testy","text":"<p>To begin let's talk about how we might go about writing tests for our application. We could start at the highest level, trying to test the whole application in one go, sending it requests and validating responses. This solves the first problem of having to not run our tests manually anymore, but doesn't solve the second of trying to isolate where the breakage occurred. A different strategy would be to break the application into the smallest pieces possible and test each piece independently of the others and then once the pieces are verified to be working in isolation, test how they work together. In this way, if something breaks we should be able to tell were the breakage occurred because the test for the broken piece should fail. In this type of testing approach we call these pieces of the application Units and this testing strategy Unit Testing.</p> <p>The next question you may be asking is \"Where to begin writing your unit tests?\". This is more personal preference but I like to start at the lowest layers of my application and work my way up. For us that means starting at the Data Layer, though it can be valid to start in the reverse order as well.</p> <p>Note</p> <p>Another valid question to ask is \"When should I write my tests?\". In this series we are following what I'd call the \"conventional\" or \"typical\" route of testing in which we already have working code and we are writing tests to ensure that is it behaving as expected. However, there is an alternative software development philosophy known as Test Driven Development that advocates for the opposite flow: That of writing the tests for a new piece of functionality first, then writing the code that passes the test. Test Driven Development has a lot of advantages, namely forcing the discussion around the desired functionality before any code to implement that functionality has been written. It is worth exploring this approach more and seeing if it aligns better with your own development style than the typical route. It's also worth mentioning that these two approaches are not mutually exclusive and can be mixed and matched as needed.</p> <p>So if we follow this bottom up approach to testing we should begin at the Data Layer of our application with the <code>MetProvider</code> class. In order to do this we will need to install a few more dependencies. Namely pytest, pytest-mock, and pytest-httpx.</p> <p>pytest is the most popular Python unit testing framework, the other two packages are extensions that will make it a bit easier to work with our codebase.</p> <p>Tip</p> <p>When installing these dependencies notice that we only need them for running our tests or more generally development activities. We don't need them for actually executing the functionality of our application itself. In this case we can take advantage of a feature of <code>uv</code> know as development dependencies to mark these libraries as development only dependencies. This is done by adding the <code>--dev</code> flag when installing them. For example <code>uv add --dev pytest</code>. It is recommended to install these dependencies in this way.</p> <p>With our development dependencies installed we then need to add a bit of configuration to our <code>pyproject.toml</code> file to tell pytest where our tests will be located.</p> pyproject.toml<pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"fastapi[standard]&gt;=0.115.6\",\n    \"httpx&gt;=0.28.1\",\n    \"pydantic-settings&gt;=2.7.1\",\n]\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.format]\nquote-style = \"single\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\npythonpath = \"src\"\ntestpaths = [\n    \"tests\",\n]\npython_files = [\n    \"test_*.py\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=8.3.5\",\n    \"pytest-mock&gt;=3.14.0\",\n    \"pytest-httpx&gt;=0.35.0\",\n]\n</code></pre> <p>Here we are telling pytest that our source code is located in a file called <code>src</code> that will need to be added to the python path when executing tests (this makes sure our imports will work). We are also specifying that all our tests will we located in a folder called <code>tests</code> and that the test scripts themselves will start with the prefix <code>test_</code>. We are also specifying a minimum version of pytest to use of 6.0 (the earliest version that supported using the pyproject.toml to specify these settings). With all the setup out of the way, let's start writing tests!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#mocking-and-rolling-the-art-of-unit-testing","title":"Mocking and Rolling: The Art of Unit Testing","text":"<p>To start, let's review the code of the provider class:</p> src/provider/met_provider.py<pre><code>from datetime import datetime\nfrom typing import Optional\nimport httpx\n\n\nfrom shared.view.met_view import DepartmentResponse, ObjectResponse, ObjectsResponse, SearchResponse\n\n\nclass MetProvider:\n    \"\"\"A client for the Metropolitan Museum of Art API.\n\n    Args:\n        base_url: The base URL of the API.\n    \"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def get_objects(\n        self, metadata_date: Optional[datetime] = None, department_ids: Optional[list[int]] = None\n    ) -&gt; ObjectsResponse:\n        \"\"\"Retrieves objects from the Metropolitan Museum of Art API.\n\n        Args:\n            metadata_date: Returns any objects with updated data after this date.\n            department_ids: Returns any objects in a specific department.\n\n        Returns:\n            A list of objects.\n        \"\"\"\n\n        query_params = {}\n\n        if metadata_date:\n            query_params['metadataDate'] = metadata_date.strftime('%Y-%m-%d')\n        if department_ids:\n            query_params['departmentIds'] = '|'.join(map(str, department_ids))\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/objects',\n            params=query_params,\n        )\n\n        return ObjectsResponse.model_validate(r.json())\n\n    def get_object(self, object_id: int) -&gt; ObjectResponse:\n        \"\"\"Retrieves an object from the Metropolitan Museum of Art API.\n\n        Args:\n            object_id: The ID of the object to retrieve.\n\n        Returns:\n            The object.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/objects/{object_id}')\n        return ObjectResponse.model_validate(r.json())\n\n    def get_departments(self) -&gt; DepartmentResponse:\n        \"\"\"Retrieves departments from the Metropolitan Museum of Art API.\n\n        Returns:\n            A list of departments.\n        \"\"\"\n\n        r = httpx.get(f'{self.base_url}/public/collection/v1/departments')\n        return DepartmentResponse.model_validate(r.json())\n\n    def search(self, q: str, title: Optional[bool] = None, has_images: Optional[bool] = None) -&gt; SearchResponse:\n        \"\"\"Executes a search against the Metropolitan Museum of Art API.\n\n        Args:\n            q: The query string.\n            title: Whether to search the title field.\n            has_images: Whether to search for objects with images.\n\n        Returns:\n            The search results.\n        \"\"\"\n\n        query_params = {'q': q}\n\n        if title is not None:\n            query_params['title'] = str(title).lower()\n\n        if has_images is not None:\n            query_params['hasImages'] = str(has_images).lower()\n\n        r = httpx.get(\n            f'{self.base_url}/public/collection/v1/search',\n            params=query_params,\n        )\n\n        return SearchResponse.model_validate(r.json())\n</code></pre> <p>We have four methods to test:</p> <ul> <li><code>get_objects()</code></li> <li><code>get_object()</code></li> <li><code>get_departments()</code></li> <li><code>search()</code></li> </ul> <p>Each should get a corresponding unit test.  We can go ahead a create a file to hold our tests located at <code>tests/unit/provider/test_met_provider.py</code>. Take note of the directory structure. We locate the tests in the <code>tests</code> folder as we configured in the <code>pyproject.toml</code>, then under a folder called <code>unit</code> (since this is a unit test), then finally we mirror the directory structure of the <code>src</code> folder so that the corresponding test will be located in a folder hierarchy structure identical as the script it is testing. This is not required but I find it makes it easier to understand were to look in the source code if a test fails.</p> <p>Note</p> <p>Unlike the <code>src</code> folder where every directory needed an empty <code>__init__.py</code> file within it to mark it as a Python module, there is no need to do this for <code>tests</code> directories.</p> <p>Let's write a test!</p> tests/unit/provider/test_met_provider.py<pre><code>from provider.met_provider import MetProvider\n\ndef test_get_objects() -&gt; None:\n    \"\"\"Test the get_objects method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = MetProvider('https://collectionapi.metmuseum.org')\n\n    # WHEN\n    response = provider.get_objects()\n\n    # THEN\n    assert response.total == 495439\n</code></pre> <p>When writing tests I like to follow a structure made popular by the Behavior Driven Development testing philosophy. Each test case has three sections:</p> <ul> <li>Given - Initial set of conditions</li> <li>When - The test action occurs</li> <li>Then - Validate that the desired behavior has happened</li> </ul> <p>In this way you can write a test case as a simple sentence. For example the above test could be written as \"Given a Met Provider connected to the Met API, when I call the get objects method, then I should get 495,439 results back.\" Now, in writing the test case in this way you might already see the problem with this test. As of right now, when I call this route on the Met API I get 495,439 results, but what happens if the Met adds another work to their collection? I would then get 495,440 results back and this test would fail even though nothing is wrong with the code. This demonstrates an important principle of unit testing, that tests should be written in such a way that they test the unit in isolation and should not be dependent on the state of any external system to the unit in order for the test to succeed. So how can we address this?</p> <p>Note</p> <p>This quality of independence from external systems is not always prohibited and is even desired for types of testing other than unit testing.</p> <p>Well, what if instead of calling the real Met API we could have a dummy API instead, and even better, we could control the output of this dummy API to make it deterministic so we could write test cases against it? In that way we could ensure that the Met API for this use case will always return the same number of results, which means if the test fails there is something wrong with the logic of the provider itself, which is exactly what we want to test! As you may guess, this is entirely possible, this process of creating dummy objects for the purposes of testing is called mocking and we can use the pytest extensions we previously installed to create our mock Met API. To do this we use <code>httpx-mock</code>. We can create a Mocked provider to use in our tests like so:</p> tests/unit/provider/test_met_provider.py<pre><code>from provider.met_provider import MetProvider\n\ndef test_get_objects(httpx_mock) -&gt; None:\n    \"\"\"Test the get_objects method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    dummy_url = 'https://collectionapi-dummy.metmuseum.org'\n    # Mock the response for the get_objects method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        }\n    )\n    provider = MetProvider(dummy_url)\n\n    # WHEN\n    response = provider.get_objects()\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n</code></pre> <p>Here we take advantage of the special <code>httpx_mock</code> fixture to create a dummy response whenever a request is made to the url <code>https://collectionapi-dummy.metmuseum.org/public/collection/v1/objects</code>. We then use this dummy response to make assertions in our tests. Now we don't have to worry about the Met API changing their collection size and breaking our tests!</p> <p>We can go ahead and run this test by simply executing the <code>pytest</code> command in the root of the project or by using the testing panel of VSCode.</p> <p>Now we could go ahead an repeat this logic for every test we want to write but it will become tedious to mock out the provider every time. Another way we could do this is create the mocked provider in a test fixture and re-use it in all of our tests. That would look something like this:</p> tests/unit/provider/test_met_provider.py<pre><code>from provider.met_provider import MetProvider\n\n@pytest.fixture\ndef provider_with_mock_api(httpx_mock) -&gt; MetProvider:\n    \"\"\"Mock responses for the Metropolitan Museum of Art API.\"\"\"\n\n    dummy_url = 'https://collectionapi-dummy.metmuseum.org'\n\n    # Mock the response for the get_objects method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n        is_optional=True,\n    )\n\n    return MetProvider(dummy_url)\n\ndef test_get_objects(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the get_objects method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # WHEN\n    response = provider.get_objects()\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n</code></pre> <p>Now every test that needs the provider mocked out can simply take in the <code>provider_with_mock_api</code> argument. With this pattern in hand we can go ahead and write the rest of our test cases:</p> tests/unit/provider/test_met_provider.py<pre><code>from provider.met_provider import MetProvider\n\n@pytest.fixture\ndef provider_with_mock_api(httpx_mock) -&gt; MetProvider:\n    \"\"\"Mock responses for the Metropolitan Museum of Art API.\"\"\"\n\n    dummy_url = 'https://collectionapi-dummy.metmuseum.org'\n\n    # Mock the response for the get_objects method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the get_object method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects/1',\n        json={\n            'objectID': 1,\n            'title': 'Test Object',\n            'primaryImage': 'https://example.com/image.jpg',\n            'additionalImages': [\n                'https://example.com/image1.jpg',\n                'https://example.com/image2.jpg',\n            ],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the get_departments method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/departments',\n        json={\n            'departments': [\n                {\n                    'departmentId': 1,\n                    'displayName': 'Test Department',\n                },\n            ],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the search method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/search?q=Test Title',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n        is_optional=True,\n    )\n\n    return MetProvider(dummy_url)\n\ndef test_get_objects(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the get_objects method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # WHEN\n    response = provider.get_objects()\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n\ndef test_get_object(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the get_object method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # WHEN\n    response = provider.get_object(1)\n\n    # THEN\n    assert response.object_id == 1\n    assert response.title == 'Test Object'\n\ndef test_get_departments(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the get_departments method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # WHEN\n    response = provider.get_departments()\n\n    # THEN\n    assert len(response.departments) == 1\n    assert response.departments[0].department_id == 1\n\ndef test_search(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the search method of the MetProvider class.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # WHEN\n    response = provider.search('Test Title')\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n</code></pre> <p>Note that we set <code>is_optional</code> to <code>True</code> for our mocked responses in the fixture, this is because not every mock response will be used for every test.</p> <p>Tip</p> <p>I find code generation tools like GitHub Copilot particularly good at generating test cases. I encourage you to try them out, they have saved me a lot of time!</p> <p>With our provider methods all tested we can move on to the Business Logic Layer and test our <code>SearchService</code> class.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#mock-all-the-things","title":"Mock all the Things!","text":"<p>Our search service just has one method to test <code>search_by_title</code>. As a reminder, this is what the source code looks like:</p> src/service/search_service.py<pre><code>from provider.met_provider import MetProvider\nfrom shared.dto.search_result import SearchResult\n\n\nclass SearchService:\n    \"\"\"A service for searching the Metropolitan Museum of Art API.\n\n    Args:\n        met_provider: A client for the Metropolitan Museum of Art API.\n    \"\"\"\n\n    def __init__(self, met_provider: MetProvider):\n        self.met_provider = met_provider\n\n    def search_by_title(self, title: str) -&gt; SearchResult:\n        \"\"\"Searches the Metropolitan Museum of Art API by title.\n\n        Args:\n            title: The title of the work to search for.\n\n        Returns:\n            The search results.\n\n        Raises:\n            ValueError: If no results are found.\n        \"\"\"\n\n        # Search for a work in the Met collection by title\n        search_response = self.met_provider.search(q=title)\n        object_ids = search_response.object_ids\n\n        # If the work exists\n        if object_ids:\n            # Fetch the details of the work\n            object_request = self.met_provider.get_object(object_id=object_ids[0])\n\n            return SearchResult(\n                object_id=object_request.object_id,\n                title=object_request.title,\n                primary_image=object_request.primary_image,\n                additional_images=object_request.additional_images,\n                total_results=search_response.total,\n            )\n        else:\n            raise ValueError('No results found.')\n</code></pre> <p>For our <code>SearchService</code> test we can go ahead and create a new file located at <code>tests/unit/service/test_search_service.py</code> to hold our tests. Now, we could re-use our same <code>provider_with_mock_api</code> fixture here as well. That would look something like this:</p> tests/unit/service/test_search_service.py<pre><code>from provider.met_provider import MetProvider\nfrom service.search_service import SearchService\n\n# Copy-Pasted provider_with_mock_api\n\ndef test_search_by_title(provider_with_mock_api: MetProvider) -&gt; None:\n    \"\"\"Test the search_by_title method of the SearchService class.\"\"\"\n\n    # GIVEN\n    service = SearchService(provider_with_mock_api)\n    title = 'Test Title'\n\n    # WHEN\n    result = service.search_by_title(title)\n\n    # THEN\n    assert result.object_id == 1\n    assert result.title == 'Test Object'\n    assert result.primary_image == 'https://example.com/image.jpg'\n    assert result.additional_images == ['https://example.com/image1.jpg', 'https://example.com/image2.jpg']\n    assert result.total_results == 1\n</code></pre> <p>However, we now created a similar problem as we had before. What if we have a bug in the <code>MetProvider</code> class? Now this test could fail even if there is nothing wrong with the <code>SearchService</code>! We can solve this dilemma in a similar manner as before, by creating a mock! Though this time since we are not mocking http requests and responses but a class instead we can use the <code>pytest-mock</code> extension. That looks something like this:</p> tests/unit/service/test_search_service.py<pre><code>from unittest.mock import MagicMock\nimport pytest\nfrom pytest_mock import MockerFixture\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\nfrom shared.view.met_view import DepartmentResponse, ObjectResponse, ObjectsResponse, SearchResponse\n\n\n@pytest.fixture\ndef mock_provider(mocker: MockerFixture) -&gt; MagicMock:\n    mock = mocker.MagicMock(MetProvider)\n    mock.get_objects.return_value = ObjectsResponse.model_validate(\n        {\n            'total': 1,\n            'objectIDs': [1],\n        }\n    )\n    mock.get_object.return_value = ObjectResponse.model_validate(\n        {\n            'objectID': 1,\n            'title': 'Test Object',\n            'primaryImage': 'https://example.com/image.jpg',\n            'additionalImages': ['https://example.com/image1.jpg', 'https://example.com/image2.jpg'],\n        }\n    )\n    mock.get_departments.return_value = DepartmentResponse.model_validate(\n        {\n            'departments': [\n                {\n                    'departmentId': 1,\n                    'displayName': 'Test Department',\n                },\n            ],\n        }\n    )\n    mock.search.return_value = SearchResponse.model_validate(\n        {\n            'total': 1,\n            'objectIDs': [1],\n        }\n    )\n    return mock \n</code></pre> <p>We create this mock as a fixture as we did before so we can re-use it in multiple tests. We also take advantage of the <code>MagicMock</code> functionality of pytest-mock to easily create a dummy version of the <code>MetProvider</code> and stub out mock responses for each of its methods. We can then use this mock in our test like so:</p> tests/unit/service/test_search_service.py<pre><code>def test_search_by_title(mock_provider: MagicMock) -&gt; None:\n    \"\"\"Test the search_by_title method of the SearchService class.\"\"\"\n\n    # GIVEN\n    service = SearchService(mock_provider)\n    title = 'Test Title'\n\n    # WHEN\n    result = service.search_by_title(title)\n\n    # THEN\n    assert result.object_id == 1\n    assert result.title == 'Test Object'\n    assert result.primary_image == 'https://example.com/image.jpg'\n    assert result.additional_images == ['https://example.com/image1.jpg', 'https://example.com/image2.jpg']\n    assert result.total_results == 1\n    mock_provider.search.assert_called_once_with(q=title)\n</code></pre> <p>One other benefit of this type of mock is we can also make assertions if methods on the mock were called and what arguments were passed into them as we are doing with the <code>assert_called_once_with</code> method. Neat!</p> <p>This test covers the case where we have results to return but noticed that the <code>SearchService</code> should raise as <code>ValueError</code> in the case that no results are found. How can we test this behavior as well?</p> <p>Note</p> <p>You will sometimes hear the terms \"Happy Path\" and \"Sad Path\" used in the context of these types of tests.  In this example the happy path is the case where things go \"right\" and we have results to return and the sad path is where things go \"wrong\" and we raise an exception. Ideally you should cover both happy and sad paths in your test cases with each path being it's own test case.</p> <p>It turns out pytest has a <code>raises</code> function that can be used in a context manager to implement this exact type of test:</p> tests/unit/service/test_search_service.py<pre><code>def test_search_by_title_no_results(mock_provider: MagicMock) -&gt; None:\n    \"\"\"Test the search_by_title method of the SearchService class when no results are found.\"\"\"\n\n    # GIVEN\n    service = SearchService(mock_provider)\n    title = 'Nonexistent Title'\n    mock_provider.search.return_value = SearchResponse.model_validate(\n        {\n            'total': 0,\n            'objectIDs': [],\n        }\n    )\n\n    # WHEN / THEN\n    with pytest.raises(ValueError, match='No results found.'):\n        service.search_by_title(title)\n    mock_provider.search.assert_called_once_with(q=title)\n</code></pre> <p>A few things to note here is we have to override the return value of the search function of our mock to simulate no results. We can also verify we get back the expected error message by passing it into the <code>match</code> argument of the <code>raises</code> function. Finally given the structure of the code I often combine the When and Then sections of the test into one.</p> <p>That wraps up the tests of our business logic layer! We only have one more layer to go: The Interface Layer.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#bringing-unit-tests-to-the-interface","title":"Bringing Unit Tests to the Interface","text":"<p>By this point you should know the drill, we don't want to use the <code>SearchService</code> directly in our tests, instead we want to create a mock instead. We can take a look at the code of our main script:</p> src/main.py<pre><code>import os\nfrom fastapi import FastAPI, HTTPException\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\nfrom shared.config.config_loader import load_config_settings\n\napp = FastAPI()\napp_settings = load_config_settings(os.getenv('ENV', 'dev'))\nsearch_service = SearchService(MetProvider(app_settings.met_api_url))\n\n\n@app.get('/api/search')\ndef search(title: str) -&gt; str:\n    \"\"\"Executes a search against the Metropolitan Museum of Art API and returns the url of the primary image of the first search result.\n\n    Args:\n        title: The title of the work you wish to search for.\n\n    Returns:\n        The url of the primary image of the first search result or 'No results found.' if no search results are found.\n    \"\"\"\n\n    try:\n        search_result = search_service.search_by_title(title)\n        return search_result.primary_image\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n</code></pre> <p>Looks pretty simple. We have both a happy path of returning the primary image url and a sad path where we return a 404 status code. We'll want to test both. FastAPI fortunately provides a convenient Test Client we can use to invoke the functions in our main script without needing to rig up making HTTP requests ourselves. Putting this together we can create a simple test script as follows:</p> tests/test_main.py<pre><code>from main import app\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import MagicMock\nimport pytest\nfrom pytest_mock import MockerFixture\n\nfrom service.search_service import SearchService\n\n\n@pytest.fixture\ndef mock_search_service(mocker: MockerFixture) -&gt; MagicMock:\n    \"\"\"Mock the SearchService class.\"\"\"\n\n    mock = MagicMock(SearchService)\n    mock.search_by_title.return_value.primary_image = 'https://example.com/image.jpg'\n    return mock\n\n\ndef test_search(mock_search_service: MagicMock, mocker: MockerFixture) -&gt; None:\n    \"\"\"Test the search endpoint.\"\"\"\n\n    # GIVEN\n    client = TestClient(app)\n    mocker.patch('main.search_service', mock_search_service)\n    title = 'Test Title'\n\n    # WHEN\n    response = client.get(f'/api/search?title={title}')\n\n    # THEN\n    assert response.status_code == 200\n    assert response.json() == 'https://example.com/image.jpg'\n    mock_search_service.search_by_title.assert_called_once_with(title)\n\n\ndef test_search_no_results(mock_search_service: MagicMock, mocker: MockerFixture) -&gt; None:\n    \"\"\"Test the search endpoint when no results are found.\"\"\"\n\n    # GIVEN\n    client = TestClient(app)\n    mocker.patch('main.search_service', mock_search_service)\n    mock_search_service.search_by_title.side_effect = ValueError('No results found.')\n    title = 'Test Title'\n\n    # WHEN\n    response = client.get(f'/api/search?title={title}')\n\n    # THEN\n    assert response.status_code == 404\n    assert response.json() == {'detail': 'No results found.'}\n</code></pre> <p>Notice how we can use the <code>TestClient</code> to make requests against our API and make assertions on the responses. We also use <code>mocker.patch</code> to replace the search service of the app after it has been created. Finally in the case where we want the search to raise an exception we can use the <code>side_effect</code> attribute of the mock. With that we have a decent suite of unit tests for our code! But how can we know how much of the code we are testing?</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#no-code-left-behind-test-coverage","title":"No Code Left Behind: Test Coverage","text":"<p>The question above relates to the concept of Code Coverage. There are many different ways to compute code coverage but perhaps one of the simplest is measuring the percentage of lines of code executed as a result of running all of your tests. Fortunately, the pytest-cov extension does exactly that. Go ahead an install it as a development dependency of the project.</p> <p>We can then add another section to our <code>pyproject.toml</code> to configure it like so:</p> pyproject.toml<pre><code>[tool.coverage.run]\nomit = [\n    \"tests\",\n]\nsource = [\n    \"src\",\n]\n\n[tool.coverage.report]\nfail_under = 60 \nshow_missing = true\nskip_empty = true\n</code></pre> <p>You can read more about these config options here. An important one to call out is the <code>fail_under</code> setting. This represents a coverage threshold under which the test suite will be marked as a failure. This can be useful to make sure un-tested code doesn't accidentally get released! I generally like to set this to 60% with a goal of getting to at least 80%. Let's go ahead and run our tests and see were we are now:</p> <pre><code>$pytest --cov\n==================================================================================== test session starts =====================================================================================\nplatform linux -- Python 3.13.1, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/datadelver/Documents/PythonProjects/DataDelver/modern-ml-microservices\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: httpx-0.35.0, anyio-4.7.0, mock-3.14.0, cov-6.0.0\ncollected 9 items                                                                                                                                                                           \n                                                                                                                                                 [ 20%]\ntests/unit/provider/test_met_provider.py ....                                                                                                                                          [ 60%]\ntests/unit/service/test_search_service.py ..                                                                                                                                           [ 80%]\ntests/unit/test_main.py ..                                                                                                                                                             [100%]\n\n---------- coverage: platform linux, python 3.13.1-final-0 -----------\nName                                 Stmts   Miss  Cover   Missing\n------------------------------------------------------------------\nsrc/main.py                             15      0   100%\nsrc/provider/met_provider.py            29      4    86%   35, 37, 84, 87\nsrc/service/search_service.py           12      0   100%\nsrc/shared/config/config_loader.py      20      0   100%\nsrc/shared/data_model_base.py            6      0   100%\nsrc/shared/dto/search_result.py          7      0   100%\nsrc/shared/view/met_view.py             19      0   100%\n------------------------------------------------------------------\nTOTAL                                  108      4    96%\n\n6 empty files skipped.\n\nRequired test coverage of 60.0% reached. Total coverage: 96.30%\n</code></pre> <p>Tip</p> <p>You can also use the Run with Test Coverage option in the VSCode testing panel to get a nice visual display of coverage!</p> <p>We are doing pretty good, but it looks like we are missing a few lines in the met provider. Taking a look we can see those lines are related to optional parameters that can get passed in. Let's add some tests to our <code>test_met_provider.py</code> script to fix this.</p> tests/unit/provider/test_met_provider.py<pre><code>def test_get_objects_with_metadata_date_and_department_ids(provider_with_mock_api: MetProvider, httpx_mock) -&gt; None:\n    \"\"\"Test the get_objects method of the MetProvider class with metadata date.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n    metadata_date = datetime(day=1, month=1, year=2023)\n    department_ids = [1]\n    # Mock the response for the get_objects method with metadata date\n    httpx_mock.add_response(\n        url=f'{provider.base_url}/public/collection/v1/objects?metadataDate=2023-01-01&amp;departmentIds=1',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n    )\n\n    # WHEN\n    response = provider.get_objects(metadata_date=metadata_date, department_ids=department_ids)\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n\ndef test_search_with_title_and_has_images(provider_with_mock_api: MetProvider, httpx_mock) -&gt; None:\n    \"\"\"Test the search method of the MetProvider class with title and has_images.\"\"\"\n\n    # GIVEN\n    provider = provider_with_mock_api\n\n    # Mock the response for the search method with title and has_images\n    httpx_mock.add_response(\n        url=f'{provider.base_url}/public/collection/v1/search?q=Test+Title&amp;title=true&amp;hasImages=true',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n    )\n\n    # WHEN\n    response = provider.search(q='Test Title', title=True, has_images=True)\n\n    # THEN\n    assert response.total == 1\n    assert response.object_ids == [1]\n</code></pre> <p>Let's re-run our tests and see what we get:</p> <pre><code>$pytest --cov\n==================================================================================== test session starts =====================================================================================\nplatform linux -- Python 3.13.1, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/datadelver/Documents/PythonProjects/DataDelver/modern-ml-microservices\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: httpx-0.35.0, anyio-4.7.0, mock-3.14.0, cov-6.0.0\ncollected 11 items                                                                                                                                                                           \n                                                                                                                                                  [ 16%]\ntests/unit/provider/test_met_provider.py ......                                                                                                                                        [ 66%]\ntests/unit/service/test_search_service.py ..                                                                                                                                           [ 83%]\ntests/unit/test_main.py ..                                                                                                                                                             [100%]\n\n---------- coverage: platform linux, python 3.13.1-final-0 -----------\nName                                 Stmts   Miss  Cover   Missing\n------------------------------------------------------------------\nsrc/main.py                             15      0   100%\nsrc/provider/met_provider.py            29      0   100%\nsrc/service/search_service.py           12      0   100%\nsrc/shared/config/config_loader.py      20      0   100%\nsrc/shared/data_model_base.py            6      0   100%\nsrc/shared/dto/search_result.py          7      0   100%\nsrc/shared/view/met_view.py             19      0   100%\n------------------------------------------------------------------\nTOTAL                                  108      0   100%\n\n6 empty files skipped.\n\nRequired test coverage of 60.0% reached. Total coverage: 100.00%\n</code></pre> <p>Nice 100%! Don't let this lull you into a false sense of security though, 100% coverage does not necessarily mean you have good tests or that there are no bugs. As with all metrics they are simply a tool for you to use, not a target. In practice I rarely get to 100% coverage on more complex codebases (though generally above 80% is a good place to shoot for). </p> <p>With that we have successfully unit tested our code, but we aren't done yet!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#testing-together-because-no-unit-is-an-island","title":"Testing Together: Because No Unit Is an Island","text":"<p>So far we have tested how each component of our application has worked in isolation, but we haven't tested how they work together. This type of testing falls under the category of Integration Testing and is similar to the premise we first started with. Of making test calls to our application and verifying the responses. In this way, we can ensure all the layers are operating together correctly.</p> <p>To start off we need to tell pytest we are going to have a different type of test now. I like to prefix my integration tests with <code>inttest_</code> instead of <code>test_</code> so we need to update our config appropriately:</p> pyproject.toml<pre><code>[tool.pytest.ini_options]\nminversion = \"6.0\"\npythonpath = \"src\"\ntestpaths = [\n    \"tests\",\n]\npython_files = [\n    \"test_*.py\",\n    \"inttest_*.py\",\n]\n</code></pre> <p>Next, we'll create a test that looks very similar to our unit test for the main script but without the mock search service.</p> tests/unit/provider/test_met_provider.py<pre><code>from main import app\nfrom fastapi.testclient import TestClient\nimport pytest\nfrom pytest_mock import MockerFixture\n\nfrom provider.met_provider import MetProvider\nfrom service.search_service import SearchService\n\n\n@pytest.fixture\ndef search_service(provider_with_mock_api: MetProvider) -&gt; SearchService:\n    \"\"\"Fixture to provide a mocked SearchService instance.\"\"\"\n    return SearchService(provider_with_mock_api)\n\n\ndef test_search(search_service: SearchService, mocker: MockerFixture) -&gt; None:\n    \"\"\"Test the search endpoint.\"\"\"\n\n    # GIVEN\n    client = TestClient(app)\n    mocker.patch('main.search_service', search_service)\n    title = 'Test Title'\n\n    # WHEN\n    response = client.get(f'/api/search?title={title}')\n\n    # THEN\n    assert response.status_code == 200\n    assert response.json() == 'https://example.com/image.jpg'\n\n\ndef test_search_no_results(search_service: SearchService, mocker: MockerFixture, httpx_mock) -&gt; None:\n    \"\"\"Test the search endpoint when no results are found.\"\"\"\n\n    # GIVEN\n    client = TestClient(app)\n    mocker.patch('main.search_service', search_service)\n    httpx_mock.add_response(\n        url=f'{search_service.met_provider.base_url}/public/collection/v1/search?q=Test No Results Title',\n        json={\n            'total': 0,\n            'objectIDs': [],\n        },\n    )\n    title = 'Test No Results Title'\n\n    # WHEN\n    response = client.get(f'/api/search?title={title}')\n\n    # THEN\n    assert response.status_code == 404\n    assert response.json() == {'detail': 'No results found.'}\n</code></pre> <p>Let's break this down. We still don't want to hit the real Met API so we still need our <code>provider_with_mock_api</code> fixture, but now we want to create a <code>SearchService</code> instance that uses it, which we do in the <code>search_service</code> fixture. In this way we will execute all the layers of the app up until the API call and be able to validate that they are working as expected. For the sad path we also have to provide a slightly different response that returns no results to trigger the error condition. </p> <p>Now, you may have noticed that I didn't copy paste the <code>provider_with_mock_api</code> fixture here, how can this be? Well, pytest provides a special file called <code>conftest.py</code> that gets run before the test suite is executed (not before every test). It's the perfect place to put shared fixtures we want to use in different test scripts. Go ahead and move the <code>provider_with_mock_api</code> fixture to a file called <code>tests/conftest.py</code> like so:</p> tests/conftest.py<pre><code>import pytest\n\nfrom provider.met_provider import MetProvider\n\n\n@pytest.fixture\ndef provider_with_mock_api(httpx_mock) -&gt; MetProvider:\n    \"\"\"Mock responses for the Metropolitan Museum of Art API.\"\"\"\n\n    dummy_url = 'https://collectionapi-dummy.metmuseum.org'\n\n    # Mock the response for the get_objects method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the get_object method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/objects/1',\n        json={\n            'objectID': 1,\n            'title': 'Test Object',\n            'primaryImage': 'https://example.com/image.jpg',\n            'additionalImages': [\n                'https://example.com/image1.jpg',\n                'https://example.com/image2.jpg',\n            ],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the get_departments method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/departments',\n        json={\n            'departments': [\n                {\n                    'departmentId': 1,\n                    'displayName': 'Test Department',\n                },\n            ],\n        },\n        is_optional=True,\n    )\n\n    # Mock the response for the search method\n    httpx_mock.add_response(\n        url=f'{dummy_url}/public/collection/v1/search?q=Test Title',\n        json={\n            'total': 1,\n            'objectIDs': [1],\n        },\n        is_optional=True,\n    )\n\n    return MetProvider(dummy_url)\n</code></pre> <p>Now we can use this fixture in all of our tests!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#just-the-beginning","title":"Just the Beginning","text":"<p>This wraps up our brief delve into the world of automated software testing! We just scratched the surface of what is possible here but hopefully it gives you enough to get started! In the future we may cover other types of testing, let me know in the comments below if that's something you'd like to see! Full code for this part is available here!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/04/13/delve-11-lets-build-a-modern-ml-microservice-application---part-5-testing.html#delve-data","title":"Delve Data","text":"<ul> <li>There are many types of software testing strategies available to validate that sofware is behaving as expected</li> <li>Unit testing seeks to test each piece of the application in isolation</li> <li>Integration testing seeks to test how each piece of the application works together</li> <li>Tools like pytest and its extensions help automate this testing process</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html","title":"Delve 12: Let's Build a Modern ML Microservice Application - Part 6, Containerization","text":"<p>\"This containers revolution is changing the basic act of software consumption. It\u2019s redefining this much more lightweight, portable unit, or atom, that is much easier to manage\u2026 It\u2019s a gateway to dynamic management and dynamic systems.\" - Craig McLuckie</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#ml-microservices-deployment-through-docker","title":"ML Microservices, Deployment through Docker","text":"<p>Hello data delvers! In part five of this series we added automated tests to our application to make it easier to catch bugs. In this part we'll cover packaging and deploying the app!  </p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#from-near-to-afar-or-how-to-leave-my-local-machine","title":"From Near to Afar, Or How to Leave my Local Machine","text":"<p>Up until this point, we have been executing our application on our local machine with the command line or the built in VSCode runner, but we aren't going to ship our whole local computer to production! We need a simple way to package our application with its dependencies in a way that makes it easy to deploy (either in an on-prem computing environment or with a cloud provider). By this point you've probably heard of containerization, most likely through its most popular implementation: Docker. Fortunately for us, Docker is the perfect tool to package our app. It provides a convenient way to package all the Python dependencies our application needs, provide an environment to execute our Fast API application, and deploy the application to our desired computing environment. So let's get started!</p> <p>Firstly, you will need to install Docker on your machine if you have not done so already. Docker Desktop is the official tooling provided by Docker to achieve this, however due to some drama about commercial use licensing of Docker Desktop, I actually prefer to use Rancher Desktop as an alternative with a more permissive license. Whichever you prefer, install it and make sure you have the WSL integration enabled if you plan to use it within your WSL environment (if you are on a Windows machine). You can verify that Docker is correctly installed by opening a shell and running the <code>docker --version</code> command. You should see something like the below output:</p> <pre><code>$docker --version\nDocker version 27.5.1-rd, build 0c97515\n</code></pre> <p>Next, you'll want to grab the official Container Tools Extension for VSCode and with that your development environment should be all set up!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#dock-and-load-packing-microservices-like-a-pro","title":"Dock and Load: Packing Microservices Like a Pro","text":"<p>To containerize our application using Docker we must create a <code>Dockerfile</code> which serves as a set of instructions for building your container image. An image is to a container what as class is to an object in OOP. The image is the static definition of the \"object\" (in this case our application) while the container is a running instance of that image. When creating Dockerfiles it is common to build them on top of existing Docker images, in this way you don't need to specify all of the dependencies of your underlying operating system for example, you can just build on top of an image that already has all of the OS dependencies installed. Fortunately for us uv provides a prebuilt Docker Image we can use for the base of our our Dockerfile as well as an example Dockerfile, a slightly modified version of which is provided below:</p> Dockerfile<pre><code>FROM ghcr.io/astral-sh/uv:python3.13-bookworm-slim\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"80\"]\n</code></pre> <p>An explanation of many of the optimizations present in the above file are provided in the uv documentation. For our purposes the only things to note are that we use <code>COPY</code> instead of <code>ADD</code> (which is the best practice recommended by Docker) and we modify the last line from the example provided by uv to be a \"prod\" configuration by not running Fast API in dev mode as well as specifying a port for the application to run on rather than a host of <code>0.0.0.0</code>. We can go ahead an add the above file to the root of our project and build the container by running:</p> <pre><code>docker build -t modern-ml-microservices-app .\n</code></pre> <p>This builds our application image with the tag <code>modern-ml-microservices-app</code> which allows us to then reference that tag to run the application like so:</p> <pre><code>docker run -p 80:80 -e ENV=prod modern-ml-microservices-app\n</code></pre> <p>This command does a few things:</p> <ul> <li>Binds port 80 on the host machine to port 80 within the container, this allows us to easily access the application on the host's network.</li> <li>Sets the <code>ENV</code> environment to <code>prod</code> to tell our application to run in a production configuration</li> <li>Runs the application</li> </ul> <p>With that you should be able to navigate to http://localhost/docs and view your running application, now in a docker container!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#optimize","title":"Optimize!","text":"<p>With our app containerized you have everything you need to deploy it on your compute platform of choice. However, before we do there are a few more optimizations we could do. Firstly, when we copy our application files to the container we are copying the entire project, including files the application does not need to execute such as the application README. Such files do nothing but increase the size of our container. We could copy the individual files we need one by one, but Docker provides a <code>.dockerignore</code> file which works exactly like a .gitignore file and allows excluding files which match a specific pattern from the build context. For example, we could add a <code>.dockerignore</code> file with the following contents to the root of our project to exclude unneeded files:</p> .dockerignore<pre><code>__pycache__\n.pytest_cache\n.venv\n.vscode\ndata\ndeployment\ntests\n.env\n.gitignore\n.git\nLICENSE\nREADME.md\n</code></pre> <p>Finally, when building your project it's best practice to also run all of your tests. Instead of remembering to run both commands I like to create a bash script that automates this process (that we can also use as part of our CI/CD pipeline later \ud83d\ude09). To do that we can create a new folder called <code>deployment</code> to hold our deployment scripts, and create an <code>app-build.sh</code> script to execute that process:</p> deployment/app-build.sh<pre><code>#! /usr/bin/env bash\n\n# This script builds a Docker image for the app.\n# Usage: ./app-build.sh\n\ncd $(dirname \"$0\")/.. || exit 1\n\n# Run tests\npytest --cov\n\n# Build the Docker image for the app\ndocker build -t modern-ml-microservices-app .\n</code></pre> <p>You can then execute the script from the root of your project like so:</p> <pre><code>./deployment/app-build.sh \n</code></pre>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#reaching-a-basecamp","title":"Reaching a Basecamp","text":"<p>Congratulations! You now have a fully containerized application! In addition, this concludes the first \"leg\" of our journey into the labyrinth of ML Microservices. Up until this point we have focused on developing the software engineering foundation to build good microservices whether they use ML or not. For the next leg of our journey we will finally introduce ML and start to discuss some of the ways we can apply what we've covered thus far to building ML powered services! As always full code for this part can be found here!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/05/03/delve-12-lets-build-a-modern-ml-microservice-application---part-6-containerization.html#delve-data","title":"Delve Data","text":"<ul> <li>Many challenges exist when deploying Python applications to compute environments other than a local machine</li> <li>Docker provides a convenient way to solve many of these challenges</li> <li>uv provides a pre-built Docker base image that makes deploying Python applications using uv even easier</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html","title":"Delve 13: Let's Build a Modern ML Microservice Application - Part 7, Model Tracking and APIs with MLFlow","text":"<p>\"Machine learning models are only as good as their deployment strategy. An unused model is just a fancy equation.\" - Chat GPT</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#reset-and-rescope","title":"Reset and Rescope","text":"<p>Hello data delvers! In part six of this series we containerized our application, making it portable and easy to deploy. For this part we will take a step back. Introduce machine learning (finally!), and explore how we can begin to incorporate machine learning models into our microservice ecosystem!   </p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#machine-learning-finally","title":"Machine Learning (Finally!)","text":"<p>This series has up until now focused on core software engineering knowledge. This is intentional, if you've read some of my previous delves, you'll know that I view machine learning grounded in good software engineering practices as essential to extract business value at scale. It is for this reason I wanted to ensure we had laid a sturdy foundation in engineering before introducing the science, but without further ado, let's dive in!</p> <p>This series will not focus on machine learning fundamentals as there are plenty of good resources out there already that already cover that, however what I will touch on is how to develop machine learning models with an engineering mindset. To begin, go ahead and create an account on kaggle. Kaggle is ubiquitous in the data science world as a place to host competitive data science competitions (like the famous Zillow Prize), but it is also an invaluable resource to learn from. For this delve we will be working with the dataset provided by the House Prices - Advanced Regression Techniques learning competition so go ahead an sign up to it with your kaggle account and take a quick look at the data it provides as well as some of the submissions others have made to the competition to get a feel for what is working. </p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#prepare-your-project","title":"Prepare Your Project","text":"<p>Instead of building off our existing codebase we are going to start fresh with a brand new project (don't worry we'll be revisiting our previous codebase in a future delve!). Go ahead and initialize a starter project with <code>uv init</code> that will give you a basic file structure that looks something like this:</p> uv Starter Project Structure<pre><code>\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 hello.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>You can go ahead a remove the <code>hello.py</code> script, we won't be using it. Now, our future project will have multiple different parts. Such as for training the model, and using the model as part of microservices. As you can probably imagine, the set of Python library dependencies we have for training the model may be different than the Python library dependencies we have when we use the model. However, any libraries we have in common throughout the different parts of the project we probably want to stay the same version.  Previously this was a big headache, we could have different <code>requirements.txt</code> files for the different project parts to maintain different sets of dependencies, but making sure that if a dependency was re-used in multiple parts it was the same version? Good luck! Fortunately for us <code>uv</code> comes to the rescue here with its concept of workspaces. We can have a single uv workspace with one <code>uv.lock</code> file but multiple different sub-applications within it with their own dependency lists.</p> <p>Let's try it out! Go ahead and create a <code>housing-price-model</code> directory to hold our model application. Within it create a new <code>pyproject.toml</code> file with the following contents:</p> housing-price-model/pyproject.toml<pre><code>[project]\nname = \"housing-price-model\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a price regression model.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\n</code></pre> <p>The final trick to make this work is we have to modify the root <code>pyproject.toml</code> file to let uv know about this workspace member:</p> pyproject.toml<pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = []\n\n[tool.uv.workspace]\nmembers = [\"housing-price-model\"]\n</code></pre> <p>You can also add a project description too if you'd like. We should now have a project directory structure that looks like so:</p> Project Structure with uv Workspace<pre><code>\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 housing-price-model\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>Next up within the <code>housing-price-model</code> directory create a sub-directory called <code>data</code> and place the dataset files from kaggle within it.</p> <p>Tip</p> <p>It's a good idea to add your <code>data</code> directory to your <code>.gitignore</code> so the data files don't get checked into github. Not only is it against the rules of the competition to do so, it's generally bad practice to version data files with git, there are better tools we will cover in the future for that. \ud83d\ude42</p> <p>Finally, create a another sub-directory called <code>notebooks</code> with the <code>housing-price-model</code> directory to hold our Jupyter notebooks for modeling. \"Jupyter notebooks!?!? I though we weren't supposed to use those?\" I might hear you say, and based on my previous delve you wouldn't be wrong in asking the question, however in this case I think Jupyter notebooks are useful for a few reasons:</p> <ul> <li>We are in the early phase of exploring the data and the problem and in this state the visual nature of the notebooks can be useful</li> <li>Notebooks are extremely useful when trying to teach or explain a concept, which is what I'm trying to do in this delve</li> <li>We'll be converting the model training code out of notebooks in a future delve \ud83d\ude09</li> </ul> <p>With that we should have our final project directory structure that looks like this:</p> Full Project Structure<pre><code>\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 hello.py\n\u251c\u2500\u2500 housing-price-model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_description.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_submission.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 pyproject.toml\n</code></pre>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#build-a-model","title":"Build a Model","text":"<p>To start off with we need to install our project dependencies. First off make sure you are in the <code>housing-price-model</code> directory in your shell, then you can use <code>uv add</code> to install all of these dependencies:</p> <ul> <li>pandas - Standard library for data manipulation</li> <li>scikit-learn - The most popular library for doing machine learning in Python</li> <li>plotly - My preferred library for generating data visualizations (there a much better options out there than matplotlib!)</li> </ul> <p>Also, depending on what environment you want to run your notebooks in there are some optional dependencies as well:</p> <ul> <li>jupyterlab - The standard web-based notebook interface available from jupyter</li> <li>anywidget - An extension for jupyter that enables interactive plots</li> </ul> <p>Note</p> <p>I will be using jupyter lab as my notebook execution environment however you can also run notebooks directly in VSCode.</p> <p>Within the <code>housing-price-model</code> directory go ahead an fire up JupyterLab by executing <code>jupyter lab</code> in your shell. Within the <code>notebooks</code> folder create a new notebook called <code>train_model.ipynb</code>.</p> <p>Tip</p> <p>If you've never used JupyterLab before you can check out their getting started docs here!</p> <p>To start off create a new cell to import our dependencies:</p> Cell 1<pre><code>import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport plotly.express as px\n</code></pre> <p>Note</p> <p>If you'd rather follow along with a completed notebook you can do so here!</p> <p>It's best practice to import all of your libraries at the top of your notebook just like a regular Python script rather than throughout the notebook itself. This way you can quickly tell at a glace the dependencies of the notebook.</p> <p>Next off we can load our dataset into memory using pandas:</p> Cell 2<pre><code># Load Dataset\n\n# Load training and test datasets\ntrain = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\n\n# Display shapes\nprint(f\"Train Shape: {train.shape}\")\nprint(f\"Test Shape: {test.shape}\")\n\n# Preview the data\ntrain.head()\n</code></pre> <p>We can take note out test data has one less column then our training data (this makes sense since it doesn't have an actual price for each house).</p> <p>If we want to visualize our target variable (Sale Price) we can do so using Plotly like so:</p> Cell 3<pre><code># Create a histogram chart using Plotly\nfig = px.histogram(\n    train, \n    x='SalePrice',\n    labels= {'SalePrice': 'Sale Price'},\n    title='Distribution of House Sale Price',\n    marginal='box'\n)\nfig.show()\n</code></pre> <p>This will give you a visualization that looks something like this:</p> <p></p> <p>Note</p> <p>While the image above is a static png file, the true power of Plotly is that it is interactive! Try out the code within the notebook and hover your mouse over the graph, it will give you more details!</p> <p>In a similar way, we can use plotly to visualize the percentage of null values in each column to identify any columns that are mostly null (it's usually good practice to drop columns that are mostly null as they won't provided much information to the model).</p> Cell 4<pre><code># Calculate percentage of null values for each column\nnull_percentages = train.isnull().mean()\nnull_percentages = null_percentages.reset_index()\nnull_percentages.columns = ['column', 'percent_null']\nnull_percentages = null_percentages[null_percentages.percent_null &gt; 0]\nnull_percentages.sort_values(by='percent_null', inplace=True, ascending=False)\n\n# Create a bar chart using Plotly\nfig = px.bar(\n    null_percentages,\n    x='column',\n    y='percent_null',\n    title='Percentage of Null Values by Column (Columns with 0% Null Omitted)',\n    labels={'percent_null': 'Percent Null (%)', 'column': 'Column'},\n    text='percent_null'\n)\n\nfig.show()\n</code></pre> <p>This will give you the following plot:</p> <p></p> <p>We can then drop columns with a high percentage of nulls using pandas:</p> Cell 5<pre><code># Drop columns with more than 30% missing values\ndrop_cols = null_percentages[null_percentages.percent_null &gt; 0.3].column\ntrain_df = train.drop(columns=drop_cols)\n</code></pre> <p>We do a final bit of cleaning with pandas and form our feature and target columns:</p> Cell 6<pre><code># Remove ID column as it won't be used for training\ntrain_df.drop(columns=['Id'], axis=1, inplace=True)\n\n# Define features and target\nX = train_df.drop(['SalePrice'], axis=1)\ny = train_df['SalePrice']\n</code></pre> <p>Next we do a pretty standard train test split using scikit-learn:</p> Cell 7<pre><code># Train-Test Split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>We round out our data exploration with identifying the type of data in each column (namely whether they are categorical or numeric):</p> Cell 8<pre><code>numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_columns = X_train.select_dtypes(include=['object']).columns\n\nprint(f'Numeric columns: {numeric_columns}')\nprint(f'Categorical columns: {categorical_columns}')\n</code></pre>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#feature-engineering","title":"Feature Engineering","text":"<p>The next part of our notebook will focus on feature engineering. It is also where I want to spend a decent amount of time. Feature engineering is in my experience often where the breakdown between Data Scientists and ML Engineers occurs. I think a large part of this is due to how feature engineering is typically presented in tutorials (and indeed many of the notebooks submitted to this competition). It is usually viewed as a separate stage of the pipeline distinct from the model itself. In such cases you will typically see pandas being used (the ubiquitous <code>pd.get_dummies()</code> for encoding categorical variables for example) to transform the dataset into the features the model utilizes. This creates two issues:</p> <ol> <li>Since feature engineering can be any arbitrary python code it can become very complex and very messy</li> <li>Any feature engineering that is performed will have to be replicated in the environment where the model is used to make predictions</li> </ol> <p>These issues combine to create situations where engineers must translate and replicate feature engineering logic from notebooks to their production serving environments. As I've discussed in previous delves, this process is prone to error and can create a lot of friction. However, there is a better way, and we already have all the tools to do it!</p> <p>Instead of treating the feature engineering logic as a separate component, we could bundle it with the model itself. We'd have to set some constraints on what feature transformations could be performed (they couldn't call out to external APIs for example), but then we wouldn't have to worry about replicating any feature engineering logic as it would be a part of the model!</p> <p>scikit-learn actually supports this concept through the use of Column Transformers &amp; Pipelines, though it is sadly not widely publicized. For example if we wanted to replace all values missing from numeric columns with their median, and all categorical values with their most frequent value we could do the following transformation in pandas:</p> Hypothetical Feature Engineering<pre><code># Fill numeric columns with median\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\nfor col in num_cols:\n    df[col] = df[col].fillna(df[col].median())\n\n# Fill categorical columns with most frequent value\ndf = df.fillna(df.mode().iloc[0])\n</code></pre> <p>However, now in order to use our model this logic will have to be run somewhere on the data before it is sent to the model (usually in a different layer of the application), hence an opportunity for breaking or mis-translation. This issue compounds if you are trying to test different versions of the model. What if one model version uses the median to impute missing numeric values, but another version of the model uses the mean. How do you ensure the right feature engineering logic is being applied to the right version of the model? The answer: You can't.</p> <p>Instead of doing it this way though we could take advantage of the column transformers to define the logic in a bit more constrained way:</p> Cell 9<pre><code>impute_transformer = ColumnTransformer(\n    [\n        (\n            'median_imputer', \n            SimpleImputer(missing_values=np.nan, strategy='median'),\n            numeric_columns\n        ),\n        (\n            'mode_imputer',\n            SimpleImputer(missing_values=np.nan, strategy='most_frequent'),\n            categorical_columns\n        )\n    ],\n    remainder='drop',\n    verbose_feature_names_out=False\n)\n</code></pre> <p>The <code>ColumnTransformer</code> takes in a list of tuples where the first value is a name for the transformation, the second value is a function conforming to the scikit-learn transformer signature, and the third value is a list of columns to apply the transformation on. Hopefully you can already see how this is much more structured and uniform compared to the previous code. Now you might say \"That's great if I want to do a transformation built into scikit-learn, but what if it doesn't have the functionality I need?\". Fortunately, scikit-learn supports creating your own transformer functions pretty easily with the FunctionTransformer or your own transformer class by inheriting from TransformerMixin so you aren't really giving up any functionality, just obeying a common signature.</p> <p>We can apply one-hot encoding to our categorical features in a similar way:</p> Cell 10<pre><code>encode_transformer = ColumnTransformer(\n    [\n        (\n            'categorical_one_hot', \n            OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n            categorical_columns\n        )\n    ],\n    remainder='passthrough',\n    verbose_feature_names_out=False\n)\n</code></pre> <p>Note</p> <p>I go into a bit more depth on what these transformations are actually doing in the companion notebook so be sure to check it out!</p> <p>Create our model:</p> Cell 11<pre><code>model = RandomForestRegressor(n_estimators=100, random_state=42)\n</code></pre> <p>And attach the transformations to the model by creating a <code>Pipeline</code>:</p> Cell 12<pre><code>pipeline = Pipeline(\n    [\n        ('impute', impute_transformer),\n        ('encode', encode_transformer),\n        ('model', model)\n    ]\n).set_output(transform='pandas')\npipeline\n</code></pre> <p>This will also display a visualization of the pipeline in the notebook output!</p> <p>Now we can train our model with the feature engineering attached:</p> Cell 13<pre><code>pipeline.fit(X_train, y_train)\n</code></pre> <p>And make predictions on the test set with:</p> Cell 14<pre><code>pipeline.predict(test)\n</code></pre> <p>Notice how we didn't have to transform our data at all before we fed it into the model for predictions? That's the power of attaching our feature engineering directly to the model! Now some poor engineer doesn't have to replicate that logic anywhere and if we wanted to test different versions of the model with different feature engineering logic? No sweat, each version has the correct logic attached to it already. This simple approach of attaching feature engineering logic to models can dramatically reduce friction when deploying them and speed up time to production.</p> <p>Tip</p> <p>As with any software pattern, bundling feature engineering logic with the model has potential to be misused. (I've seen whole A/B tests orchestrated within bundled feature engineering logic for example). When deciding what logic to bundle with the model a good rule of thumb is if it specific to the training set the model was built on (a median value changes based on the samples in the training set for example) consider bundling it with the model. If the transformation is not related to the training set (choosing which version of the model to use for which customers for example) don't bundle it. This rule can be broken however if computing the feature on-the-fly as data is sent to the model is computationally expensive. As with all things, a balance must be found between convenience and computational practicality.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#get-in-the-flow-with-mlflow","title":"Get in the Flow with MLFlow","text":"<p>As with all science, the key to success is experimentation. We rarely get it right the first try, we have to try different algorithms and parameters to see what works and what doesn't for the problem we are trying to solve, there is no free lunch. Therefore, keeping track of our experiments is of paramount importance and that's where another tool, MLFlow, is particularly helpful. Backed by the same company that created Databricks, MLFlow is an all in one solution for experiment tracking and reporting, model versioning, and more. We can install it by running <code>uv add mlflow</code> in the root of our project.</p> <p>MLFlow follows a centralized server paradigm but for example purposes we can run the server locally. Create a new directory in the project root called <code>mlflow-server</code> and inside an executable bash script with the following contents:</p> mlflow-server/start-mlflow.sh<pre><code>#! /usr/bin/env bash\n\n# This script starts the MLflow server on localhost:9999\n# Ensure the script is run from the directory where it is located\n# Usage: ./start-mlflow.sh\n\n# Check if MLflow is installed\nif ! command -v mlflow &amp;&gt; /dev/null\nthen\n    echo \"MLflow could not be found. Please install it first.\"\n    exit 1\nfi\n\ncd $(dirname \"$0\")\nmlflow server --host 127.0.0.1 --port 9999\n</code></pre> <p>Go ahead execute the script to start MLFlow and check out the server UI, there won't be a whole lot here yet! </p> <p>Tip</p> <p>MLFlow will create two directories <code>mlartifacts</code> and <code>mlruns</code> when it executes. You can choose to add these to the .gitignore so they don't get checked in (and you probably should in production settings) but it means your trained model artifacts will not be checked into git.</p> <p>Next we are going to be modifying our notebook to utilize MLFlow. Since MLFlow is a dependency of modeling, we can add it to the <code>housing-price-model</code> application by running <code>uv add mlflow</code> in the <code>housing-price-model</code> directory as well (importantly the same version of MLFlow will be used).</p> <p>First we have to import MLFlow:</p> Cell 1 (Modified)<pre><code>import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport plotly.express as px\n\nimport mlflow\nfrom mlflow.models import infer_signature\n</code></pre> <p>We also have to tell MLFlow where the server is running:</p> Cell 13 (Modified)<pre><code># Create a new MLflow Experiment\nmlflow.set_tracking_uri(uri='http://127.0.0.1:9999')\nmlflow.set_experiment('Housing Price Prediction')\n</code></pre> <p>Next we modify our training code to run under an MLFlow experiment and log the trained model along with some metrics using MLFlow:</p> Cell 14 (Modified)<pre><code># Create a new MLflow Experiment\n# Start an MLflow run\nwith mlflow.start_run():\n    pipeline.fit(X_train, y_train)\n\n    y_pred = pipeline.predict(X_val)\n\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    mae = mean_absolute_error(y_val, y_pred)\n    fig = px.scatter(\n        x=y_val, \n        y=y_pred, \n        labels={\n            'x': 'Actual House Sale Price',\n            'y': 'Predicted House Sale Price'\n        },\n        title='Actual vs Predicted House Sale Price'\n    )\n\n    # Log metrics and figures\n    mlflow.log_metric('RMSE', rmse)\n    mlflow.log_metric('MAE', mae)\n    mlflow.log_figure(fig, 'actual_vs_predicted.html')\n\n    # Infer the model signature\n    signature = infer_signature(train[X_train.columns], y_pred)\n\n    # Log the model\n    model_info = mlflow.sklearn.log_model(\n        sk_model=pipeline,\n        artifact_path='model',\n        signature=signature,\n        input_example=X_train\n    )\n\nprint(\"Random Forest RMSE:\", rmse)\nprint(\"Random Forest MAE:\", mae)\n</code></pre> <p>A few thing to note here:</p> <ul> <li>The model training code is run under a <code>with mlflow.start_run():</code> clause, this tells MLFlow this code will be a new <code>run</code> under the experiment.</li> <li>Model metrics and figures can be logged along with the model using the <code>log_metric()</code> and <code>log_figures()</code> functions respectively</li> <li><code>infer_signature</code> can be used to auto-infer a model data contract from the training data</li> <li>The model artifact itself is logged with <code>log_model()</code></li> </ul> <p>Tip</p> <p>If you read the MLFlow documentation you may come across the autolog feature. I've always found this feature to be buggy and unrealiable so I prefer to log experiment artifacts explicitly.</p> <p>Go ahead and run the above code an check out the MLFlow UI. You should see a Housing Price Prediction experiment available with the run under it (usually with a cute autogenerated name). If you click around you should be able to view the logged metrics and figures as well as the model artifact itself. Subsequent runs will also be available here (provided the are run under the same experiment name), allowing you to easily compare different versions of the model and importantly have a record (ideally backed up on an external server) of all variations trained!</p> <p>Tip</p> <p>I'm just scratching the surface of MLFlow functionality, I encourage you to check out the MLFlow Docs to see all of the functionality available!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#create-an-api","title":"Create an API","text":"<p>With our model tracked in MLFlow believe it or not we have all we need to turn it into an API. One of the benefits of MLFlow is it comes with a serving layer that can host any MLFlow model as an API within a Docker container!</p> <p>To take advantage of this we can create a new sub-directory under <code>housing-price-model</code> called <code>deployment</code> with the following <code>model-build.sh</code> script:</p> housing-price-model/deployment/model-build.sh<pre><code>#! /usr/bin/env bash\n\n# This script builds a Docker image for an MLflow model using the specified model URI.\n# It uses the MLflow CLI to create the Docker image and enables MLServer for serving the model.\n# Usage: ./model-docker-build.sh &lt;model_uri&gt; &lt;image_tag&gt;\n\n# Ensure that MLflow is installed and available in your environment.\n# Make sure to replace the model URI with your actual model URI if needed.\n# Example model URI: runs:/&lt;run_id&gt;/model\n\n# Check if MLflow is installed\nif ! command -v mlflow &amp;&gt; /dev/null\nthen\n    echo \"MLflow could not be found. Please install it first.\"\n    exit 1\nfi\n\n# Generate the Docker File for the Model\ncd $(dirname \"$0\")/.. || exit 1\nmlflow models generate-dockerfile --model-uri $1 --output-directory build --enable-mlserver\n\n# Build the Docker image for the MLflow model\ndocker build -t $2 build\n</code></pre> <p>We can then execute it by running <code>./model-build.sh runs:/{RUN_ID}/model house-price-model</code> where <code>{RUN_ID}</code> is the Run ID of the model training run, which you can find in the MLFlow UI.</p> <p>Tip</p> <p>You may need to export the location of the MLFlow tracking server to an environment variable with the following command <code>export MLFLOW_TRACKING_URI=http://localhost:9999</code> in order to for it work (or add it to you .bashrc file or equivalent)</p> <p>This will: </p> <ol> <li>Create a new directory called <code>build</code></li> <li>Create a <code>Dockerfile</code> within it that will define the image to host the model and its dependencies</li> <li>Download the model artifact from the MLFlow server into a sub-directory calld <code>model_dir</code></li> <li>Build the Docker image and tag it as <code>house-price-model</code></li> </ol> <p>Note</p> <p>There is a mlflow models build-docker command that will do this all in one command, but I prefer to use <code>mlflow models generate-dockerfile</code> and <code>docker build</code> separatly so I can view the intermediate build outputs in case something goes wrong.</p> <p>It's hard to overstate the magic that just occured. I have seen engineers spend weeks trying to get a model artifact, with all its dependencies, with all the right versions bundled together into a container and we just did it with two commands!</p> <p>We can spin up our model API pretty easily too! In the root of the project create a <code>compose.yaml</code> file with the following contents:</p> compose.yaml<pre><code>services:\n  model:\n    image: house-price-model:latest\n    ports:\n      - \"8080:8080\"\n      - \"8081:8081\"\n      - \"8082:8082\"\n</code></pre> <p>Then run <code>docker compose up</code>!</p> <p>In order to hit out API, MLFlow expects the data in something know as dataframe_split format. Fortunately, we can easily convert our test dataset into that format by running the following snippet of code in our model training notebook:</p> Model Input JSON Conversion Snippet<pre><code>import json\n\njson.dumps({\n    \"dataframe_split\": json.loads(test.head(5).to_json(orient=\"split\"))\n})\n</code></pre> <p>This will grab the first five rows of the dataset. Use your REST client of choice (I like Bruno) to put the JSON in the body of a POST request and send it to <code>http://localhost:8080/invocations</code> You should get something like the following output back:</p> Model Output Predictions<pre><code>{\n    \"predictions\": [\n        128528.33,\n        154789.5,\n        179432.35,\n        187597.95,\n        204727.06\n    ]\n}\n</code></pre> <p>Where each element in the array is the prediction for the corresponding row in the input dataframe. Congradulations! You now have a running model API!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#putting-it-all-together","title":"Putting it all Together","text":"<p>Training and deploying models is the heart of MLOps and is often frought with frustation and friction, but by being intentional about the tooling we use to train our models and how we bundle the feature engineering logic they depend on we can greatly reduce the amount of overhead it takes! In the next part of this series we'll take a look at how we can integrate this deployed model into the microservices concepts we have developed before, stay tuned! Code for this part can be found here!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/06/01/delve-13-lets-build-a-modern-ml-microservice-application---part-7-model-tracking-and-apis-with-mlflow.html#delve-data","title":"Delve Data","text":"<ul> <li>Many challenges exist when training and deploying ML models</li> <li>By leveraging scikit-learn column transformers and pipelines we can greatly reduce the amount of feature engineering translation logic that needs to be done</li> <li>MLFlow provides a convenient framework for both tracking model experimentation and deploying model artifacts as APIs</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html","title":"Delve 14: Reflections on a Job Quest","text":"<p>\"Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work.\" - Steve Jobs</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#a-new-chapter","title":"A New Chapter","text":"<p>Hello data delvers! I recently successfully wrapped up a journey to find a new job! Along the way, I had the opportunity to interview at several different companies, experience many different styles of interviews, and explore different types of roles. For this delve, I intend to distill some thoughts about this process and share some lessons learned in the hopes they may be useful to others either looking to break into this field or find their next opportunity within it. If that sounds of interest stick around!</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#beginning-the-quest","title":"Beginning the Quest","text":"<p>To kick off my job hunt I first had to decide the criteria I was looking for in my next role. This set of criteria will be unique to you but it's a good idea to have a list of must haves and nice to haves in the role you are searching for. Write this list down and revisit it throughout your search to see if it still holds true or if things have shifted as a result of your journey. For example, your list might look something like this:</p> <p>Must Have:</p> <ul> <li>Fully Remote</li> <li>Staff+ Title</li> <li>Engineering-Focused Position</li> </ul> <p>Nice to Have:</p> <ul> <li>\"Founding\" Role</li> <li>Large scope</li> <li>Flexible PTO</li> </ul> <p>With your set of criteria in hand, begin applying to jobs that meet them! I primarily relied on LinkedIn for my job search, though there are other job boards available, particularly if you are interested in startup roles. It's also not a bad idea to set up job alerts on LinkedIn so you can be notified of new roles as they are posted.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#crafting-your-profile","title":"Crafting your Profile","text":"<p>If after applying to roles you start getting contacted by potential employers, great! If not, then you may want to work on crafting your resume/profile. It's hard to say exactly why some resumes climb to the top while other do not but based on the feedback I received here are a few tips:</p> <p>Be Specific - If you optimized a process and made it faster don't just say that, say how much faster, bonus points if you can tie it to business value. \"I optimized a critical business process and reduced computation speeds by a factor of 10x, resulting in $10,000 a month in savings to the enterprise.\" sounds a lot more impressive than \"I made processes faster\".</p> <p>Use Keywords Strategically - For better or worse, resumes are often screened by an automated system before they are put in front of a recruiter and these systems often look for keywords. You don't want your resume to be a buzzword salad, but pick a handful of key technologies/frameworks you are familiar with and be sure list them on your resume.</p> <p>Differentiate Yourself - Think about what unique skills/capabilities you bring to the table and how you can highlight them in your resume. For myself, I view one of my differentiators as my ability to write and breakdown technical topics. In order to demonstrate this, I selected a handful of my blog posts and linked them directly on my resume with a brief description of each. I received feedback from multiple potential employers that this was something that made my resume stand out (some of them had also read my blog and we were able to talk about the posts in my interviews!).</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#what-is-a-machine-learning-engineer-anyway","title":"What is a Machine Learning Engineer Anyway?","text":"<p>For my job search, I was targeting a Machine Learning Engineer job title. However, Machine Learning Engineer can mean different things to different people and the role can have a different focus depending on the requirements of the job. Some roles were much closer to a pure Data Scientist position, while others were more a DevOps engineer. I would try to determine the focus of the role based on the job description but often times couldn't really tell until I was able to speak to someone at the company about the responsibilities of the role. If you find that the focus of the role is not what you are looking for, be upfront about it. You don't want to waste the time of the interviewer or yourself if there isn't alignment, they will appreciate your candor and it may open up doors with that employer in the future if they do eventually open a role that aligns more with your interests.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#interviewing","title":"Interviewing","text":"<p>If you made it to the stage of interviewing, congratulations! Your effort on building your profile has paid off! My main piece of advice at this stage is to be authentic and to be fun. Individuals at the end of the day want to hire people A: They think can do the job and B: They would enjoy working with. A lot of people when prepping for interviews really focus on the former but in doing so try to warp themselves and their personalities to fit an artificial persona they think matches the job description. I'm not saying you shouldn't try to emphasize your past experience that best aligns with the job (you should absolutely do that), just don't do it in a way that feels fake or contrived, experienced interviews can see right through that. Instead, try to let your own self shine through in ways that align with the requirements of the job, but still demonstrate what is unique and personable about you. The final high-level piece of advice I'd give is don't be afraid to admit you don't know something, many interviewers will appreciate you saying something like \"I'm not very familiar with this topic, but if I were to try to reason through it on the spot I would do X.\" rather than you just trying to \"fake\" your way to an answer. It goes back to the second point, who would you rather work with? Someone who can never admit they don't know something and will hide that from you or someone who isn't afraid to admit they don't have the answer to a problem and will ask for help?</p> <p>With the high level out of the way I wanted to discuss some of the different types of interviews I encountered and how I prepared for each of them. Broadly speaking, interviews could be in one of two categories: Technical and Non-Technical.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#technical-interviews","title":"Technical Interviews","text":"<p>The first style of interview you typically will encounter is some form of a technical interview. These interviews emphasize your technical \"know-how\" and \"hard\" skills. They usually involve solving some kind of technical problem during the interview. The main categories of interviews of this type I encountered included:</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#leetcode","title":"Leetcode","text":"<p>While there has recently been controversy related to AI-cheating on this style of interview, it is still prevalent and something you should expect to encounter. In order to prep for this style of interview you can go through problem sets like NeetCode 150 to get familiar with types of problems asked. I would particularly focus on the topics of \"Arrays &amp; Hashing\", \"Two Pointers\", and \"Sliding Window\" as these were the most common types of questions I was asked.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#sql-leetcode","title":"SQL Leetcode","text":"<p>A slight variation of the Leetcode style interview is a SQL coding interview, the premise is the same but instead of using a programming language like Python you instead use SQL. This style of interview wasn't as common but to prep for it you can use resources such as DataLemur.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#numpy-optimization","title":"NumPy Optimization","text":"<p>A more common variant of the Leetcode interview I encountered was a NumPy optimization task. This was usually presented as here's a task (usually involving computing some aggregate statistics on a dataset), write the code to compute the output first using plain old Python for loops (sometimes this code was already provided), then optimize the computation using NumPy. For this I referenced Chapter 4 of the excellent free book \"Python for Data Analysis\" by Wes McKinney (the whole book is a good read if you want to brush up on Pandas as well).</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#notebook-critique","title":"Notebook Critique","text":"<p>This was actually one of my favorite styles of technical interview. The premise was given as \"Here's a notebook one of our data scientists wrote, how would you improve and productionize the code within it?\". I really liked this type of interview because I think it well represented an actual task one would do on the job (Interviewers take note!). In order to prep for this style of interview I found Kaggle to be an excellent resource. Enter a competition, take a look at the other submissions, and try to improve their code. </p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#data-science-fundamentals","title":"Data Science Fundamentals","text":"<p>This type of technical was more prevalent for roles that emphasized the Data Science aspect of the role. They typically involved a more academic survey of Data Science fundamentals. In order to prep the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition by Aur\u00e9lien G\u00e9ron remains my go-to. Also, the Transformer Architecture is very much in vouge right now, be prepared to answer questions about what it is and how it works, this course from Hugging Face is a good reference.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#system-design","title":"System Design","text":"<p>Rounding out the technical interviews is system design. Typically you are presented with a vague series of requirements which you must work to refine and then design a system around. This guide is a good reference for these types of interviews (the repo it is in also has useful materials for other types of interviews as well!).</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#non-technical-interviews","title":"Non-Technical Interviews","text":"<p>If you make it through the initial round of technical interviews you will also typically be asked to do non-technical interviews as well. In contrast to technical interviews, these interviews emphasize how you collaborate with others and your \"soft\" skills. The subject matter can be diverse but usually focuses on either past experiences \"Tell me a time when...\" or how to collaborate with others solving a business problem. The main categories of interviews of this type I encountered included:</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#behavioralhiring-manager","title":"Behavioral/Hiring Manager","text":"<p>These are typically conducted by the Hiring Manger and usually focused on your past experiences. It's good to have a set of stories prepared for different types of situations such as learning something new, navigating a conflict with a co-worker, collaborating with business partners, etc. The stories don't always have to be completely positive either, having examples of \"Looking back I would have done this differently because of X\" can be useful to demonstrate your own self-introspection and growth. The STAR Method can be particularly useful here.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#productpast-project","title":"Product/Past Project","text":"<p>This type of interview can be presented in a few different ways, it sometimes takes the form of being asked to give an overview of a past project that you've worked on, other times you are being asked to develop a new product from scratch. What ties these together is the emphasis on measurable business value. For your past project be prepared to discuss what some of the Key Performance Indicators (KPIs) were, how you measured the impact of your project against these KPIs, and what the outcome was (hopefully positive!). For designing a new product the ask is the same but instead of having real results you are theorizing what hypothetical results could be. In this scenario be prepared to discuss how you would adjust if a certain KPI shifted, and what some of the tradeoffs in your approach could be.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#stakeholder","title":"Stakeholder","text":"<p>The final type of interview I encountered in this category was a Stakeholder interview. This was typically with a senior leader at the company and could be thought of as combination of the previous two types of interviews with typically more time left for you to ask questions at the end. Ask questions! This is your opportunity to get a sense of the strategic direction of the business and what the thought leadership at the top is like.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#ask-questions","title":"Ask Questions","text":"<p>Did I mention ask questions? An important aspect of interviewing is that you are interviewing your potential employer just as much as they are interviewing you. Most interviews leave time at the end for questions, use it! I usually like to ask similar questions to all the people I was interviewing with to get different perspectives on the same topic within the company such as \"What is the biggest challenge you see someone in this role tackling?\" or \"What are you most excited about?\". You'll (hopefully) be spending a decent amount of time with these people, you want to make sure they are people you want to spend time with! </p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#the-offer","title":"The Offer","text":"<p>If your interviewing is successful you will have one, or possibly multiple, offers to consider. When considering an offer there are many things to consider such as:</p> <ul> <li>Compensation</li> <li>Work/Life Balance</li> <li>Company Culture</li> <li>Nature of the Work</li> </ul> <p>How these stack rank is up to you, the one piece of advice I'd give at this stage is to be respectful and honest. Both you and your potential employer have probably put in a considerable amount of time to get to this stage, they are invested in you by this point, you don't want to give them a bad impression and ruin the opportunity. For example, do not try to counter on one of the above points if it will not make a difference in you accepting the offer or not. If you decide not to accept an offer, do not lead the potential employer on. If you do decline, be very gracious that you were given the opportunity, even though you are declining now, you may find that you may cross paths with them again in the future.</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#closing-thoughts","title":"Closing Thoughts","text":"<p>The journey to find a new job can be both exciting and stressful. My final words of advice are do not rush. Allocate 3-6 months for your search, you don't want to necessarily accept the first opportunity that comes your way if it doesn't check all of your boxes but also don't go searching for the \"perfect\" role either, there are always tradeoffs. Finally, don't get discouraged! If you are finding you are not getting responses to your applications take a step back, work on your profile and try again. If you find that you aren't passing a particular type of interview, identify your weaknesses and work on them. The right opportunity will eventually come. \ud83d\ude42</p>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#additional-reading","title":"Additional Reading","text":"<ul> <li>(Opinionated) Guide to ML Engineer Job Hunting - An article from Yuan Meng on his experience hunting for an MLE role, provides a good additional perspective on the types of interviews encountered</li> <li>Preparing for Your Full Loop Interview at Meta - A guide from the Meta MLE group on their interview process, good reference for what to expect at FAANG level roles</li> </ul>","tags":["Opinion","Resource"]},{"location":"2025/07/20/delve-14-reflections-on-a-job-quest.html#delve-data","title":"Delve Data","text":"<ul> <li>Searching for a Machine Learning Engineer Role can be both exciting and stressful</li> <li>Define what type of role you are looking for, write down a list of the attributes you'd like it to have</li> <li>Work on developing a profile that will get you past the resume screens</li> <li>Identify and close your gaps on both Technical and Non-Technical Interviews</li> <li>Be respectful when negotiating and declining offers</li> <li>Don't get discouraged!</li> </ul>","tags":["Opinion","Resource"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html","title":"Delve 15: Let's Build a Modern ML Microservice Application - Part 8, The Orchestrator Service","text":"<p>\"Only a small fraction of real-world ML systems is composed of the ML code... The required surrounding infrastructure is vast and complex.\" - Hidden Technical Debt in Machine Learning Systems, Sculley et al. </p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#machine-learning-services-as-a-system","title":"Machine Learning Services as a System","text":"<p>Greetings data delvers! In part seven of this series we finally deployed a model! For this part we'll examine how to utilize our model as part of a larger microservice ecosystem!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#a-larger-system","title":"A Larger System","text":"<p>As we began to explore in our last delve in this series, the model code alone is just a small part of overall codebase. In this delve we will expand our codebase beyond simply serving the model as an API, but instead look to leverage that API as part of a larger system, but first let's take a look at the system we produced in our last delve:</p> <p>Figure 1: A Basic ML Ecosystem</p> <p></p> <p>This system is pretty simple but has some drawbacks when we examine it from the client's perspective. Let's take a look at a a sample API request to see what I mean:</p> MLFlow Model API Request<pre><code>{\n    \"dataframe_split\": {\n        \"columns\": [\n            \"Id\",\n            \"MSSubClass\",\n            \"MSZoning\",\n            \"LotFrontage\",\n            \"LotArea\",\n            \"Street\",\n            \"Alley\",\n            \"LotShape\",\n            \"LandContour\",\n            \"Utilities\",\n            \"LotConfig\",\n            \"LandSlope\",\n            \"Neighborhood\",\n            \"Condition1\",\n            \"Condition2\",\n            \"BldgType\",\n            \"HouseStyle\",\n            \"OverallQual\",\n            \"OverallCond\",\n            \"YearBuilt\",\n            \"YearRemodAdd\",\n            \"RoofStyle\",\n            \"RoofMatl\",\n            \"Exterior1st\",\n            \"Exterior2nd\",\n            \"MasVnrType\",\n            \"MasVnrArea\",\n            \"ExterQual\",\n            \"ExterCond\",\n            \"Foundation\",\n            \"BsmtQual\",\n            \"BsmtCond\",\n            \"BsmtExposure\",\n            \"BsmtFinType1\",\n            \"BsmtFinSF1\",\n            \"BsmtFinType2\",\n            \"BsmtFinSF2\",\n            \"BsmtUnfSF\",\n            \"TotalBsmtSF\",\n            \"Heating\",\n            \"HeatingQC\",\n            \"CentralAir\",\n            \"Electrical\",\n            \"1stFlrSF\",\n            \"2ndFlrSF\",\n            \"LowQualFinSF\",\n            \"GrLivArea\",\n            \"BsmtFullBath\",\n            \"BsmtHalfBath\",\n            \"FullBath\",\n            \"HalfBath\",\n            \"BedroomAbvGr\",\n            \"KitchenAbvGr\",\n            \"KitchenQual\",\n            \"TotRmsAbvGrd\",\n            \"Functional\",\n            \"Fireplaces\",\n            \"FireplaceQu\",\n            \"GarageType\",\n            \"GarageYrBlt\",\n            \"GarageFinish\",\n            \"GarageCars\",\n            \"GarageArea\",\n            \"GarageQual\",\n            \"GarageCond\",\n            \"PavedDrive\",\n            \"WoodDeckSF\",\n            \"OpenPorchSF\",\n            \"EnclosedPorch\",\n            \"3SsnPorch\",\n            \"ScreenPorch\",\n            \"PoolArea\",\n            \"PoolQC\",\n            \"Fence\",\n            \"MiscFeature\",\n            \"MiscVal\",\n            \"MoSold\",\n            \"YrSold\",\n            \"SaleType\",\n            \"SaleCondition\"\n        ],\n        \"index\": [\n            0\n        ],\n        \"data\": [\n            [\n                1461,\n                20,\n                \"RH\",\n                80.0,\n                11622,\n                \"Pave\",\n                null,\n                \"Reg\",\n                \"Lvl\",\n                \"AllPub\",\n                \"Inside\",\n                \"Gtl\",\n                \"NAmes\",\n                \"Feedr\",\n                \"Norm\",\n                \"1Fam\",\n                \"1Story\",\n                5,\n                6,\n                1961,\n                1961,\n                \"Gable\",\n                \"CompShg\",\n                \"VinylSd\",\n                \"VinylSd\",\n                null,\n                0.0,\n                \"TA\",\n                \"TA\",\n                \"CBlock\",\n                \"TA\",\n                \"TA\",\n                \"No\",\n                \"Rec\",\n                468.0,\n                \"LwQ\",\n                144.0,\n                270.0,\n                882.0,\n                \"GasA\",\n                \"TA\",\n                \"Y\",\n                \"SBrkr\",\n                896,\n                0,\n                0,\n                896,\n                0.0,\n                0.0,\n                1,\n                0,\n                2,\n                1,\n                \"TA\",\n                5,\n                \"Typ\",\n                0,\n                null,\n                \"Attchd\",\n                1961.0,\n                \"Unf\",\n                1.0,\n                730.0,\n                \"TA\",\n                \"TA\",\n                \"Y\",\n                140,\n                0,\n                0,\n                0,\n                120,\n                0,\n                null,\n                \"MnPrv\",\n                null,\n                0,\n                6,\n                2010,\n                \"WD\",\n                \"Normal\"\n            ]\n        ]\n    }\n}\n</code></pre> <p>And a sample response:</p> MLFlow Model API Response<pre><code>{\n    \"predictions\": [\n        128528.33\n    ]\n}\n</code></pre> <p>While this gets the job done it isn't the most interpretable API contract. Imagine if you were a front end developer and you were asked to produce a request that looked like that to call that backend, you'd probably scream. In addition, while MLFlow is great for getting a model API up and running the validation it does on the data being passed in is minimal. Go ahead and modify the request to put in a value that doesn't exist in the training data for one of the fields like <code>\"Dummy\"</code> and send it to the API, you probably still get a price prediction back! What's happening here is the model is just treating that field as <code>null</code> for the purposes of making a prediction. This probably isn't the behavior that we want since it could lead to silent bugs in our code. Finally, the output of our API is not very informative either, <code>\"predictions\"</code> is pretty vague as to what the output is supposed to mean. So in summary the things we'd like to change about our API are:</p> <ul> <li>Have a more natural REST contract for our API</li> <li>Have stricter validation on the data being passed into our API</li> <li>Have a more clear response</li> </ul> <p>In order to achieve this we are going to lean on the microservice codebase we built before to orchestrate calls to our model API!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#the-orchestrator-service","title":"The Orchestrator Service","text":"<p>The system we have currently built is an example of loose coupling between our client and backend model service. In this setup, both are independent services that could be deployed on their own. This is generally what is desired in a cloud microservice architecture, however the challenge comes in when independent services need to communicate to preform a task (in this case predict the price of a house). We could embed our business logic into our client application itself (validating the inputs, wrangling the request and response JSON, etc.) and this would work for a simple setup. However, what if in the future we have multiple different use cases for our price prediction model each with their own \"clients\"? Now we would have to replicate our business logic in multiple places which is not ideal. Or, what if we had multiple different models we want to apply as part of our logic (one to predict the price of a house, another to predict its insurance cost, etc.), now to process one house our clients must make calls out to multiple models each with their own contracts, yuck! In either case the solution is to introduce a new dedicated service (the orchestrator service) to control the sequence of API calls needed to perform the task, as well as perform any necessary business logic surrounding the calls. In this way, the orchestrator now becomes responsible for ensuring the workflow to complete the task is executed correctly and consistently each time, rather than relying on each client to remain consistent with each other.</p> <p>Figure 2: The Orchestrator Pattern</p> <p></p> <p>This provides a number of benefits:</p> <ul> <li>Centralized control over workflows ensures consistency across all invocations of the workflow</li> <li>It becomes easier to implement more complex workflows which becomes particularly common when working with ML services</li> <li>Error handling and recovery becomes easier to manage (If we are using multiple expensive models we won't want to retry the whole flow if only one model fails)</li> <li>We now have one convenient place to perform all of our logging and tracing</li> </ul> <p>In the context of our simple ecosystem this pattern would look like this:</p> <p>Figure 3: A Basic ML Ecosystem with an Orchestrator Service</p> <p></p> <p>Note</p> <p>The Orchestrator Pattern is often contrasted with the Choreography Pattern in microservice architectural design. In the Choreography Pattern, each service communicates asynchronously by emitting events that other services consume to produce a workflow.</p> <p>Figure 4: The Choreography Pattern</p> <p></p> <p>In practice, both patterns are utilized and we may examine this pattern in future delves as well.</p> <p>With the theory out of the way, let's write some code!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-setup","title":"Building the Orchestrator: Setup","text":"<p>To start off we need to modify our project structure to include an additional project within our workspace to house our orchestrator service. We can rely on the uv workspace concept we introduced in the previous delve to do this pretty easily! Add a new folder in the root of our project called <code>housing-price-orchestrator</code> with its own <code>pyproject.toml</code>:</p> housing-price-orchestrator/pyproject.toml<pre><code>[project]\nname = \"housing-price-orchestrator\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to orchestrate a price regression model.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\n</code></pre> <p>Then we need to update the root <code>pyproject.toml</code> to include it:</p> pyproject.toml<pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = []\n\n[tool.uv.workspace]\nmembers = [\"housing-price-model\", \"housing-price-orchestrator\"]\n</code></pre> <p>Finally, our orchestrator is going to be in the style of the service we built in part six so we can go ahead and give it a similar directory structure with a <code>src</code> and <code>tests</code> directory, each with their respective sub-directories. Our final directory structure should look something like this:</p> Full Project Structure<pre><code>\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 compose.yaml\n\u251c\u2500\u2500 housing-price-model\n\u251c\u2500\u2500 housing-price-orchestrator\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u251c\u2500\u2500 provider\n\u2502   \u2502   \u251c\u2500\u2500 service\n\u2502   \u2502   \u2514\u2500\u2500 shared\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u251c\u2500\u2500 integration\n\u2502       \u2514\u2500\u2500 unit\n\u251c\u2500\u2500 mlflow-server\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 uv.lock\n</code></pre> <p>Tip</p> <p>For the rest of this delve we will be doing a speed-run of the first 6 parts to build an orchestrator service. It's a good idea to review the corresponding part if at any point the steps seem unclear.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-the-data-layer","title":"Building the Orchestrator: The Data Layer","text":"<p>We can now follow a similar setup to the first six parts of this series beginning with the data layer. To start we need to do a bit more configuration of VScode to let it know where our source code will be located since it's not in the root of our project:</p> .vscode/launch.json<pre><code>{\n    \"python.autoComplete.extraPaths\": [\n        \"${workspaceFolder}/housing-price-orchestrator/src\"\n    ],\n    \"python.analysis.extraPaths\": [\n        \"${workspaceFolder}/housing-price-orchestrator/src\"\n    ]\n}\n</code></pre> <p>Next we need to model our MLFlow API response (make sure you create the correct base classes as before):</p> housing-price-orchestrator/src/shared/view/mlflow_view.py<pre><code>from typing import Any\nfrom shared.data_model_base import ViewBase\n\n\nclass MLFlowPredictionsView(ViewBase):\n    \"\"\"View model for the prediction results of a machine learning model.\"\"\"\n\n    predictions: list[Any]\n    \"\"\"List of predictions made by the model.\"\"\"\n</code></pre> <p>And a corresponding provider:</p> housing-price-orchestrator/src/shared/provider/mlflow_view.py<pre><code>from typing import Optional\nimport httpx\nimport pandas as pd\nfrom shared.view.mlflow_view import MLFlowPredictionsView\nimport json\n\n\nclass MLFlowModelProvider:\n    \"\"\"Provider for interacting with MLFlow models.\n\n    Args:\n        model_uri: The URI of the MLFlow model.\n        client: An optional httpx client for making requests. If not provided, a new client will be created.\n    \"\"\"\n\n    def __init__(self, base_url: str, client: Optional[httpx.Client] = None):\n        self.base_url = base_url\n        self.client = client or httpx.Client()\n\n    def health(self) -&gt; bool:\n        \"\"\"Checks the health of the MLFlow model provider.\n\n        Returns:\n            A string indicating the health status of the model provider.\n        \"\"\"\n        try:\n            response = httpx.get(f'{self.base_url}/ping')\n            return response.status_code == 200\n        except httpx.RequestError:\n            return False\n\n    def predict(self, data: pd.DataFrame) -&gt; MLFlowPredictionsView:\n        \"\"\"Makes a prediction using the MLFlow model.\n\n        Args:\n            data: The input data for the prediction.\n\n        Returns:\n            A ModelPredictionsView containing the predictions.\n\n        Raises:\n            HTTPStatusError: If the prediction request fails.\n        \"\"\"\n\n        payload = {'dataframe_split': json.loads(data.to_json(orient='split'))}\n\n        response = self.client.post(f'{self.base_url}/invocations', json=payload)\n        response.raise_for_status()\n\n        predictions = response.json()\n        return MLFlowPredictionsView.model_validate(predictions)\n</code></pre> <p>A few notes here:</p> <ul> <li>We add an optional <code>httpx.Client</code> in the constructor, this allows us to reuse the same client connection multiple times rather than creating a new connection each time (as is the default httpx behavior). Functionally this changes nothing about the code but can provide a nice performance bump.</li> <li>This provider knows nothing about the specifics of our model but instead takes in a pandas dataframe and produces a generic response. This is intentional, as it allows us to reuse this provider for any MLFlow model. </li> </ul> <p>That wraps up our data layer, onto the next one!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-the-business-logic-layer","title":"Building the Orchestrator: The Business Logic Layer","text":"<p>For our BLL we create a <code>PricingService</code>. It is in this layer we take our generic <code>MLFlowModelProvider</code> and tailor it to our specific business context. To start we need to model our request and response from the service:</p> housing-price-orchestrator/src/shared/view/request_view.py<pre><code>from shared.data_model_base import ViewBase\nfrom typing import Literal, Optional\nfrom pydantic import Field\n\n\nclass PricePredictionRequest(ViewBase):\n    \"\"\"View model for the request to predict housing prices.\"\"\"\n\n    id: int = Field(ge=1, serialization_alias='Id')\n    \"\"\"Unique identifier for the housing unit.\"\"\"\n\n    ms_sub_class: int = Field(ge=20, le=190, serialization_alias='MSSubClass')\n    \"\"\"Identifies the type of dwelling involved in the sale\"\"\"\n\n    ms_zoning: Literal['A', 'C', 'FV', 'I', 'RH', 'RL', 'RP', 'RM'] = Field(serialization_alias='MSZoning')\n    \"\"\"Identifies the general zoning classification of the sale\"\"\"\n\n    # This is only a subset of fields, the full class is available here: https://github.com/DataDelver/modern-ml-microservices/blob/part-eight/housing-price-orchestrator/src/shared/view/request_view.py#L6 \n</code></pre> <p>Tip</p> <p>This data model is massive. Instead of modeling it all by hand this was actually a very good use case for Generative AI. I used the Copilot chat feature of VSCode to prompt Copilot to generate a data model given a sample input row as well as the <code>data_description.txt</code> file from Kaggle. It did a great job with minimal tweaking. Much better than writing all the code by hand!</p> <p>One subtle thing to note is the <code>serialization_alias</code> on this model. The input csv of our model had the columns in PascalCase as well as non standard formats such as starting with numbers. When we send this data to the model those field names must match. However, in our orchestrator REST API contact, we'd like to use standard JSON camelCase and avoid non-standard formats. This is what the <code>serialization_alias</code> allows us to do! We can use the standard <code>alias_generator</code> of our <code>ViewBase</code> class to validate the JSON we parse to our model, but use a separate <code>serialization_alias</code> when sending the data to our MLFlow API. </p> <p>This view only models a single request, we can also model a batch prediction view as well, which is just a list of individual requests:</p> housing-price-orchestrator/src/shared/view/request_view.py<pre><code>from shared.data_model_base import ViewBase\nfrom typing import Literal, Optional\nfrom pydantic import Field\n\n\nclass PricePredictionBatchRequest(ViewBase):\n    \"\"\"View model for a batch request to predict housing prices.\"\"\"\n\n    data: list[PricePredictionRequest]\n    \"\"\"List of housing data dictionaries to be used for predictions.\"\"\"\n</code></pre> <p>We also need to create a DTO to model the response from our service layer:</p> housing-price-orchestrator/src/shared/dto/price_prediction.py<pre><code>from pydantic import Field\nfrom shared.data_model_base import DTOBase\n\n\nclass PricePrediction(DTOBase):\n    \"\"\"Data Transfer Object for price prediction requests.\"\"\"\n\n    id: int = Field(ge=1)\n    \"\"\"Unique identifier for the housing unit.\"\"\"\n\n    predicted_price: float = Field(gt=0)\n    \"\"\"Predicted price of the housing unit.\"\"\"\n</code></pre> <p>In addition to the price we also include an <code>id</code> field. This is to make it easier to join the prediction back to the original input row that produced it.</p> <p>Finally, with our views and DTOs created, we can build our <code>PricingService</code>:</p> housing-price-orchestrator/src/service/pricing_service.py<pre><code>from provider.mlflow_model_provider import MLFlowModelProvider\nfrom shared.view.request_view import PricePredictionRequest, PricePredictionBatchRequest\nfrom shared.dto.price_prediction import PricePrediction\nimport pandas as pd\n\n\nclass PricingService:\n    def __init__(self, pricing_model_provider: MLFlowModelProvider):\n        self.pricing_model_provider = pricing_model_provider\n\n    def predict_price(self, price_prediction_request: PricePredictionRequest) -&gt; PricePrediction:\n        \"\"\"Predicts the price using the MLFlow model provider.\n\n        Args:\n            data: The input data for the prediction.\n\n        Returns:\n            A view containing the predictions.\n        \"\"\"\n\n        input_data = price_prediction_request.model_dump(by_alias=True)\n\n        input_df = pd.DataFrame([input_data])\n        predictions = self.pricing_model_provider.predict(input_df)\n        predicted_price = predictions.predictions[0] if predictions.predictions else None\n\n        if predicted_price is None:\n            raise ValueError('No predictions returned from the model.')\n\n        return PricePrediction(id=price_prediction_request.id, predicted_price=predicted_price)\n\n    def predict_price_batch(self, price_prediction_batch_request: PricePredictionBatchRequest) -&gt; list[PricePrediction]:\n        \"\"\"Predicts the prices for a batch of requests using the MLFlow model provider.\n\n        Args:\n            price_prediction_batch_request: The batch request containing multiple price prediction requests.\n\n        Returns:\n            A list of views containing the predictions.\n        \"\"\"\n\n        input_data = price_prediction_batch_request.model_dump(by_alias=True)\n        input_df = pd.DataFrame(input_data['data'])\n        predictions = self.pricing_model_provider.predict(input_df)\n        predicted_prices = predictions.predictions if predictions.predictions else []\n\n        if not predicted_prices:\n            raise ValueError('No predictions returned from the model.')\n\n        return [\n            PricePrediction(id=req.id, predicted_price=price)\n            for req, price in zip(price_prediction_batch_request.data, predicted_prices)\n        ]\n</code></pre> <p>Take note how we are using <code>price_prediction_request.model_dump(by_alias=True)</code> when serializing the input views, this ensures the model is receiving the column names it is expecting. Depending on your use case, you might have a batch or single record prediction flow. This service supports both and importantly can use the same provider call in each case!</p> <p>All that's left now is to expose an interface to our service!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-the-interface-layer","title":"Building the Orchestrator: The Interface Layer","text":"<p>To expose our service we create a simple interface layer using FastAPI:</p> housing-price-orchestrator/src/main.py<pre><code>import os\nfrom fastapi import FastAPI, HTTPException\nimport httpx\nfrom service.pricing_service import PricingService\nfrom provider.mlflow_model_provider import MLFlowModelProvider\nfrom shared.config.config_loader import load_config_settings\nfrom shared.view.request_view import PricePredictionBatchRequest, PricePredictionRequest\nfrom shared.view.response_view import PricePredictionBatchResponseView, PricePredictionResponseView\n\napp = FastAPI()\napp_settings = load_config_settings(os.getenv('ENV', 'dev'))\n\n# Initialize HTTP client\nclient = httpx.Client()\npricing_service = PricingService(MLFlowModelProvider(app_settings.pricing_model_url, client))\n\n\n@app.post('/api/v1/price/predict')\ndef predict(price_prediction_request: PricePredictionRequest) -&gt; PricePredictionResponseView:\n    \"\"\"Endpoint to predict the price of a housing unit.\n\n    Args:\n        price_prediction_request: The request containing the input data for the prediction.\n\n    Returns:\n        A PricePredictionResponseView containing the predicted price.\n    \"\"\"\n\n    try:\n        price_prediction = pricing_service.predict_price(price_prediction_request)\n        return PricePredictionResponseView(id=price_prediction.id, predicted_price=price_prediction.predicted_price)\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n\n\n@app.post('/api/v1/price/predict/batch')\ndef batch_predict(price_prediction_requests: PricePredictionBatchRequest) -&gt; PricePredictionBatchResponseView:\n    \"\"\"Endpoint to predict the price of multiple housing units.\n\n    Args:\n        price_prediction_requests: A list of requests containing the input data for the predictions.\n\n    Returns:\n        A list of PricePredictionResponseView containing the predicted prices.\n    \"\"\"\n\n    try:\n        price_predictions = pricing_service.predict_price_batch(price_prediction_requests)\n        return PricePredictionBatchResponseView(\n            predictions=[\n                PricePredictionResponseView(id=pred.id, predicted_price=pred.predicted_price)\n                for pred in price_predictions\n            ]\n        )\n    except ValueError:\n        raise HTTPException(status_code=404, detail='No results found.')\n</code></pre> <p>A few notes:</p> <ul> <li>We are using the same config loader pattern as in previous delves to hold the url of the model API</li> <li>We instantiate the <code>httpx.Client</code> globally so it can be re-used across multiple invocations</li> <li>We are including a version number: <code>v1</code> as part of our url, this insures if we need to make a breaking change to our API in the future we can clearly denote which contract version our route supports</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-testing","title":"Building the Orchestrator: Testing","text":"<p>Don't forget about the importance of testing our code! In order to test our code we need to add a few configuration settings to our main <code>pyproject.toml</code> file to tell pytest where our tests are:</p> pyproject.toml<pre><code>[project]\nname = \"modern-ml-microservices\"\nversion = \"0.1.0\"\ndescription = \"Example repository of how to build a modern microservice architecture to support machine learning applications.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = []\n\n[tool.uv.workspace]\nmembers = [\"housing-price-model\", \"housing-price-orchestrator\"]\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.format]\nquote-style = \"single\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\npythonpath = [\n    \"housing-price-orchestrator/src\"\n]\ntestpaths = [\n    \"housing-price-orchestrator/tests\",\n]\npython_files = [\n    \"test_*.py\",\n    \"inttest_*.py\",\n]\n\n[tool.coverage.run]\nomit = [\n    \"housing-price-orchestrator/tests\",\n]\nsource = [\n    \"housing-price-orchestrator/src\",\n]\n\n[tool.coverage.report]\nfail_under = 60 \nshow_missing = true\nskip_empty = true\n</code></pre> <p>If you haven't already, make sure your ruff configurations are included there as well. Next we create unit and integration tests for each of our classes, for example, here's how you can test the <code>MLFlowModelProvider</code>:</p> housing-price-orchestrator/tests/unit/provider/test_mlflow_model_provider.py<pre><code>import pandas as pd\nimport httpx\nimport pytest\nfrom unittest.mock import MagicMock\n\nfrom provider.mlflow_model_provider import MLFlowModelProvider\nfrom shared.view.mlflow_view import MLFlowPredictionsView\n\n\ndef test_health_success(mocker):\n    \"\"\"Test the health method returns True when /ping returns 200.\"\"\"\n    # GIVEN\n    mocker.patch('httpx.get', return_value=MagicMock(status_code=200))\n    provider = MLFlowModelProvider(base_url='http://fake-url')\n\n    # WHEN\n    result = provider.health()\n\n    # THEN\n    assert result is True\n\n\ndef test_health_failure(mocker):\n    \"\"\"Test the health method returns False when /ping raises an error.\"\"\"\n    # GIVEN\n    mocker.patch('httpx.get', side_effect=httpx.RequestError('fail'))\n    provider = MLFlowModelProvider(base_url='http://fake-url')\n\n    # WHEN\n    result = provider.health()\n\n    # THEN\n    assert result is False\n\n\ndef test_predict_success(mocker):\n    \"\"\"Test the predict method returns MLFlowPredictionsView on success.\"\"\"\n    # GIVEN\n    mock_client = MagicMock()\n    provider = MLFlowModelProvider(base_url='http://fake-url', client=mock_client)\n    df = pd.DataFrame([{'a': 1, 'b': 2}])\n    expected_payload = {'dataframe_split': df.to_dict(orient='split')}\n    mock_response = MagicMock()\n    mock_response.json.return_value = {'predictions': [123.45]}\n    mock_client.post.return_value = mock_response\n    mock_response.raise_for_status.return_value = None\n    mocker.patch.object(\n        MLFlowPredictionsView, 'model_validate', return_value=MLFlowPredictionsView(predictions=[123.45])\n    )\n\n    # WHEN\n    result = provider.predict(df)\n\n    # THEN\n    mock_client.post.assert_called_once()\n    assert isinstance(result, MLFlowPredictionsView)\n    assert result.predictions == [123.45]\n\n\ndef test_predict_http_error(mocker):\n    \"\"\"Test the predict method raises if HTTP error occurs.\"\"\"\n    # GIVEN\n    mock_client = MagicMock()\n    provider = MLFlowModelProvider(base_url='http://fake-url', client=mock_client)\n    df = pd.DataFrame([{'a': 1, 'b': 2}])\n    mock_response = MagicMock()\n    mock_response.raise_for_status.side_effect = httpx.HTTPStatusError(\n        'fail', request=MagicMock(), response=MagicMock()\n    )\n    mock_client.post.return_value = mock_response\n\n    # WHEN / THEN\n    with pytest.raises(httpx.HTTPStatusError):\n        provider.predict(df)\n</code></pre> <p>A full collection of tests can be found here!</p> <p>Tip</p> <p>Writing tests is another area where GenAI shines, I utilized Github Copilot to generate these tests with a bit of prompting.</p> <p>With our code now tested let's Dockerize it so it can be deployed!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#building-the-orchestrator-dockerizing","title":"Building the Orchestrator: Dockerizing","text":"<p>To start we need to define our Dockerfile for our orchestrator, no surprises here, it's almost same one we've used before!</p> housing-price-orchestrator/Dockerfile<pre><code>FROM ghcr.io/astral-sh/uv:python3.13-bookworm-slim\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>You may have noticed this <code>from=project_root</code> section of the mounts. What's going on here? This has to do with the Docker build context. Our <code>uv.lock</code> file is in the root of the project. However, our Dockerfile is nested underneath the <code>housing-price-orchestrator</code> directory. We'd like to use this directory as the Docker build context so that all of the <code>COPY</code> commands work correctly, but this excludes the <code>uv.lock</code> file. Fortunately Docker has the concept of additional contexts to support including additional directories as part of the build which is what we are leveraging here. We create the <code>project_root</code> context as part of our <code>compose.yaml</code>: </p> compose.yaml<pre><code>services:\n  housing-price-model:\n    build:\n      context: housing-price-model/build\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:8080\"\n      - \"8081:8081\"\n      - \"8082:8082\"\n  housing-price-orchestrator:\n    build: \n      context: housing-price-orchestrator\n      additional_contexts:\n        project_root: .\n    environment:\n      - ENV=dev\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - housing-price-model\n</code></pre> <p>Note</p> <p>The build for the price model container has been changed as well, instead of referencing an already built image, the image is built as part of the <code>docker compose</code> command, this is personal preference, either approach is acceptable.</p> <p>Compose also lets us specify the dependency between the orchestrator service and the model service, ensuring the price model is spun up first before the orchestrator.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#running-our-orchestrator","title":"Running our Orchestrator!","text":"<p>With our compose configured we can spin up our services by first executing <code>docker compose build</code> and then <code>docker compose up</code>, you should see something like the following:</p> <pre><code>$docker compose up\nAttaching to housing-price-model-1, housing-price-orchestrator-1\nhousing-price-orchestrator-1  | \nhousing-price-orchestrator-1  |    FastAPI   Starting production server \ud83d\ude80\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |              Searching for package file structure from directories with         \nhousing-price-orchestrator-1  |              __init__.py files                                                  \nhousing-price-orchestrator-1  |              Importing from /app/src\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |     module   \ud83d\udc0d main.py\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |       code   Importing the FastAPI app object from the module with the following\nhousing-price-orchestrator-1  |              code:                                                              \nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |              from main import app\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |        app   Using import string: main:app\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |     server   Server started at http://0.0.0.0:8000\nhousing-price-orchestrator-1  |     server   Documentation at http://0.0.0.0:8000/docs\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |              Logs:\nhousing-price-orchestrator-1  |  \nhousing-price-orchestrator-1  |       INFO   Started server process [1]\nhousing-price-orchestrator-1  |       INFO   Waiting for application startup.\nhousing-price-orchestrator-1  |       INFO   Application startup complete.\nhousing-price-orchestrator-1  |       INFO   Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nhousing-price-model-1         | /usr/local/lib/python3.13/site-packages/starlette_exporter/middleware.py:97: FutureWarning: group_paths and filter_unhandled_paths will change defaults from False to True in the next release. See https://github.com/stephenhillier/starlette_exporter/issues/79 for more info\nhousing-price-model-1         |   warnings.warn(\nhousing-price-model-1         | 2025-08-17 19:02:14,704 [mlserver.parallel] DEBUG - Starting response processing loop...\nhousing-price-model-1         | 2025-08-17 19:02:14,723 [mlserver.rest] INFO - HTTP server running on http://0.0.0.0:8080\nhousing-price-model-1         | INFO:     Started server process [70]\nhousing-price-model-1         | INFO:     Waiting for application startup.\nhousing-price-model-1         | 2025-08-17 19:02:14,833 [mlserver.metrics] INFO - Metrics server running on http://0.0.0.0:8082\nhousing-price-model-1         | 2025-08-17 19:02:14,833 [mlserver.metrics] INFO - Prometheus scraping endpoint can be accessed on http://0.0.0.0:8082/metrics\nhousing-price-model-1         | INFO:     Started server process [70]\nhousing-price-model-1         | INFO:     Waiting for application startup.\nhousing-price-model-1         | INFO:     Application startup complete.\nhousing-price-model-1         | 2025-08-17 19:02:16,363 [mlserver.grpc] INFO - gRPC server running on http://0.0.0.0:8081\nhousing-price-model-1         | INFO:     Application startup complete.\nhousing-price-model-1         | INFO:     Uvicorn running on http://0.0.0.0:8082 (Press CTRL+C to quit)\nhousing-price-model-1         | INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\nhousing-price-model-1         | 2025-08-17 19:02:19,153 [mlserver] INFO - Loaded model 'mlflow-model' succesfully.\n</code></pre> <p>Compose will nicely separate out the logs for each service for us. With our services up and running we can now hit our orchestrator! Go ahead and hit <code>http://localhost:8000/api/v1/price/predict</code> with the following request using your REST client of choice:</p> Orchestrator API Request<pre><code>{\n    \"id\": 1461,\n    \"msSubClass\": 20,\n    \"msZoning\": \"RH\",\n    \"lotFrontage\": 80,\n    \"lotArea\": 11622,\n    \"street\": \"Pave\",\n    \"alley\": null,\n    \"lotShape\": \"Reg\",\n    \"landContour\": \"Lvl\",\n    \"utilities\": \"AllPub\",\n    \"lotConfig\": \"Inside\",\n    \"landSlope\": \"Gtl\",\n    \"neighborhood\": \"NAmes\",\n    \"condition1\": \"Feedr\",\n    \"condition2\": \"Norm\",\n    \"bldgType\": \"1Fam\",\n    \"houseStyle\": \"1Story\",\n    \"overallQual\": 5,\n    \"overallCond\": 6,\n    \"yearBuilt\": 1961,\n    \"yearRemodAdd\": 1961,\n    \"roofStyle\": \"Gable\",\n    \"roofMatl\": \"CompShg\",\n    \"exterior1St\": \"VinylSd\",\n    \"exterior2Nd\": \"VinylSd\",\n    \"masVnrType\": null,\n    \"masVnrArea\": 0,\n    \"exterQual\": \"TA\",\n    \"exterCond\": \"TA\",\n    \"foundation\": \"CBlock\",\n    \"bsmtQual\": \"TA\",\n    \"bsmtCond\": \"TA\",\n    \"bsmtExposure\": \"No\",\n    \"bsmtFinType1\": \"Rec\",\n    \"bsmtFinSf1\": 468,\n    \"bsmtFinType2\": \"LwQ\",\n    \"bsmtFinSf2\": 144,\n    \"bsmtUnfSf\": 270,\n    \"totalBsmtSf\": 882,\n    \"heating\": \"GasA\",\n    \"heatingQc\": \"TA\",\n    \"centralAir\": \"Y\",\n    \"electrical\": \"SBrkr\",\n    \"firstFlrSf\": 896,\n    \"secondFlrSf\": 0,\n    \"lowQualFinSf\": 0,\n    \"grLivArea\": 896,\n    \"bsmtFullBath\": 0,\n    \"bsmtHalfBath\": 0,\n    \"fullBath\": 1,\n    \"halfBath\": 0,\n    \"bedroomAbvGr\": 2,\n    \"kitchenAbvGr\": 1,\n    \"kitchenQual\": \"TA\",\n    \"totRmsAbvGrd\": 5,\n    \"functional\": \"Typ\",\n    \"fireplaces\": 0,\n    \"fireplaceQu\": null,\n    \"garageType\": \"Attchd\",\n    \"garageYrBlt\": 1961,\n    \"garageFinish\": \"Unf\",\n    \"garageCars\": 1,\n    \"garageArea\": 730,\n    \"garageQual\": \"TA\",\n    \"garageCond\": \"TA\",\n    \"pavedDrive\": \"Y\",\n    \"woodDeckSf\": 140,\n    \"openPorchSf\": 0,\n    \"enclosedPorch\": 0,\n    \"threeSsnPorch\": 0,\n    \"screenPorch\": 120,\n    \"poolArea\": 0,\n    \"poolQc\": null,\n    \"fence\": \"MnPrv\",\n    \"miscFeature\": null,\n    \"miscVal\": 0,\n    \"moSold\": 6,\n    \"yrSold\": 2010,\n    \"saleType\": \"WD\",\n    \"saleCondition\": \"Normal\"\n}\n</code></pre> <p>You should get back a response that looks something like this:</p> Orchestrator API Response<pre><code>{\n    \"id\": 1461,\n    \"predictedPrice\": 128528.33\n}\n</code></pre> <p>Importantly try changing the request to include an invalid value such as <code>Dummy</code> for the <code>street</code> field, instead of getting back a 200 response you should now get a pydantic validation error:</p> Orchestrator API Validation Error Response<pre><code>{\n    \"detail\": [\n        {\n            \"type\": \"literal_error\",\n            \"loc\": [\n                \"body\",\n                \"street\"\n            ],\n            \"msg\": \"Input should be 'Grvl' or 'Pave'\",\n            \"input\": \"Dummy\",\n            \"ctx\": {\n                \"expected\": \"'Grvl' or 'Pave'\"\n            }\n        }\n    ]\n}\n</code></pre> <p>Much nicer! I'll leave trying out the batch endpoint to you \ud83d\ude42</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#orchestrated-chaos","title":"Orchestrated Chaos","text":"<p>With the creation of the orchestrator service we have now created what I consider an MVP production system. There is still more functionality to add (and more delves to complete!), but this system now has the core functionality of a user-friendly API for producing model predictions as well as a tested code base. Subsequent delves will expand on this core to cover additional use cases and add more quality of life features. Congratulations on making it this far! Code for this part can be found here!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/08/17/delve-15-lets-build-a-modern-ml-microservice-application---part-8-the-orchestrator-service.html#delve-data","title":"Delve Data","text":"<ul> <li>Exposing MLFlow model APIs directly comes with several drawbacks</li> <li>Utilizing the Microservice Orchestration Pattern can get us around these drawbacks</li> <li>Using a combination of uv workspaces and Docker compose we can easily create this multi-service setup in one codebase</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/09/10/delve-16-the-quest-for-a-full-screen-raspberry-pi-application---part-2.html","title":"Delve 16: The Quest for a Full Screen Raspberry Pi Application - Part 2","text":"<p>\"The only constant in life is change.\" - Heraclitus</p>","tags":["Tools","Fun"]},{"location":"2025/09/10/delve-16-the-quest-for-a-full-screen-raspberry-pi-application---part-2.html#full-screen-applications-on-the-raspberry-pi-why-so-hard-again","title":"Full Screen Applications on the Raspberry Pi, Why so Hard AGAIN?","text":"<p>Hello data delvers! I recently revisited my Raspberry Pi after a long hiatus. As part of this I made sure to update all the packages and OS to the latest version. If you've read my previous Raspberry Pi delve, you'll know that being able to make applications full screen isn't as straightforward as it should be. Much to my surprise, after updating everything my fullscreen keyboard shortcut broke!  After spending some time internet sleuthing I'd like to share what the fix is with you all!</p>","tags":["Tools","Fun"]},{"location":"2025/09/10/delve-16-the-quest-for-a-full-screen-raspberry-pi-application---part-2.html#whats-going-on","title":"What's Going On?","text":"<p>The culprit is that in late 2024 (I know I haven't touched my Pi in that long!), Raspberry Pi switched it's default compositor (the software component for rendering windows) from Wayfire to labwc as it was better optimized for the Raspberry Pi's limited hardware resources (when updating I was presented with a screen asking if I wanted switch to labwc). In doing so though the keyboard configuration for Wayfire located at <code>~/.config/wayfire.ini</code> would no longer work!</p>","tags":["Tools","Fun"]},{"location":"2025/09/10/delve-16-the-quest-for-a-full-screen-raspberry-pi-application---part-2.html#a-new-config","title":"A New Config","text":"<p>labwc uses its own config file located at <code>~/.config/lawbwc/rc.xml</code>, by default it will look something like this:</p> ~/.config/lawbwc/rc.xml<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;openbox_config\n    xmlns=\"http://openbox.org/3.4/rc\"&gt;\n    &lt;theme&gt;\n        &lt;font place=\"ActiveWindow\"&gt;\n            &lt;name&gt;PibotoLt&lt;/name&gt;\n            &lt;size&gt;16&lt;/size&gt;\n            &lt;weight&gt;Normal&lt;/weight&gt;\n            &lt;slant&gt;Normal&lt;/slant&gt;\n        &lt;/font&gt;\n        &lt;font place=\"InactiveWindow\"&gt;\n            &lt;name&gt;PibotoLt&lt;/name&gt;\n            &lt;size&gt;16&lt;/size&gt;\n            &lt;weight&gt;Normal&lt;/weight&gt;\n            &lt;slant&gt;Normal&lt;/slant&gt;\n        &lt;/font&gt;\n        &lt;name&gt;PiXnoir_l&lt;/name&gt;\n    &lt;/theme&gt;\n&lt;/openbox_config&gt;\n</code></pre> <p>With some prompting of CoPilot, I discovered the trick is to add a new <code>&lt;keyboard&gt;</code> section to this file with your desired shortcuts. For example, to have my previous shortcut of Super+Shift+F to toggle fullscreen you could add the following section:</p> ~/.config/lawbwc/rc.xml<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;openbox_config\n    xmlns=\"http://openbox.org/3.4/rc\"&gt;\n    &lt;theme&gt;\n        &lt;font place=\"ActiveWindow\"&gt;\n            &lt;name&gt;PibotoLt&lt;/name&gt;\n            &lt;size&gt;16&lt;/size&gt;\n            &lt;weight&gt;Normal&lt;/weight&gt;\n            &lt;slant&gt;Normal&lt;/slant&gt;\n        &lt;/font&gt;\n        &lt;font place=\"InactiveWindow\"&gt;\n            &lt;name&gt;PibotoLt&lt;/name&gt;\n            &lt;size&gt;16&lt;/size&gt;\n            &lt;weight&gt;Normal&lt;/weight&gt;\n            &lt;slant&gt;Normal&lt;/slant&gt;\n        &lt;/font&gt;\n        &lt;name&gt;PiXnoir_l&lt;/name&gt;\n    &lt;/theme&gt;\n    &lt;keyboard&gt;\n        &lt;keybind key=\"W-S-f\"&gt;\n            &lt;action name=\"ToggleFullscreen\"/&gt;\n        &lt;/keybind&gt;\n    &lt;/keyboard&gt;\n&lt;/openbox_config&gt;\n</code></pre> <p>Tip</p> <p>A more complete list of configuration options can be found here.</p> <p>After making the above modifications and rebooting my Pi I can make applications fullscreen again!</p>","tags":["Tools","Fun"]},{"location":"2025/09/10/delve-16-the-quest-for-a-full-screen-raspberry-pi-application---part-2.html#delve-data","title":"Delve Data","text":"<ul> <li>In late 2024 the Raspberry Pi switched its default compositor from Wayfire to labwc</li> <li>This requires a new config file located at <code>~/.config/lawbwc/rc.xml</code> to define keyboard shortcuts</li> </ul>","tags":["Tools","Fun"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html","title":"Delve 17: Data Science, An Industry Perspective","text":"<p>\"Academia is about discovering truth. Industry is about delivering value.\" - Cassie Kozyrkov</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#oops","title":"Oops","text":"<p>Hello data delvers! I recently had the opportunity to give a guest lecture at my Alma Mater to students about the differences between practicing data science in academia vs industry. It was a lively discussion and I wanted to capture some of my thoughts as I think they could be particularly useful to individuals transitioning from an academic setting to an industrial one. The objectives, methods of working, and ultimately what determines your success are vastly different in academia compared to industry. To illustrate this here is an example of a conversation I had early in my career:</p> <p>\ud83e\uddd1\u200d\ud83c\udf93New Grad Chase: \"Look at this cool model I built! I was able to improve my F1 score by 0.1!\"</p> <p>\ud83d\ude4d\u200d\u2642\ufe0fBusiness Stakeholder: \"Sounds interesting! What's the impact to our OKRs by making this improvement??\"</p> <p>\ud83e\uddd1\u200d\ud83c\udf93New Grad Chase: \"Um, I don't know if there is one...\"</p> <p>\ud83e\udd26\u200d\u2642\ufe0fBusiness Stakeholder: \"...\"</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#a-shift","title":"A Shift","text":"<p>The above conversation encapsulates the biggest shift I found when transitioning from academia to industry: Business Value. When I was a grad student doing research, my focus was on advancing science, discovering new knowledge, and working on novel problems. However, when working in industry, the ultimate goal is unlocking business value, whether that requires machine learning or not. There's an adage that often comes up when talking to other ML engineers that goes something like \"The best model in academia is the most accurate. The best model in industry is the one that ships.\" which I have found to be very true. I have seen new hire data scientists trying to maximize the predictive performance of their models, adding more and more features, without considering that for each new data source they leverage, they are adding months to the delivery time of their models. I have also seen scientists fall into the trap of maximizing a theoretical performance metric without correlating it to a real-world outcome.</p> <p>These types of mistakes often go unnoticed when times are good, however, if your enterprise is honed-in on discovering value, it will quickly become apparent that you are delivering none: don't be in that position.</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#or-not","title":"Or Not","text":"<p>You may have noticed that I mentioned that being a data scientist in industry means delivering business value whether the solution requires machine learning or not. Non-technical stakeholders are often quick to jump to whatever the latest trends be, whether that be neural networks, automl, or generative AI. It is your responsibility as a practitioner to determine what is the simplest solution that achieves the desired business outcome. Another example:</p> <p>Early in my career I was asked to develop a method for detecting anomolous data points. \"I know how to do that!\" I thought to myself as I broke out my isolation forrest models and autoencoders. However, after spending some time tinkering with modeling I decided to plot the distribution of the data points I was working with (something I should have done to begin with), to my surprise the key feature in the dataset had a normal distribution, detecting anomalies was a simple as determining if the key feature was more than two standard deviations away from the mean. </p> <p>This also had the added benefit of being much easier to explain to my stakeholders that had never even heard of an isolation forrest before, let alone understood how it worked. Ironically, this same type of situation has popped up multiple times in my career. The takeaway here is simple, explainable approaches, that meet the business objective are always better than more complex ones, even if they perform worse.  If you need to slice a stick of butter, you could use a chainsaw, but why would you when a butter knife will work just as well?</p> <p>With all the latest hype around GenAI and LLMs I have observed a trend of business leaders mandating that AI should be used in \"everything\", even in situations where it doesn't make sense. GenAI is definitely a valuable tool, but it is not a panacea that invalidates all other approaches, don't jump to using it to solve problems when a simple logistic regression is good enough.</p> <p>This is a hard lesson to accept at times, there's an inherent draw to using the newest flashy technique, however your future self with thank you when something breaks and you need to understand why, or you need to get a key stake holder to trust your approach.</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#fast-over-flashy","title":"Fast over Flashy","text":"<p>Industry often follows tight delivery times, with quick iterations and a focus on delivering minimum viable products. This is in sharp contrast to the academic process, taking months or even years to validate approaches with rigor. Once more, in academia you are often working with the most cutting-edge algorithms and techniques, whereas in industry you often fall back to tried-and-true methods. That isn't to say you can't try something new and flashy but my recommendation is to always try a simple technique first before jumping to the latest and greatest approach, this has two main benefits:</p> <ol> <li>The simple approach is usually faster meaning you can get to an MVP sooner and start realizing value on a shorter timeline</li> <li>The simple approach now becomes your baseline to compare any further developments against</li> </ol> <p>It is that second benefit that I find can be particularly beneficial for your own career. If you can go to your stakeholders and say \"I solved X and increased our metrics by Y% over the baseline\", it is a much more compelling reference point when your annual review comes up than just \"I solved X\". Do your future self a favor and create a baseline!</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#the-trouble-with-data","title":"The Trouble with Data","text":"<p>The other big difference I've experienced between academia and industry is the dramatic difference in the type of data that is leveraged. In academia you often find yourself leveraging benchmark datasets that are pristine and clean so the focus can be on the modeling techniques themselves. However, the world of industry is messy, data is missing, misleading, or flat out wrong. Much more time and effort must be spent to validate that the data you have is even suitable to build a model with (hint: often it is not).</p> <p>In a similar vein, the data in industry is dynamic, customer preferences change over time, new products are released, the market ebbs and flows. Due to this fluid nature, the techniques you employ must be much more tolerant to change than if you were simply optimizing on a static reference dataset.</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#the-end-result","title":"The End Result","text":"<p>Finally, the work output is very different between these two worlds. As an academic I was expected to produce papers, present research, and develop prototypes. However, as a practitioner in industry, I produce products, APIs, and systems. These systems are not frozen artifacts either, they must change and evolve with new requirements, shifting customer trends, and the emergence of new technologies. I find this constant change exhilarating, but by its very nature, it forces a different prioritization of time. Ultimately in academia I was successful when I prioritized rigorous validation and research, but in industry I am successful when I can move fast, deliver positive impacts to the business, and do it in a way that is sustainable over time. </p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#closing-thoughts","title":"Closing Thoughts","text":"<p>Academia and industry are both areas where Data Science thrives. However, the focus, and end result, are vastly different. Hopefully if you are considering making the switch to industry, these thoughts are helpful! If you want to see more of how to build and launch data science products in an enterprise setting, stick around, we have a lot more delves to do. \ud83d\ude42</p>","tags":["Opinion"]},{"location":"2025/10/07/delve-17-data-science-an-industry-perspective.html#delve-data","title":"Delve Data","text":"<ul> <li>The focus of academia and industry in Data Science is very different</li> <li>Academia emphasizes exploration and rigor whereas industry prioritizes speed and impact</li> <li>Prefer simple, explainable approaches in industry</li> <li>Real-world data is often very messy</li> <li>The goal is to build sustainable products that deliver business value</li> </ul>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html","title":"Delve 18: The Challenges of AI in Industry","text":"<p>\"The first rule of any technology used in a business is that automation applied to an efficient operation will magnify the efficiency. The second is that automation applied to an inefficient operation will magnify the inefficiency.\" - Bill Gates</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#a-challenge","title":"A Challenge","text":"<p>\"What are the biggest challenges you've faced when integrating AI into your work?\"</p> <p>Greetings data delvers! I had the opportunity to serve on a multi-disciplinary panel recently where I was asked that very question. At the time, my answer focused on immediate challenges with data quality and access to tooling. While those are legitimate concerns, the question has stuck with me for the past few weeks as requiring a deeper answer. As I've been thinking more about it, I've developed a more nuanced view which I'd like to capture here. Hopefully, it may be of use if you are looking to adopt more AI into your workflows!</p> <p>When AI first emerged into the cultural zeitgeist with the release of ChatGPT a few years ago, I distinctly remember many people heralding it as the end of software engineering, that programmers would become \"prompt engineers\", and that we should stop learning to code because that would be obsolete, yet here we are a few years later, still writing code. Now, it would be naive to state that AI has not transformed software engineering, it absolutely has, but it has not replaced it. It is this relationship of augmentation rather than automation that I think perfectly encapsulates the limits of AI today. There is an often quoted statistic that 80% of AI projects fail, why then do we see continued investment in it? What challenges are enterprising facing that are causing them to fail? Do individuals have anything to learn from these challenges when implementing AI in their own work? What can I do to increase the odds of my project being more successful? These are the questions I'd like to wrangle with today.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#the-promise-of-ai","title":"The Promise of AI","text":"<p>With the inherent high failure rate of AI projects, why do enterprises continue to pursue them? Would those dollars be better spent somewhere else? In many cases: yes. I think this speaks to a disconnect between the marketing and hype of AI and the problems it is actually good at solving. Marketing would have you believe that AI is a silver bullet that can solve all conceivable problems when in reality there is only a narrow set of tasks it really excels at completing. Make no mistake, AI as it exists today is just a more sophisticated form of auto-complete. Given the preceding words (tokens) in a string, it predicts the next most likely token in the sequence. That's it\u2014it has no more understanding of the content it is producing than some of the first chatbots created in the 1960s. It simply is better at simulating realistic responses.</p> <p>This next most likely token flow also explains why AI systems hallucinate. When an AI hallucinates, it has not inherently produced an \"incorrect\" output; it produced the next most likely token in the sequence, which is what it is designed to do. The issue is that simply predicting next most likely token in the sequence does not convey any understanding about the world. This is actually the focus of a different line of AI model research championed by Yann LeCun, the former Chief of AI at Meta, known as World Models which may prove to be more useful than the Large Language Models we use today. However, all of the LLMs in use today suffer from this drawback.</p> <p>Given these drawbacks, enterprises often fail to realize value from AI when they task it with solving problems where hallucination poses too great a risk or when the problem could be solved with a much simpler approach. I have seen misinformed executives mandate that teams must use AI in \"everything\". However, this perspective is fundamentally flawed as AI cannot, in its current form, solve everything. It also doesn't invalidate decades of other approaches, it is simply another tool in the toolbox. For example, if you have a dataset where you'd like to recognize phone numbers within it you could prompt an LLM to solve this problem and it most likely would, though it would be:</p> <ul> <li>Expensive to operate</li> <li>Still prone to the occasional hallucination</li> </ul> <p>Or you could simply write a regular expression that captures all common phone number formats. The regular expression approach has the added benefit of eliminating the risk of hallucination!</p> <p>It can also go the other direction as well. I sometimes see individuals requesting \"AI Magic\" to essentially replace an entire high-skilled professional's job. However, there is no way to break such problems down into a series of steps that an AI could reasonably solve, as they require many years of experience to understand. For example, you probably don't want your physician to be replaced by an AI, given its propensity to hallucinate, though a human physician could see benefit consulting one, because they have the expertise to determine when an AI's outputs could be trusted.</p> <p>Thus, the first challenge of leveraging AI in your work is to determine if you should use AI to solve it, given the inherent limitations of AI today. If you have decided to use AI, the next step is to figure out if you can.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#organizational-challenge-1-data","title":"Organizational Challenge 1: Data","text":"<p>Often, the first barrier to an organization leveraging AI is the data quality of the organization itself. This is not a new challenge\u2014Garbage In, Garbage Out has been in the computer science vernacular since at least the 1950s. However, AI especially depends on high quality data to be successful, and it is not in itself a substitute for poor data quality within an organization. I have observed executives essentially trying to \"short-cut\" to AI without putting in any of the foundational effort needed to support it. What is needed to support AI is the same good data quality as was needed for ML &amp; statistics before it, ignoring the effort to get it right will result in projects that under deliver or ultimately fail.</p> <p>To overcome this challenge, organizations should establish Data Governance by defining clear roles and processes for data ownership (who can modify data and when), as well as implementing strict data validation rules as far upstream as possible to ensure data consistency across the organization rather than in isolated fragments. </p> <p>The second challenge related to organizational data is the architecture surrounding it. My former colleague Yafet Tamene, now a Principal MLE at Microsoft put it well when he stated:</p> <p>Traditional applications can be built in silos. Conventional software can function effectively within departmental boundaries, operating on isolated data sets and delivering value independently. AI applications cannot.</p> <p>As an effect of Conway's Law, organizations often build architectures that mirror the hierarchical and siloed structure of how their people are organized. This tends to work fairly well for static, independent applications. However, in order to truly gain value from AI, it must have context of the data across the entire enterprise, not just within a single operating group. If I want to understand how a product will perform, I may need context on how it is operating, current sales metrics, and customer feedback. If each of these sources of data is stored in separate isolated systems I can't hope to build a AI that can accurately forecast demand.</p> <p>To address this, organizations must invest in building a unified data ecosystem that makes data readily accessible and easily joinable across the enterprise. (More to come on this in future Delves!)</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#organizational-challenge-2-the-proof-on-concept-trap","title":"Organizational Challenge 2: The Proof on Concept Trap","text":"<p>Another common challenge organizations face is building endless proof-of-concept projects. AI often performs well at small scale but fails when attempted to scale to \"real-world\" use cases. This can happen for many reasons: there may not be the infrastructure or MLOps practices in place to deploy the model, important edge cases may have been overlooked during the PoC, or measuring the return on investment may prove difficult.</p> <p>To address this, organizations should start small with well-defined use cases where the data is clean and readily available, and the desired outcome is clear and objectively measurable.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#organizational-challenge-3-ethics","title":"Organizational Challenge 3: Ethics","text":"<p>Perhaps one of the most important challenges an organization may face is considering the ethical implications of using AI. Answering questions such as \"Was there any bias in the data that was used to build this model?\" or \"Do we have safeguards in place to prevent undesired outcomes in the case of hallucination?\" are critical to answer before deploying an AI solution. This is further compounded by the \"black box\" nature of AI. Unlike prior ML approaches that could provide some level of explainability through weights and decision boundaries, AI is inherently unexplainable. It should only be used in applications where there is no legal or moral requirement to explain why a decision was made.</p> <p>Organizations should take care when deploying AI solutions to ensure they consider the ethical and legal implications before impacting their customers.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#individual-challenge-1-verification-tax","title":"Individual Challenge 1: Verification Tax","text":"<p>In addition to the organizational challenges above, individuals can also face challenges when leveraging AI. Perhaps the largest and most time-consuming is verification. An AI might generate a full project in minutes, but verifying it works correctly could take you hours. In their article Being \"Confidently Wrong\" is holding AI back, PromptQL co-founder Tanmai Gopal coins this the \"Verification Tax\". This friction is a direct result of shifting from being a creator to an editor. Editing often requires more cognitive effort because it demands a higher level of vigilance in spotting subtle errors in AI outputs.</p> <p>To combat this, individuals should break larger tasks into smaller subtasks before handing them to an AI. This makes it easier to verify the AI's output is correct before moving to the next task.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#individual-challenge-2-prompt-fatigue","title":"Individual Challenge 2: Prompt Fatigue","text":"<p>Another challenge individuals face is endlessly adjusting prompts to get desired outputs. This is particularly challenging when prompts are vague, requiring multiple iterations to get the desired output making it feel faster to just do the work yourself. This also occurs when the AI lacks necessary context, such as knowledge of previous attempts at creating a solution.</p> <p>To address this challenge, you should develop a collection of templates for your most useful prompts. Start each problem with one of these templates rather than a blank prompt and iterate from there. In addition, provide context and structure to your prompts by giving the AI a persona, goal, and constraints. For example:</p> <p>Act as a data analyst. Analyze the following spreadsheet and produce three key takeaways for an executive audience, limiting your answer to 150 words.</p> <p>This provide the AI a framework to produce the type of output you are looking for rather than a free-form response.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#individual-challenge-3-skill-atrophy","title":"Individual Challenge 3: Skill Atrophy","text":"<p>Finally, a legitimate concern for many individuals leveraging AI is becoming over-reliant on it. The goal is to treat AI as a collaborative partner rather than a shortcut, ensuring you remain \"in-the-loop\" in the development cycle.</p> <p>Some strategies you could employ include:</p> <ul> <li>Always produce the initial draft of work yourself and then ask AI to optimize or refine it. This forces you to attempt a solution yourself before relying on AI.</li> <li>Reverse engineer the output, particularly when learning new skills. Instead of using AI output verbatim, ask the AI why it produced a specific output and cross-reference it with other non-AI sources to gain deeper understanding.</li> <li>Delegate tasks to AI, not skills. Use AI for solving repetitive, low-value tasks (the ones it is good at!) but do the complex, high-value tasks yourself that require the skills of your career.</li> </ul> <p>Ultimately, it is up to you to determine where your boundary with AI is, but remember you are responsible for how the output is utilized, so take steps to use it responsibly.</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#responsible-innovation","title":"Responsible Innovation","text":"<p>AI has truly accelerated our ability to explore, comprehend, and produce vast amounts of data. However, if one is not aware of the limitations and challenges that come with leveraging AI, it will continue to underdeliver on the value it promises. Organizations should focus on strong data governance above all else and shouldn't invest in the latest flashy AI tools until their data house is in order. Individuals should focus on \"human-in-the-loop\" workflows and treat AI as a collaborator, not a replacement.</p> <p>With these challenges addressed you are ready to innovate responsibly with AI!</p>","tags":["Opinion"]},{"location":"2025/11/30/delve-18-the-challenges-of-ai-in-industry.html#delve-data","title":"Delve Data","text":"<ul> <li>Leveraging AI effectively comes with several challenges both to organizations and individuals</li> <li>Organizations should focus on effective data governance before attempting to leverage AI</li> <li>Individuals should treat AI as a collaborative partner and keep themselves \"in-the-loop\" in the development cycle</li> </ul>","tags":["Opinion"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html","title":"Delve 19: Let's Build a Modern ML Microservice Application - Part 9, Docker Container Optimization","text":"<p>\"Containerization is the new virtualization.\" - James Turnbull</p> <p>Greetings data delvers! In part eight of this series we deployed our first multi-service system. In this part, we examine more deeply how we are deploying our services with Docker and look for opportunities to make our deployment more optimized and secure.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#where-we-left-off","title":"Where we Left Off","text":"<p>In our last part we briefly touched on dockerizing our application using the below Dockerfile:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM ghcr.io/astral-sh/uv:python3.13-bookworm-slim\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>We can quickly check the size of the image built from this file with the <code>docker image ls</code> command:</p> <pre><code>docker image ls modern-ml-microservices-housing-price-orchestrator\nREPOSITORY                                           TAG       IMAGE ID       CREATED        SIZE\nmodern-ml-microservices-housing-price-orchestrator   latest    c02bbc40ca76   2 months ago   375MB\n</code></pre> <p><code>375MB</code> \u2014 not bad! This file does what we need it to do but has a few shortcomings; let's break them down.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#standard-base-image","title":"Standard Base Image","text":"<p>We are currently using the <code>ghcr.io/astral-sh/uv:python3.13-bookworm-slim</code> base image provided by Astral. This works great; however, enterprises often have a set of standard base images approved by the organization. What if we want to use a standard base image but install <code>uv</code> into it? Fortunately, Astral provides guidance on how to do that. We can instead copy the <code>uv</code> binary from one of their official images into ours:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>Note</p> <p>The offical Astral docs show installing <code>uv</code> from the <code>latest</code> tag however I recommend pinning to a specific version instead. This ensures that no breaking changes in <code>uv</code> inadvertently break your build.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#build-dependencies","title":"Build Dependencies","text":"<p>Many Python popular packages with C extensions (such as Numpy) will often require compilers such as <code>gcc</code> or <code>g++</code> to be available on the machine in which they are installed. We can preemptively install these into our image to ensure that we won't run into any issues, we can also use this as an opportunity to make sure all system packages are up to date in the image as well:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>Note</p> <p>Notice how we are adding <code>rm -rf /var/lib/apt/lists/*</code> to the end of the install command, this saves us space in the final image. When <code>apt-get update</code> is executed inside the Docker container, the package manager downloads package lists and metadata into /var/lib/apt/lists/. These files are crucial for the installation process but are not needed for running the final application. Removing them frees up significant space. </p> <p>This will have the effect of increasing the size of the image but we'll see how to deal with that soon.</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#environment-variables","title":"Environment Variables","text":"<p>Version <code>0.8.7</code> of <code>uv</code> added the UV_NO_DEV environment variable. Since we don't want dev dependencies in this image we can set it globally to ensure that no dev dependencies are installed:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Don't install dev dependencies\nENV UV_NO_DEV=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen --no-dev\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>It's also worth explaining the other environment variables we are leveraging:</p> <ul> <li> <p>UV_COMPILE_BYTECODE - Setting this ensures that <code>uv</code> will compile the bytecode of all Python source files ahead of time, leading to longer container build times but shorter execution times, typically a desired tradeoff in deployed images.</p> </li> <li> <p>UV_LINK_MODE \u2014 We can pair setting this variable along with a caching strategy described in the <code>uv</code> documentation to speed up local builds by reusing the system <code>uv</code> cache instead of forcing <code>uv</code> to create its own inside the container.</p> </li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#installing-dependencies","title":"Installing Dependencies","text":"<p>We can clean up and optimize the dependency installation steps as well:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Don't install dev dependencies\nENV UV_NO_DEV=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>We are taking advantage of our caching strategy when installing our dependencies, we also no longer need the <code>--no-dev</code> flag since we've set the environment variable.</p> <p>We also want to install dependencies exactly as they exist in the <code>uv.lock</code> file without modification so we are using the <code>--frozen</code> flag.</p> <p>Notice we are installing project dependencies separately from the source code. This is due to the Docker build cache. Each command in the Dockerfile creates a new layer in the final image. These layers are cached by Docker. Whenever a layer changes, it will need to be rebuilt. When this happens, all layers that come after that layer will also have to be rebuilt. This means you should always put layers that are more likely to change after layers that are less likely to change. In our case, it's more likely we'll change our project's source code while developing it rather than its dependencies. Breaking the install step into two separate layers allows us to reuse the dependency installation layer when rebuilding our Docker image if only the source code has changed, leading to faster local build times.</p> <p>Warning</p> <p>The official <code>uv</code> docs recommend using the <code>--locked</code> flag instead of <code>--frozen</code> to prevent building with an outdated lockfile; however, this does not work when using <code>uv</code> workspaces as we are. This is because <code>uv</code> would need access to all <code>pyproject.toml</code> files to verify that the lockfile is up to date, not just the individual workspace lockfile. As such, ensure that your lockfile is up to date by running <code>uv sync</code> before building the image!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#everything-and-the-kitchen-sink","title":"Everything and the Kitchen Sink","text":"<p>The rest of the commands in the file are pretty self-explanatory and do not change. Let's go ahead and check the size of our built image now:</p> <pre><code>docker image ls modern-ml-microservices-housing-price-orchestrator\nREPOSITORY                                           TAG       IMAGE ID       CREATED         SIZE\nmodern-ml-microservices-housing-price-orchestrator   latest    2fea0c60cd2b   8 seconds ago   667MB\n</code></pre> <p>Yikes! <code>667MB</code>! Our image has more than doubled in size, likely due to the additional build dependencies. However, we shouldn't be compiling any code when our container is running. Moreover, if a bad actor gained access to our container, they could now compile malicious code, presenting a larger attack surface. Additionally, if any of these build dependencies had unknown vulnerabilities, we would be susceptible to them even though we don't need them at runtime! We can reduce the size of our container and make it more secure by leveraging multi-stage builds.</p> <p>Keeping with Docker's nautical theme, I liken building an image to launching a ship. To build a ship, you need substantial scaffolding around it so shipyard workers can do their jobs. However, when it comes time to launch, the scaffolding is removed before the ship leaves the dry dock. If I told you we were launching a ship with scaffolding still attached, you'd say I was crazy! Right now, we are launching our image with the scaffolding attached. In our case, the scaffolding is all the build-time dependencies.</p> <p>To fix this, let's first designate our current image as our <code>builder</code> shipyard:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm AS builder\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Don't install dev dependencies\nENV UV_NO_DEV=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>Next, once our code is built, we can copy only what we need to execute the final application from our <code>builder</code> stage into the final image, thus removing the scaffolding:</p> housing-price-orchestrator/Dockerfile<pre><code>FROM python:3.13-slim-bookworm AS builder\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:0.9.16 /uv /uvx /bin/\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Install build dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Don't install dev dependencies\nENV UV_NO_DEV=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Install the project's dependencies using the lockfile and settings\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project\n\n# Then, copy the rest of the project source code and install it\n# Installing separately from its dependencies allows optimal layer caching\nCOPY . /app\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock,from=project_root \\\n    uv sync --frozen\n\nFROM python:3.13-slim-bookworm\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Copy project from the builder stage\nCOPY --from=builder /app /app\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run the FastAPI application by default\nCMD [\"fastapi\", \"run\", \"src/main.py\", \"--port\", \"8000\"]\n</code></pre> <p>Since this is a new base image, we can take the opportunity to update its system packages. This is also where we could install any runtime system dependencies.</p> <p>With our scaffolding removed let's check the size of the final image:</p> <pre><code> docker image ls modern-ml-microservices-housing-price-orchestrator\nREPOSITORY                                           TAG       IMAGE ID       CREATED         SIZE\nmodern-ml-microservices-housing-price-orchestrator   latest    a28fc433e42d   7 seconds ago   324MB\n</code></pre> <p><code>324MB</code> \u2014 smaller than where we started (likely because <code>uv</code> is no longer installed in the final image). Congratulations! You now have an image that is both smaller and more secure than our original, while capable of supporting projects with more complex system-dependency requirements. Code for this part can be found here!</p>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2025/12/07/delve-19-lets-build-a-modern-ml-microservice-application---part-9-docker-container-optimization.html#delve-data","title":"Delve Data","text":"<ul> <li>There are a number of optimizations described in the <code>uv</code> documentation for Docker image builds.</li> <li>Using multi-stage Docker builds, we can support additional build-time dependencies while ensuring they don't increase the size of the overall image.</li> </ul>","tags":["Series","Tutorial","Modern ML Microservices"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html","title":"Delve 20: A Look back at 2025","text":"<p>\"Self-reflection is the school of wisdom.\" - Baltasar Gracian</p> <p>Happy 2026 data delvers! As we enter a new year I wanted to take some time to reflect on the past year, share some thoughts about how I think it went, and shed some light on my goals for 2026!</p>","tags":["Opinion"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html#datadelver-blog","title":"Datadelver Blog","text":"<p>2025 was my most active blogging year yet. I published a total of 14 blog posts including the start of my Modern ML Microservices Series, which received a lot of positive feedback. I'm also particularly pleased with Delve 17: Data Science, An Industry Perspective, which had some of the best LinkedIn engagement with a single blog post I've ever had. I began 2025 with a soft goal of publishing one blog post per month which I'm happy to say I stuck to. Overall, I'd say I enjoyed the variety in the types of posts I made throughout the year and it's something I hope to continue into 2026!</p>","tags":["Opinion"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html#career","title":"Career","text":"<p>2025 came with a fairly large career shift, pivoting from a large enterprise to a startup and from hybrid to fully remote. The transition is something I'd like to write more about in the future but to suffice to say, it's going well! Some of the biggest takeaways I had from the shift was the amount of preparation it took to land a new role in this space, especially one that is fully remote. I'm very glad I documented much of this preparation in Delve 14: Reflections on a Job Quest!</p>","tags":["Opinion"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html#travel","title":"Travel","text":"<p>I didn't go on any large international trips in 2025 but did a few domestic trips around the country, something I hope to change in 2026!</p>","tags":["Opinion"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html#personal-pursuits","title":"Personal Pursuits","text":"<p>In terms of personal pursuits I endeavored to continue my study of the Japanese language, working my way through the Genki textbooks. Learning a non-Latin based language has been very challenging but rewarding for me, though my consistency in learning is something I'd like to improve. On the health front I have had better success consistently doing strength training, something I will strive to continue in 2026.</p>","tags":["Opinion"]},{"location":"2026/01/24/delve-20-a-look-back-at-2025.html#goals-for-2026","title":"Goals for 2026","text":"<p>Overall my goals for 2026 will probably stay in line with what I have been doing in the previous year. I'd like to maintain my blogging pace, develop my career, and continue my personal pursuits. Where I could see things changing is traveling more internationally, picking up some of my old hobbies like guitar again, or reading more books! Overall I feel pretty good about how 2025 went and am looking forward to 2026!</p>","tags":["Opinion"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html","title":"Delve 21: A Local Claude Code","text":"<p>\"The right tool doesn't just make a job easier; it changes how you think about the problem.\" - Gemini 3</p> <p>Hello data delvers! Though I am cautiously skeptical of the hype around AI, one area that I have seen my own productivity increase is by leveraging AI as a pair programmer. Up until this point, I have primarily relied on Github Copilot as my AI assistant. However, I recently gave Claude Code a try at work and was pleasantly surprised at its ability to go beyond what I had seen from Copilot and really \"pair\" with me. Based on these results, I resolved to set it up on my own machine to use for my personal projects. However, not wanting to spend money on tokens I wanted to be able to run an LLM locally and connect it to Claude Code. Fortunately with the latest release of Ollama this is a pretty straightforward thing to do!</p>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#ollama-oh-llama","title":"Ollama, Oh Llama?","text":"<p>Ollama is a leading open-source framework that enables you to run Large Language Models (LLMs) completely locally and connect them to a variety of applications. In a recent release, it added support for connecting directly to Claude Code.</p> <p>Getting started with Ollama is as simple as running the following install script from the command line on Linux or Mac:</p> <p><code>curl -fsSL https://ollama.com/install.sh | sh</code></p> <p>Once Ollama is installed you can start it by running:</p> <p><code>ollama serve</code></p> <p>The next step is to choose which LLM you want to use!</p>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#llms-a-plenty","title":"LLMs A Plenty","text":"<p>There are many different LLMs you may choose based on your available hardware. Some of the most popular currently include:</p> <ul> <li>devstral-small-2 - Generally considered a good baseline if you have a decent amount of RAM (25GB) and a GPU available.</li> <li>qwen3-coder - Another strong model but more resource intensive (32GB+ RAM), generally considered one of the best models currently available.</li> <li>flash:q8_0 - A model prioritizing speed and low latency with quantized weights.</li> </ul> <p>Note</p> <p>Konstantin Taletskiy gave a very detailed breakdown of several different models you could consider in a great blog post here!</p> <p>Once you've decided which LLM you prefer you can install it in a new terminal by running:</p> <p><code>ollama pull &lt;model_name&gt;</code></p>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#claude","title":"Claude!","text":"<p>Your next step is to install Claude Code! On Linux or Mac you can again run a simple shell script:</p> <p><code>curl -fsSL https://claude.ai/install.sh | bash</code></p> <p>We then have to configure Claude to point to the local Ollama instance. The easiest way to do this is to modify the Claude <code>settings.json</code> file (by default located at <code>~/.claude/settings.json</code>) to add in the following environment variables:</p> ~/.claude/settings.json<pre><code>{\n    \"$schema\": \"https://json.schemastore.org/claude-code-settings.json\",\n    \"env\": {\n        \"ANTHROPIC_AUTH_TOKEN\": \"ollama\",\n        \"ANTHROPIC_BASE_URL\": \"http://localhost:11434\"\n    }\n}\n</code></pre>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#ollama-claude","title":"Ollama + Claude = \u2728","text":"<p>Next, I recommend killing your running Ollama instance and re-running it with a larger context window (the default context window of 2048 tokens is rather limiting):</p> <p><code>OLLAMA_CONTEXT_LENGTH=32768 ollama serve</code></p> <p>You can then launch claude with your model of choice:</p> <p><code>claude --model &lt;model_name&gt;</code></p> <p></p> <p>Congratulations! You now have a fully-local Claude Code instance!</p>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#that-gui-feel","title":"That GUI Feel","text":"<p>If you are like me and coming from Github Copilot you may find having to use the CLI jarring. Fortunately, there is a Claude Code VS Code Plugin that will provide a seamless integration within the IDE. In order to get it to work you will need to follow the steps for using third party providers, namely, disabling the login prompt.</p> <p>You will also need to go into the extension settings and set your selected model to your local LLM.</p> <p>With that, you should be good to open up the Claude Code extension and get coding!</p> <p></p>","tags":["Tools","Tutorial"]},{"location":"2026/02/22/delve-21-a-local-claude-code.html#delve-data","title":"Delve Data","text":"<ul> <li>Running LLMs locally with Ollama provides a cost-effective alternative to cloud-based AI services.</li> <li>Ollama supports a variety of models with different resource requirements and performance characteristics.</li> <li>Claude Code can be configured to use a local Ollama instance instead of cloud APIs.</li> <li>Increasing the context window (e.g., to 32768 tokens) improves Claude Code's ability to handle complex codebases.</li> <li>The Claude Code VS Code extension provides seamless IDE integration for local LLM usage.</li> <li>Using local LLMs with Claude Code enables private, offline AI-assisted development.</li> </ul>","tags":["Tools","Tutorial"]},{"location":"archive/2026.html","title":"2026","text":""},{"location":"archive/2025.html","title":"2025","text":""},{"location":"archive/2024.html","title":"2024","text":""},{"location":"archive/2023.html","title":"2023","text":""},{"location":"category/ai.html","title":"AI","text":""},{"location":"category/meta.html","title":"Meta","text":""},{"location":"category/ml-engineering.html","title":"ML Engineering","text":""},{"location":"category/data-science.html","title":"Data Science","text":""},{"location":"category/software-engineering.html","title":"Software Engineering","text":""},{"location":"page/2/index.html","title":"Blog","text":""},{"location":"page/3/index.html","title":"Blog","text":""},{"location":"archive/2025/page/2.html","title":"2025","text":""}]}